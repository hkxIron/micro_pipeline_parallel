[[{"text": "Did you ever look at Deep Seek's", "start": 0.16, "duration": 6.48}, {"text": "pipeline parallelism algorithm dualpipe", "start": 3.36, "duration": 5.36}, {"text": "and ever think to yourself what black", "start": 6.64, "duration": 4.24}, {"text": "magic do these researchers use to", "start": 8.72, "duration": 4.72}, {"text": "conceive of such an algorithm? Well,", "start": 10.88, "duration": 4.879}, {"text": "don't worry because in this course we", "start": 13.44, "duration": 4.88}, {"text": "will derive pipeline parallelism from", "start": 15.759, "duration": 4.641}, {"text": "first principles and from scratch so", "start": 18.32, "duration": 4.48}, {"text": "that you can also make such an algorithm", "start": 20.4, "duration": 5.2}, {"text": "afterwards. My name is Kian and welcome", "start": 22.8, "duration": 4.799}, {"text": "to the pipeline parallelism from scratch", "start": 25.6, "duration": 4.079}, {"text": "course. And let's get into it. And", "start": 27.599, "duration": 3.6}, {"text": "before we get into it, I quickly want to", "start": 29.679, "duration": 3.201}, {"text": "mention the prerequisites of this course", "start": 31.199, "duration": 4.321}, {"text": "which are both PyTorch and Python. So in", "start": 32.88, "duration": 3.839}, {"text": "order to get the full value of this", "start": 35.52, "duration": 3.039}, {"text": "course, you should have experience with", "start": 36.719, "duration": 4.641}, {"text": "using both of these frameworks. So let's", "start": 38.559, "duration": 5.441}, {"text": "get started. The first step of course is", "start": 41.36, "duration": 6.08}, {"text": "going to be cloning the repository which", "start": 44.0, "duration": 6.079}, {"text": "you can take here or by going to the", "start": 47.44, "duration": 4.639}, {"text": "repo itself which is linked in the video", "start": 50.079, "duration": 4.16}, {"text": "description. and then just going through", "start": 52.079, "duration": 5.361}, {"text": "the standard GitHub cloning instructions", "start": 54.239, "duration": 5.361}, {"text": "and then once you've done that you will", "start": 57.44, "duration": 8.24}, {"text": "find the following menu of files. So let", "start": 59.6, "duration": 8.32}, {"text": "me first bring your attention to the my", "start": 65.68, "duration": 4.56}, {"text": "work directory because this is where all", "start": 67.92, "duration": 5.68}, {"text": "of the skeleton code is which we will", "start": 70.24, "duration": 5.84}, {"text": "complete together throughout the course.", "start": 73.6, "duration": 5.04}, {"text": "As you can see, there are five steps.", "start": 76.08, "duration": 6.16}, {"text": "And then moving on, we have docs which", "start": 78.64, "duration": 7.119}, {"text": "can be ignored because all of the docs", "start": 82.24, "duration": 6.08}, {"text": "are at this website which is just the", "start": 85.759, "duration": 6.881}, {"text": "same URL but in the GitHub pages format", "start": 88.32, "duration": 7.68}, {"text": "as the repository. So the index is here", "start": 92.64, "duration": 5.839}, {"text": "and then you can go through all of the", "start": 96.0, "duration": 5.68}, {"text": "different sections of the homepage which", "start": 98.479, "duration": 5.361}, {"text": "I will go through one at a time. So", "start": 101.68, "duration": 4.88}, {"text": "there's no need to go at this time and", "start": 103.84, "duration": 4.319}, {"text": "check out the website unless you want to", "start": 106.56, "duration": 4.0}, {"text": "check it out beforehand.", "start": 108.159, "duration": 6.081}, {"text": "And furthermore, we have the source", "start": 110.56, "duration": 6.32}, {"text": "directory. And these are all of the", "start": 114.24, "duration": 4.159}, {"text": "completed", "start": 116.88, "duration": 5.12}, {"text": "ground truth files, which is to say that", "start": 118.399, "duration": 5.76}, {"text": "as we go through the course, every", "start": 122.0, "duration": 4.719}, {"text": "single step here will correspond to one", "start": 124.159, "duration": 4.641}, {"text": "of the files here. So for example, step", "start": 126.719, "duration": 4.641}, {"text": "one manual will be the same as the", "start": 128.8, "duration": 4.0}, {"text": "manual file. And this is just the", "start": 131.36, "duration": 3.519}, {"text": "completed version so that it has", "start": 132.8, "duration": 4.079}, {"text": "comments first of all to explain all the", "start": 134.879, "duration": 4.481}, {"text": "steps like pretty thorough comments and", "start": 136.879, "duration": 5.36}, {"text": "it also works. So if you have a bug and", "start": 139.36, "duration": 4.72}, {"text": "you can't find the reason why it's not", "start": 142.239, "duration": 3.921}, {"text": "working then you can simply refer to", "start": 144.08, "duration": 4.32}, {"text": "this and other than that there's just", "start": 146.16, "duration": 4.799}, {"text": "the readme and main.py which is also", "start": 148.4, "duration": 4.559}, {"text": "implemented for us and it just the", "start": 150.959, "duration": 4.64}, {"text": "runner which when we go into step five", "start": 152.959, "duration": 4.241}, {"text": "and actually implement our pipeline", "start": 155.599, "duration": 3.521}, {"text": "scheduler it will what it is essentially", "start": 157.2, "duration": 5.36}, {"text": "what orchestrates the entire run. So now", "start": 159.12, "duration": 4.56}, {"text": "that we've gone through the file", "start": 162.56, "duration": 3.84}, {"text": "structure of the repository, let's go", "start": 163.68, "duration": 5.68}, {"text": "through the syllabus. So in this course,", "start": 166.4, "duration": 5.44}, {"text": "we will start with a MLP and train on a", "start": 169.36, "duration": 4.959}, {"text": "single CPU. It's called monolith because", "start": 171.84, "duration": 6.0}, {"text": "it only has one device or one GPU, CP,", "start": 174.319, "duration": 5.28}, {"text": "whatever you want to call it. And the", "start": 177.84, "duration": 2.96}, {"text": "goal here is just to establish a", "start": 179.599, "duration": 4.56}, {"text": "baseline with a sequential neural", "start": 180.8, "duration": 6.079}, {"text": "network that is simply 16 layers. And", "start": 184.159, "duration": 5.601}, {"text": "then we will motivate pipeline", "start": 186.879, "duration": 4.561}, {"text": "parallelism. In fact, we can just", "start": 189.76, "duration": 4.72}, {"text": "discuss it right now. So, as you may or", "start": 191.44, "duration": 6.48}, {"text": "may have heard before, a memory wall is", "start": 194.48, "duration": 5.2}, {"text": "essentially when your model does not fit", "start": 197.92, "duration": 4.08}, {"text": "on the GPU that you have. So, there are", "start": 199.68, "duration": 4.16}, {"text": "many ways to overcome this and one of", "start": 202.0, "duration": 4.0}, {"text": "them is through pipeline parallelism. So", "start": 203.84, "duration": 4.399}, {"text": "in this example, we have a 10 billion", "start": 206.0, "duration": 6.879}, {"text": "parameter LLM for example and it has the", "start": 208.239, "duration": 7.041}, {"text": "weight stored in float point 32 which", "start": 212.879, "duration": 4.561}, {"text": "would mean four bytes per parameter and", "start": 215.28, "duration": 4.16}, {"text": "if we multiply 4 by 10 billion then we", "start": 217.44, "duration": 4.799}, {"text": "get 40 GB and let's say for example we", "start": 219.44, "duration": 6.079}, {"text": "have a 4090 as our hardware. Well, you", "start": 222.239, "duration": 6.801}, {"text": "can see here that the VRAM of this chip", "start": 225.519, "duration": 5.601}, {"text": "is much less than the weights that we", "start": 229.04, "duration": 4.479}, {"text": "have. And of course, you don't simply", "start": 231.12, "duration": 5.039}, {"text": "need 40 GB to run this model. Since you", "start": 233.519, "duration": 4.961}, {"text": "have to store activations, this will be", "start": 236.159, "duration": 4.241}, {"text": "required. This will require more than 40", "start": 238.48, "duration": 4.56}, {"text": "GB. So, the solution that pipeline", "start": 240.4, "duration": 4.8}, {"text": "parallel introduces is model", "start": 243.04, "duration": 3.919}, {"text": "partitioning, which we will discuss in", "start": 245.2, "duration": 3.52}, {"text": "detail. And we will do this first", "start": 246.959, "duration": 4.081}, {"text": "manually. So, this is not what you ever", "start": 248.72, "duration": 4.0}, {"text": "want to do in production, but it's just", "start": 251.04, "duration": 3.919}, {"text": "to illustrate the point. And then once", "start": 252.72, "duration": 3.84}, {"text": "we've done this manually without any", "start": 254.959, "duration": 3.361}, {"text": "pipeline then we will go through the", "start": 256.56, "duration": 5.519}, {"text": "distributed basics which gives us the", "start": 258.32, "duration": 6.4}, {"text": "foundation for how we will communicate", "start": 262.079, "duration": 4.481}, {"text": "between the different GPUs because if", "start": 264.72, "duration": 3.12}, {"text": "you didn't know it already pipeline", "start": 266.56, "duration": 3.919}, {"text": "parallelism requires at least two GPUs.", "start": 267.84, "duration": 5.68}, {"text": "It's multiple GPUs which have are which", "start": 270.479, "duration": 4.641}, {"text": "are split up which are splitting the", "start": 273.52, "duration": 3.679}, {"text": "model amongst themselves.", "start": 275.12, "duration": 4.4}, {"text": "Then we'll talk about the naive solution", "start": 277.199, "duration": 5.361}, {"text": "and then the two sequential upgrades", "start": 279.52, "duration": 4.56}, {"text": "from the naive solution. So the first", "start": 282.56, "duration": 3.68}, {"text": "one is called G-pipe which was made by", "start": 284.08, "duration": 4.48}, {"text": "Google in 2019. And then after that we", "start": 286.24, "duration": 4.48}, {"text": "have one forward one backward which came", "start": 288.56, "duration": 4.0}, {"text": "out in 2021. Of course the", "start": 290.72, "duration": 3.759}, {"text": "state-of-the-art has improved since then", "start": 292.56, "duration": 3.919}, {"text": "but if you know these two then you're", "start": 294.479, "duration": 4.561}, {"text": "already pretty well mastered the concept", "start": 296.479, "duration": 4.801}, {"text": "of pipeline parallelism. So that is our", "start": 299.04, "duration": 5.599}, {"text": "syllabus. And with that being said let's", "start": 301.28, "duration": 6.88}, {"text": "start off with monolith. So, as you'll", "start": 304.639, "duration": 6.801}, {"text": "see, monolith only exists in the SRC and", "start": 308.16, "duration": 5.44}, {"text": "not in my work because we're not going", "start": 311.44, "duration": 3.36}, {"text": "to implement this since it has nothing", "start": 313.6, "duration": 2.96}, {"text": "to do with pipeline parallelism. This is", "start": 314.8, "duration": 7.52}, {"text": "literally just a 16 layer MLP.", "start": 316.56, "duration": 8.639}, {"text": "And since it's only 54 lines, we're", "start": 322.32, "duration": 5.36}, {"text": "going to go through the entire file very", "start": 325.199, "duration": 4.0}, {"text": "quickly just to see what's going on", "start": 327.68, "duration": 3.359}, {"text": "because this is the same template as all", "start": 329.199, "duration": 5.44}, {"text": "of the succeeding files in the course.", "start": 331.039, "duration": 6.401}, {"text": "So we have a 32 batch size with a hidden", "start": 334.639, "duration": 5.921}, {"text": "dimension of 128 and 16 layers with 50", "start": 337.44, "duration": 6.319}, {"text": "steps in our training loop. And then we", "start": 340.56, "duration": 6.0}, {"text": "initialize the MLP here. It's called the", "start": 343.759, "duration": 7.121}, {"text": "monolithic MLP. And simply um we take a", "start": 346.56, "duration": 6.32}, {"text": "list of layers. Of course, since it does", "start": 350.88, "duration": 4.56}, {"text": "inherit from the nn.module class, which", "start": 352.88, "duration": 4.159}, {"text": "is the base class for all neural network", "start": 355.44, "duration": 4.479}, {"text": "modules, then we need to call the super", "start": 357.039, "duration": 6.401}, {"text": "method so that all of the parameters of", "start": 359.919, "duration": 5.601}, {"text": "the NN module are initialized. For", "start": 363.44, "duration": 5.28}, {"text": "example, this sets up autograd and also", "start": 365.52, "duration": 6.399}, {"text": "the device tracking because pytorch will", "start": 368.72, "duration": 5.199}, {"text": "move your tensors and models", "start": 371.919, "duration": 4.4}, {"text": "automatically from device to device. But", "start": 373.919, "duration": 4.481}, {"text": "this won't be able to happen unless you", "start": 376.319, "duration": 5.361}, {"text": "have the uh parent method initialized", "start": 378.4, "duration": 5.68}, {"text": "which is nn module. And yeah we we have", "start": 381.68, "duration": 4.639}, {"text": "our layers here which we simply append", "start": 384.08, "duration": 5.28}, {"text": "to. So every layer of the model is just", "start": 386.319, "duration": 6.081}, {"text": "a nn linear which is a fully connected", "start": 389.36, "duration": 6.32}, {"text": "layer with hidden dimension 120 in this", "start": 392.4, "duration": 6.32}, {"text": "case followed by a nonlinear activation", "start": 395.68, "duration": 4.72}, {"text": "function which is just relu. So, we're", "start": 398.72, "duration": 4.319}, {"text": "really doing the most simple primitive", "start": 400.4, "duration": 5.6}, {"text": "MLP um operations because this is not", "start": 403.039, "duration": 4.561}, {"text": "the purpose of the course to make some", "start": 406.0, "duration": 3.199}, {"text": "say our MLP. It's just to have a", "start": 407.6, "duration": 4.48}, {"text": "baseline. And then we have our head", "start": 409.199, "duration": 4.56}, {"text": "which is what will perform the", "start": 412.08, "duration": 3.44}, {"text": "classification. So, I haven't mentioned", "start": 413.759, "duration": 4.88}, {"text": "it but we're just training an MLP on", "start": 415.52, "duration": 6.32}, {"text": "random data to form to do binary", "start": 418.639, "duration": 5.761}, {"text": "classification. As you can see, our", "start": 421.84, "duration": 5.919}, {"text": "dimension is reduced from 128 to two in", "start": 424.4, "duration": 5.359}, {"text": "the last layer because we want to have", "start": 427.759, "duration": 7.761}, {"text": "two classes. And then we take our 16 +", "start": 429.759, "duration": 9.921}, {"text": "one head layers and then uh use the star", "start": 435.52, "duration": 7.36}, {"text": "to unpack all of those layers into a n", "start": 439.68, "duration": 5.6}, {"text": "sequential and we also establish a cross", "start": 442.88, "duration": 4.08}, {"text": "entropy loss as our loss function since", "start": 445.28, "duration": 3.359}, {"text": "it is once again binary classification.", "start": 446.96, "duration": 2.959}, {"text": "Then the forward function is just", "start": 448.639, "duration": 4.081}, {"text": "getting our logits and then using the", "start": 449.919, "duration": 6.0}, {"text": "cross entry loss to calculate our loss", "start": 452.72, "duration": 5.28}, {"text": "from those logits. So hopefully up to", "start": 455.919, "duration": 4.161}, {"text": "now this is all pretty simple. One thing", "start": 458.0, "duration": 5.12}, {"text": "worth noting is that the manual seed is", "start": 460.08, "duration": 5.04}, {"text": "set to 42 in all of our files. So we can", "start": 463.12, "duration": 4.479}, {"text": "have our so we can have a consistent way", "start": 465.12, "duration": 4.16}, {"text": "of checking results between different", "start": 467.599, "duration": 3.6}, {"text": "methods. And then we'll initialize our", "start": 469.28, "duration": 3.28}, {"text": "model here. Initialize the atom", "start": 471.199, "duration": 4.801}, {"text": "optimizer with a learning rate of 0.001.", "start": 472.56, "duration": 4.8}, {"text": "The same thing is done throughout the", "start": 476.0, "duration": 5.039}, {"text": "entire course. And as I mentioned, we", "start": 477.36, "duration": 7.839}, {"text": "are going to overfit to a random tensor", "start": 481.039, "duration": 6.641}, {"text": "of batch size 32 and hidden mentioned", "start": 485.199, "duration": 3.921}, {"text": "128. So the model is literally just", "start": 487.68, "duration": 6.48}, {"text": "learning a random u tensor. And", "start": 489.12, "duration": 7.359}, {"text": "then our fixed target is the class of", "start": 494.16, "duration": 6.24}, {"text": "each random tensor. So for every single", "start": 496.479, "duration": 7.601}, {"text": "element in this 32 batch size tensor,", "start": 500.4, "duration": 5.76}, {"text": "we'll give it a class through the", "start": 504.08, "duration": 4.48}, {"text": "torch.randit function which just creates", "start": 506.16, "duration": 5.36}, {"text": "a tensor from 0 to one because the low", "start": 508.56, "duration": 4.959}, {"text": "is inclusive and the high is exclusive", "start": 511.52, "duration": 3.6}, {"text": "just like range for example. And it will", "start": 513.519, "duration": 3.52}, {"text": "just be of shape 32. And here's the", "start": 515.12, "duration": 4.32}, {"text": "training loop. We start the training and", "start": 517.039, "duration": 5.12}, {"text": "we'll time it as well. And then for 50", "start": 519.44, "duration": 5.6}, {"text": "steps, we'll first um zero our gradient", "start": 522.159, "duration": 4.8}, {"text": "so we're not accumulating the gradients", "start": 525.04, "duration": 4.32}, {"text": "unknowingly. And then we calculate the", "start": 526.959, "duration": 5.601}, {"text": "loss. We do the backward pass back", "start": 529.36, "duration": 5.52}, {"text": "propagation our optimizer step. And then", "start": 532.56, "duration": 4.32}, {"text": "every single", "start": 534.88, "duration": 5.2}, {"text": "five steps we will print our loss and", "start": 536.88, "duration": 5.12}, {"text": "the step that we're on. And that's it.", "start": 540.08, "duration": 3.92}, {"text": "So as I said, we're not actually", "start": 542.0, "duration": 4.56}, {"text": "implementing this oursel. So what we're", "start": 544.0, "duration": 5.12}, {"text": "going to do instead is just use UV run", "start": 546.56, "duration": 5.04}, {"text": "to do this right now. And throughout the", "start": 549.12, "duration": 5.279}, {"text": "entire course I'll be using um UV run.", "start": 551.6, "duration": 7.04}, {"text": "But if you prefer or any other way of", "start": 554.399, "duration": 7.041}, {"text": "managing a Python project then please do", "start": 558.64, "duration": 4.4}, {"text": "that instead. That's the way I'm doing", "start": 561.44, "duration": 3.28}, {"text": "it in this course. And everything that I", "start": 563.04, "duration": 3.6}, {"text": "do um you will be able to reproduce", "start": 564.72, "duration": 3.84}, {"text": "because UV is just really nice and", "start": 566.64, "duration": 3.28}, {"text": "reproducible. So that's my", "start": 568.56, "duration": 4.08}, {"text": "recommendation. Anyways, as you can see", "start": 569.92, "duration": 5.76}, {"text": "in our training um setup with 50", "start": 572.64, "duration": 4.8}, {"text": "iterations, we started with a loss of", "start": 575.68, "duration": 4.24}, {"text": "0.7 in our cross entropy loss and then", "start": 577.44, "duration": 6.16}, {"text": "it went all the way down to 0.2436.", "start": 579.92, "duration": 6.56}, {"text": "And because once again we have our", "start": 583.6, "duration": 4.64}, {"text": "manual seat set to 42. This is", "start": 586.48, "duration": 3.52}, {"text": "deterministic. So if I run this one more", "start": 588.24, "duration": 3.36}, {"text": "time, it's the same exact thing. Of", "start": 590.0, "duration": 3.2}, {"text": "course the time is not deterministic", "start": 591.6, "duration": 4.08}, {"text": "because wherever the memory is happening", "start": 593.2, "duration": 4.8}, {"text": "like of course the memory management", "start": 595.68, "duration": 3.839}, {"text": "that's happening at a very low level in", "start": 598.0, "duration": 3.839}, {"text": "C probably and the buffer management", "start": 599.519, "duration": 4.32}, {"text": "makes it such that the time is different", "start": 601.839, "duration": 3.601}, {"text": "slightly every single round but the", "start": 603.839, "duration": 3.761}, {"text": "final loss is not and if I just show you", "start": 605.44, "duration": 4.0}, {"text": "something here", "start": 607.6, "duration": 3.52}, {"text": "by commenting out the manual seed it", "start": 609.44, "duration": 3.44}, {"text": "will just do the default manual seed and", "start": 611.12, "duration": 3.6}, {"text": "you can see that it's different in terms", "start": 612.88, "duration": 3.68}, {"text": "of the final loss so the final loss is", "start": 614.72, "duration": 3.92}, {"text": "always changing because um I'm not", "start": 616.56, "duration": 4.0}, {"text": "actually sure what the behavior is when", "start": 618.64, "duration": 3.199}, {"text": "you don't set a manual ual seed. It", "start": 620.56, "duration": 3.44}, {"text": "probably just gives a random seed to the", "start": 621.839, "duration": 4.481}, {"text": "random seed. But in any case, what we'll", "start": 624.0, "duration": 3.839}, {"text": "note here is that the final loss is", "start": 626.32, "duration": 3.84}, {"text": "always the same with our manual seed. So", "start": 627.839, "duration": 5.44}, {"text": "this is our monolith. And now let's come", "start": 630.16, "duration": 5.52}, {"text": "back to our syllabus. So we establish", "start": 633.279, "duration": 4.721}, {"text": "the baseline and it's just an sequential", "start": 635.68, "duration": 5.839}, {"text": "with 16 layers. And now once again we're", "start": 638.0, "duration": 5.2}, {"text": "going to hypothetically consider the", "start": 641.519, "duration": 4.0}, {"text": "fact that we're just running this on a", "start": 643.2, "duration": 4.24}, {"text": "CPU of course and it's a really small", "start": 645.519, "duration": 3.681}, {"text": "model. But let's say that this would no", "start": 647.44, "duration": 3.28}, {"text": "longer fit on the model and we want to", "start": 649.2, "duration": 3.52}, {"text": "split it in half. So we can split it", "start": 650.72, "duration": 4.48}, {"text": "across on two GPUs. So we'll be cutting", "start": 652.72, "duration": 4.799}, {"text": "the neural network sequential class into", "start": 655.2, "duration": 5.28}, {"text": "a part one and a part two by manually", "start": 657.519, "duration": 4.88}, {"text": "passing the output of part one into part", "start": 660.48, "duration": 6.72}, {"text": "two. And this is our first step in my", "start": 662.399, "duration": 8.961}, {"text": "work. So let's open up my work and step", "start": 667.2, "duration": 8.16}, {"text": "one which is called manual.py.", "start": 671.36, "duration": 6.719}, {"text": "So here you go. As you can see, it's", "start": 675.36, "duration": 6.4}, {"text": "quite similar in structure to our man", "start": 678.079, "duration": 6.641}, {"text": "our models.py file except it's got a", "start": 681.76, "duration": 6.16}, {"text": "bunch of to-dos where we need to add the", "start": 684.72, "duration": 5.84}, {"text": "functionality which will make it sharded", "start": 687.92, "duration": 5.84}, {"text": "across two models. So here you can see", "start": 690.56, "duration": 4.88}, {"text": "first of all we have part one part two", "start": 693.76, "duration": 4.8}, {"text": "instead of just the monolithic MLP in", "start": 695.44, "duration": 6.8}, {"text": "the first file and furthermore we need", "start": 698.56, "duration": 7.279}, {"text": "to for example make our optimizer track", "start": 702.24, "duration": 6.56}, {"text": "both the parameters for the first part", "start": 705.839, "duration": 5.201}, {"text": "of the model and the second part. And", "start": 708.8, "duration": 5.2}, {"text": "then another thing worth noting is that", "start": 711.04, "duration": 5.84}, {"text": "in terms of the total layers, we will", "start": 714.0, "duration": 6.64}, {"text": "not do 4 I in range 16 for each model", "start": 716.88, "duration": 5.199}, {"text": "because then the final model will be", "start": 720.64, "duration": 3.759}, {"text": "double the size as the monolith. So", "start": 722.079, "duration": 4.0}, {"text": "instead we'll have to divide by the", "start": 724.399, "duration": 3.68}, {"text": "number of devices that we have. In this", "start": 726.079, "duration": 3.521}, {"text": "case, it's two. So we'll do 16 divid by", "start": 728.079, "duration": 4.081}, {"text": "two and every single part or the two", "start": 729.6, "duration": 4.16}, {"text": "parts will hold eight. To be more", "start": 732.16, "duration": 3.359}, {"text": "precise, this one will hold eight and", "start": 733.76, "duration": 3.519}, {"text": "part two will hold nine because it's", "start": 735.519, "duration": 4.961}, {"text": "also storing the classification head. So", "start": 737.279, "duration": 5.521}, {"text": "that's the context that I'll give you. I", "start": 740.48, "duration": 4.4}, {"text": "would highly encourage you to implement", "start": 742.8, "duration": 6.4}, {"text": "this or try to once again all of the", "start": 744.88, "duration": 7.199}, {"text": "solution is here in manual.py. So if", "start": 749.2, "duration": 5.6}, {"text": "during this implementation you get stuck", "start": 752.079, "duration": 5.76}, {"text": "somewhere you can refer to this and I", "start": 754.8, "duration": 4.4}, {"text": "will not be mentioning this in the", "start": 757.839, "duration": 3.041}, {"text": "future which is to say now we're just", "start": 759.2, "duration": 2.879}, {"text": "going to start imple implementing this", "start": 760.88, "duration": 2.639}, {"text": "and when I do step two I'm not going to", "start": 762.079, "duration": 3.841}, {"text": "say okay go try on your own with coms.py", "start": 763.519, "duration": 4.801}, {"text": "high, but I would recommend to do this", "start": 765.92, "duration": 3.76}, {"text": "through the entire course if you would", "start": 768.32, "duration": 2.959}, {"text": "like to because that's more active in", "start": 769.68, "duration": 3.12}, {"text": "terms of learning as opposed to passive.", "start": 771.279, "duration": 2.721}, {"text": "But now that I've said that, we're", "start": 772.8, "duration": 2.08}, {"text": "actually going to implement this", "start": 774.0, "duration": 4.8}, {"text": "together. So let's start off with part", "start": 774.88, "duration": 6.16}, {"text": "one. And once we've done this, we can", "start": 778.8, "duration": 5.36}, {"text": "copy paste most of what we've uh written", "start": 781.04, "duration": 7.28}, {"text": "here in the part two. So as you remember", "start": 784.16, "duration": 6.799}, {"text": "previously we have to initialize a list", "start": 788.32, "duration": 5.84}, {"text": "called layer layers which stores all of", "start": 790.959, "duration": 7.44}, {"text": "the nn.linear linear um fully connected", "start": 794.16, "duration": 7.44}, {"text": "layers and then we will just iterate", "start": 798.399, "duration": 5.921}, {"text": "over the total layers here which should", "start": 801.6, "duration": 5.359}, {"text": "come up here and then divide this by two", "start": 804.32, "duration": 4.959}, {"text": "using integer integer division because", "start": 806.959, "duration": 3.601}, {"text": "one thing worth noting is that if you", "start": 809.279, "duration": 5.521}, {"text": "divide by two like this if I just um", "start": 810.56, "duration": 6.64}, {"text": "open up a Python terminal and do 16", "start": 814.8, "duration": 3.68}, {"text": "divid by two which is what we're going", "start": 817.2, "duration": 4.319}, {"text": "to do in the code it gives you a float", "start": 818.48, "duration": 4.64}, {"text": "and I'm not sure if this will cause an", "start": 821.519, "duration": 2.481}, {"text": "error with range if it will", "start": 823.12, "duration": 3.6}, {"text": "automatically um input it as an int or", "start": 824.0, "duration": 4.56}, {"text": "not. But just to play it safe, we want", "start": 826.72, "duration": 4.4}, {"text": "to do 16 divided by two integer", "start": 828.56, "duration": 4.88}, {"text": "division. And one thing worth noting is", "start": 831.12, "duration": 3.36}, {"text": "that we're never going to have an odd", "start": 833.44, "duration": 2.32}, {"text": "number of layers since we can choose", "start": 834.48, "duration": 3.44}, {"text": "that. But if we actually do have odd", "start": 835.76, "duration": 3.6}, {"text": "number of layers and this will prevent", "start": 837.92, "duration": 3.2}, {"text": "an error where you do four in range like", "start": 839.36, "duration": 3.68}, {"text": "7.5 which will definitely give you an", "start": 841.12, "duration": 4.56}, {"text": "error. So now that we've done that,", "start": 843.04, "duration": 6.159}, {"text": "we're going to initialize our", "start": 845.68, "duration": 6.8}, {"text": "part one with the layer uh the NN layers", "start": 849.199, "duration": 4.721}, {"text": "as we already mentioned. So we have to", "start": 852.48, "duration": 5.28}, {"text": "append them n linear", "start": 853.92, "duration": 6.96}, {"text": "and the dim is going from dim to dim. So", "start": 857.76, "duration": 6.24}, {"text": "pretty simple. And then we also want to", "start": 860.88, "duration": 6.72}, {"text": "add a nn uhrelu which does not take any", "start": 864.0, "duration": 6.32}, {"text": "parameters. So I can just delete this.", "start": 867.6, "duration": 4.799}, {"text": "And this is everything that we need in", "start": 870.32, "duration": 4.0}, {"text": "terms of the layers for our nn", "start": 872.399, "duration": 5.521}, {"text": "sequential. So we'll say self.net", "start": 874.32, "duration": 5.199}, {"text": "equals", "start": 877.92, "duration": 4.56}, {"text": "nn.sequential sequential", "start": 879.519, "duration": 5.361}, {"text": "of the unpacked layers. So once again,", "start": 882.48, "duration": 5.599}, {"text": "the star iterator or the star operator", "start": 884.88, "duration": 5.759}, {"text": "just takes a list and then unpacks it,", "start": 888.079, "duration": 5.681}, {"text": "which is what sequential expects as its", "start": 890.639, "duration": 5.521}, {"text": "argument if I'm not mistaken. So if we", "start": 893.76, "duration": 4.319}, {"text": "come here,", "start": 896.16, "duration": 4.239}, {"text": "yeah, it's an order dict. I don't know", "start": 898.079, "duration": 6.56}, {"text": "how the exact um input requirement is,", "start": 900.399, "duration": 6.0}, {"text": "but what I do know is that this is the", "start": 904.639, "duration": 4.081}, {"text": "way that you should input it in order to", "start": 906.399, "duration": 5.921}, {"text": "get a proper um neural network. And then", "start": 908.72, "duration": 4.88}, {"text": "our forward method is already", "start": 912.32, "duration": 3.04}, {"text": "implemented because it's kind of simple.", "start": 913.6, "duration": 3.52}, {"text": "Um we didn't really need to do that on", "start": 915.36, "duration": 4.159}, {"text": "our own, but all we all we do here is", "start": 917.12, "duration": 6.48}, {"text": "return self.net of x and we don't return", "start": 919.519, "duration": 6.241}, {"text": "the loss. So one thing worth worth", "start": 923.6, "duration": 4.0}, {"text": "noting here that's already different is", "start": 925.76, "duration": 3.68}, {"text": "that the forward for part one and part", "start": 927.6, "duration": 4.56}, {"text": "two are not the same because this is", "start": 929.44, "duration": 5.04}, {"text": "just an intermediate step right this is", "start": 932.16, "duration": 4.72}, {"text": "the output of the eighth layer whereas", "start": 934.48, "duration": 4.159}, {"text": "here it's the output of the entire", "start": 936.88, "duration": 2.959}, {"text": "network at which point we calculate the", "start": 938.639, "duration": 4.161}, {"text": "loss you can calculate the loss at step", "start": 939.839, "duration": 6.0}, {"text": "8 but then it defeats the purpose of", "start": 942.8, "duration": 6.159}, {"text": "having 16 layers. So now what you can", "start": 945.839, "duration": 6.081}, {"text": "think here is that this activation which", "start": 948.959, "duration": 5.041}, {"text": "is called the output of this uh this", "start": 951.92, "duration": 3.919}, {"text": "part one neural network will be fed into", "start": 954.0, "duration": 4.399}, {"text": "part two and now we need to implement", "start": 955.839, "duration": 4.721}, {"text": "part two. But as I mentioned earlier we", "start": 958.399, "duration": 4.88}, {"text": "can copy most of it and then just add", "start": 960.56, "duration": 6.56}, {"text": "the um LLM head not the LM head but the", "start": 963.279, "duration": 5.761}, {"text": "um neural network head because it", "start": 967.12, "duration": 2.959}, {"text": "actually needs to perform the", "start": 969.04, "duration": 5.2}, {"text": "classification. So before we initialize", "start": 970.079, "duration": 6.641}, {"text": "our self.net net. I'm just going to do", "start": 974.24, "duration": 4.159}, {"text": "self.", "start": 976.72, "duration": 3.919}, {"text": "Um, what am I going to do? I'm going to", "start": 978.399, "duration": 7.841}, {"text": "do not self, but rather layers.append", "start": 980.639, "duration": 7.2}, {"text": "of", "start": 986.24, "duration": 6.159}, {"text": "we're going to append a nn.linear.", "start": 987.839, "duration": 7.521}, {"text": "And this nn.linear is going to go from", "start": 992.399, "duration": 7.041}, {"text": "dimension dim to dimension 2 because", "start": 995.36, "duration": 6.88}, {"text": "we're doing binary classification. And", "start": 999.44, "duration": 6.319}, {"text": "then yeah we should be fine to create", "start": 1002.24, "duration": 7.2}, {"text": "our net which is saved as a member of", "start": 1005.759, "duration": 6.481}, {"text": "the part two class. And then in our", "start": 1009.44, "duration": 6.24}, {"text": "forward method we simply do the same", "start": 1012.24, "duration": 6.32}, {"text": "step as here which is self.net ofx which", "start": 1015.68, "duration": 4.959}, {"text": "calls the forward method on the", "start": 1018.56, "duration": 3.68}, {"text": "sequential neural network which will", "start": 1020.639, "duration": 3.041}, {"text": "just do a forward pass. And of course,", "start": 1022.24, "duration": 3.04}, {"text": "as you know, we don't need to implement", "start": 1023.68, "duration": 3.759}, {"text": "backward in PyTorch because it will just", "start": 1025.28, "duration": 4.88}, {"text": "take the forward computational graph and", "start": 1027.439, "duration": 5.201}, {"text": "do this in the opposite direction with", "start": 1030.16, "duration": 4.399}, {"text": "Autograd, which will maintain all the", "start": 1032.64, "duration": 3.279}, {"text": "gradients and all that good stuff that", "start": 1034.559, "duration": 2.961}, {"text": "we don't have to worry about anyways.", "start": 1035.919, "duration": 4.721}, {"text": "And then once we have this um these", "start": 1037.52, "duration": 6.399}, {"text": "logs, which is the output of our", "start": 1040.64, "duration": 5.199}, {"text": "classification head, it will be two", "start": 1043.919, "duration": 4.241}, {"text": "logits, one for class zero and one for", "start": 1045.839, "duration": 5.041}, {"text": "class one. And since we do input a", "start": 1048.16, "duration": 4.8}, {"text": "random tensor, these logistics don't", "start": 1050.88, "duration": 4.96}, {"text": "really have any meaning whatsoever. But", "start": 1052.96, "duration": 4.719}, {"text": "of course, if you want to interpret this", "start": 1055.84, "duration": 3.839}, {"text": "in like the image classification sense,", "start": 1057.679, "duration": 4.481}, {"text": "maybe our data set is dogs and cats and", "start": 1059.679, "duration": 5.281}, {"text": "we want to classify between these two.", "start": 1062.16, "duration": 4.96}, {"text": "So that could be a potential example.", "start": 1064.96, "duration": 3.599}, {"text": "But for simplicity sake, once again,", "start": 1067.12, "duration": 4.559}, {"text": "we're just learning a random tensor. And", "start": 1068.559, "duration": 5.36}, {"text": "then we calculate loss. Oh, and that's", "start": 1071.679, "duration": 3.761}, {"text": "the one thing that I forgot. We didn't", "start": 1073.919, "duration": 3.601}, {"text": "actually initialize our loss function.", "start": 1075.44, "duration": 4.32}, {"text": "So it's great to notice that now and not", "start": 1077.52, "duration": 5.2}, {"text": "later. So this will just be nn cross", "start": 1079.76, "duration": 5.76}, {"text": "entropy loss. The criterion computes the", "start": 1082.72, "duration": 4.4}, {"text": "cross entropy loss between input logs", "start": 1085.52, "duration": 5.519}, {"text": "and targets and target. So we once again", "start": 1087.12, "duration": 6.08}, {"text": "have the targets here but not the", "start": 1091.039, "duration": 5.201}, {"text": "targets here. And this is because we're", "start": 1093.2, "duration": 5.12}, {"text": "not calculating the loss in part one.", "start": 1096.24, "duration": 5.84}, {"text": "But even so the targets are just fake um", "start": 1098.32, "duration": 6.0}, {"text": "values. But if we take the dogcat", "start": 1102.08, "duration": 3.92}, {"text": "example once again for every single", "start": 1104.32, "duration": 4.0}, {"text": "input data we would have the ground", "start": 1106.0, "duration": 3.6}, {"text": "truth because this is supervised", "start": 1108.32, "duration": 3.68}, {"text": "learning so that the model can learn", "start": 1109.6, "duration": 4.56}, {"text": "which images are cats and which images", "start": 1112.0, "duration": 4.799}, {"text": "are dogs and I'm just going to check", "start": 1114.16, "duration": 3.84}, {"text": "here. I don't think this takes any", "start": 1116.799, "duration": 3.681}, {"text": "arguments. Um let's just look at an", "start": 1118.0, "duration": 4.88}, {"text": "example. I would really recommend to", "start": 1120.48, "duration": 5.76}, {"text": "look at the um typeins that pop up in", "start": 1122.88, "duration": 5.44}, {"text": "your IDE for PyTorch. It's really really", "start": 1126.24, "duration": 3.76}, {"text": "helpful. So once again here you can see", "start": 1128.32, "duration": 5.04}, {"text": "that it is not um initialized with any", "start": 1130.0, "duration": 6.08}, {"text": "arguments. And we also can call this", "start": 1133.36, "duration": 4.48}, {"text": "self.criterion. That's a really common", "start": 1136.08, "duration": 3.92}, {"text": "name um used here. It looks like they", "start": 1137.84, "duration": 4.719}, {"text": "call it loss but um they use the the", "start": 1140.0, "duration": 4.559}, {"text": "word criterion. So criterion and loss in", "start": 1142.559, "duration": 4.801}, {"text": "this case are interchangeable.", "start": 1144.559, "duration": 6.0}, {"text": "And now unless I'm forgetting something", "start": 1147.36, "duration": 5.52}, {"text": "is set up such that when we do the", "start": 1150.559, "duration": 3.841}, {"text": "forward pass through these both through", "start": 1152.88, "duration": 3.84}, {"text": "these two networks um we will call", "start": 1154.4, "duration": 5.519}, {"text": "self.lost lost on the logits which come", "start": 1156.72, "duration": 5.28}, {"text": "here from the neural network and the and", "start": 1159.919, "duration": 4.0}, {"text": "we'll compare this to the targets to", "start": 1162.0, "duration": 4.0}, {"text": "yeah give our loss and then in the", "start": 1163.919, "duration": 4.481}, {"text": "backward pass update our weights with", "start": 1166.0, "duration": 5.28}, {"text": "the gradients okay so we did that now", "start": 1168.4, "duration": 5.12}, {"text": "let's go to the next todo in this here", "start": 1171.28, "duration": 4.88}, {"text": "in this case here oh I didn't actually", "start": 1173.52, "duration": 6.159}, {"text": "say what we need to do so um in essence", "start": 1176.16, "duration": 5.84}, {"text": "we are checking if cuda is available so", "start": 1179.679, "duration": 3.761}, {"text": "what I didn't mention is that in this", "start": 1182.0, "duration": 4.48}, {"text": "course we are going to assume and just", "start": 1183.44, "duration": 6.8}, {"text": "use the CPU and um just shard over", "start": 1186.48, "duration": 6.4}, {"text": "virtual cores as opposed to using the", "start": 1190.24, "duration": 5.12}, {"text": "GPU because I don't have actually access", "start": 1192.88, "duration": 5.12}, {"text": "to a GPU and I'm assuming that most of", "start": 1195.36, "duration": 6.4}, {"text": "you don't and moreover to do pipeline", "start": 1198.0, "duration": 6.72}, {"text": "parallelism you need multiple GPUs. So", "start": 1201.76, "duration": 5.279}, {"text": "for sake of simplicity we're going to be", "start": 1204.72, "duration": 6.24}, {"text": "using CPU as the default CUDA uh sorry", "start": 1207.039, "duration": 7.121}, {"text": "torch device but the repository still", "start": 1210.96, "duration": 6.16}, {"text": "has support for GPU. So if you have in", "start": 1214.16, "duration": 4.8}, {"text": "this case two GPUs then we're going to", "start": 1217.12, "duration": 5.28}, {"text": "implement the um proper device handling", "start": 1218.96, "duration": 8.88}, {"text": "to allow the um model to move the", "start": 1222.4, "duration": 8.72}, {"text": "tensors and the different um neural", "start": 1227.84, "duration": 6.4}, {"text": "networks onto the correct device. So", "start": 1231.12, "duration": 6.24}, {"text": "first of all we check if torch.ca is", "start": 1234.24, "duration": 6.4}, {"text": "available. So if you come into your own", "start": 1237.36, "duration": 5.199}, {"text": "terminal with UV run Python, I'll show", "start": 1240.64, "duration": 5.12}, {"text": "you how to um check this for yourself.", "start": 1242.559, "duration": 4.48}, {"text": "So first thing is I'm going to import", "start": 1245.76, "duration": 3.039}, {"text": "torch. And you might be wondering how is", "start": 1247.039, "duration": 4.481}, {"text": "it possible that it's not giving an", "start": 1248.799, "duration": 5.601}, {"text": "error. And I'll just show you just so we", "start": 1251.52, "duration": 4.72}, {"text": "can check for ourselves. If I just run", "start": 1254.4, "duration": 4.96}, {"text": "Python 3 outside of UV and try to import", "start": 1256.24, "duration": 5.6}, {"text": "torch, we'll get an error because I", "start": 1259.36, "duration": 3.52}, {"text": "don't have it in the virtual", "start": 1261.84, "duration": 4.16}, {"text": "environment. But since we do have torch", "start": 1262.88, "duration": 7.44}, {"text": "the dependency of our um project and", "start": 1266.0, "duration": 6.799}, {"text": "furthermore I have opened the Python", "start": 1270.32, "duration": 4.32}, {"text": "interpreter with UV then everything's", "start": 1272.799, "duration": 4.24}, {"text": "handled for us. So this course is almost", "start": 1274.64, "duration": 4.159}, {"text": "becoming a UV advertisement but it", "start": 1277.039, "duration": 4.561}, {"text": "really is amazing to use. So what I was", "start": 1278.799, "duration": 5.841}, {"text": "going to do is I think we copied it. Oh", "start": 1281.6, "duration": 4.4}, {"text": "okay I didn't I didn't copy it. I", "start": 1284.64, "duration": 3.12}, {"text": "thought I copied the value. So let's", "start": 1286.0, "duration": 5.039}, {"text": "come ahead here and copy if torch.ca is", "start": 1287.76, "duration": 6.399}, {"text": "available. Jeez man. And then we'll see", "start": 1291.039, "duration": 5.201}, {"text": "that it's false. Um because I don't have", "start": 1294.159, "duration": 4.161}, {"text": "a torch uh I don't have a CUDA device on", "start": 1296.24, "duration": 3.84}, {"text": "my computer. But if you do, then you", "start": 1298.32, "duration": 4.56}, {"text": "would get this value here. And", "start": 1300.08, "duration": 4.16}, {"text": "furthermore, you can see the number of", "start": 1302.88, "duration": 4.799}, {"text": "devices available by um calling device", "start": 1304.24, "duration": 5.84}, {"text": "count. Oh. Oh, it's because I just found", "start": 1307.679, "duration": 4.48}, {"text": "a bug in my code. That's amazing. Uh we", "start": 1310.08, "duration": 3.92}, {"text": "just called the fun. We just initialized", "start": 1312.159, "duration": 3.361}, {"text": "the function, but we didn't actually", "start": 1314.0, "duration": 3.6}, {"text": "call it. So you can see see here that we", "start": 1315.52, "duration": 4.88}, {"text": "have zero in terms of the device count.", "start": 1317.6, "duration": 4.64}, {"text": "So this is both checking is the device", "start": 1320.4, "duration": 5.279}, {"text": "available and are there more than two or", "start": 1322.24, "duration": 4.799}, {"text": "more than or equal to two because in", "start": 1325.679, "duration": 3.521}, {"text": "this case we need at least two GPUs and", "start": 1327.039, "duration": 3.361}, {"text": "then if that is the case then we set", "start": 1329.2, "duration": 3.04}, {"text": "CUDA to true. So now here we can say", "start": 1330.4, "duration": 4.88}, {"text": "that if CUDA we will do something and", "start": 1332.24, "duration": 4.88}, {"text": "then if not CUDA we will do something", "start": 1335.28, "duration": 5.279}, {"text": "else because we want to move our model", "start": 1337.12, "duration": 4.96}, {"text": "which in this case is part one and part", "start": 1340.559, "duration": 4.641}, {"text": "two onto CUDA if we do have those", "start": 1342.08, "duration": 4.64}, {"text": "devices available. So in the case that", "start": 1345.2, "duration": 3.839}, {"text": "we don't we don't do anything because if", "start": 1346.72, "duration": 5.04}, {"text": "you didn't know this in PyTorch the", "start": 1349.039, "duration": 4.161}, {"text": "tensors and models that you initialize", "start": 1351.76, "duration": 4.24}, {"text": "are auto always um initialized on the", "start": 1353.2, "duration": 5.76}, {"text": "CPU by default you don't have to specify", "start": 1356.0, "duration": 6.32}, {"text": "to device CPU but if you want it to be", "start": 1358.96, "duration": 5.44}, {"text": "on your GPU then you do so we're going", "start": 1362.32, "duration": 6.0}, {"text": "to do here is initialize our neural", "start": 1364.4, "duration": 7.2}, {"text": "network right part one is initializing a", "start": 1368.32, "duration": 5.28}, {"text": "eight layer neural network and then", "start": 1371.6, "duration": 4.72}, {"text": "we're going to do two device to", "start": 1373.6, "duration": 5.199}, {"text": "torch.cuda.de", "start": 1376.32, "duration": 6.16}, {"text": "device. And this will select the", "start": 1378.799, "duration": 5.921}, {"text": "relevant GPU. In this case, we'll just", "start": 1382.48, "duration": 5.28}, {"text": "select GPU0 because this is the first", "start": 1384.72, "duration": 6.72}, {"text": "layer. And then uh GPU 1 because this is", "start": 1387.76, "duration": 5.44}, {"text": "the second layer. Of course, it doesn't", "start": 1391.44, "duration": 3.68}, {"text": "really matter. You could flip the order", "start": 1393.2, "duration": 3.68}, {"text": "of the GPUs you put the model on. It's", "start": 1395.12, "duration": 3.439}, {"text": "just a number for the index, but let's", "start": 1396.88, "duration": 4.72}, {"text": "be consistent here. So now if we have", "start": 1398.559, "duration": 6.321}, {"text": "GPUs, we're going to put the model one", "start": 1401.6, "duration": 9.199}, {"text": "on the GPU zero and model 2 on GPU one.", "start": 1404.88, "duration": 7.52}, {"text": "And once again, the reason for this is", "start": 1410.799, "duration": 4.721}, {"text": "let's say in a hypothetical in a", "start": 1412.4, "duration": 5.36}, {"text": "hypothetical case, we don't have enough", "start": 1415.52, "duration": 3.92}, {"text": "space to store this model on one GPU.", "start": 1417.76, "duration": 3.52}, {"text": "We're going to do that. Okay? And we're", "start": 1419.44, "duration": 3.359}, {"text": "going to do the same for all our input", "start": 1421.28, "duration": 3.759}, {"text": "tensors because they the the tensors", "start": 1422.799, "duration": 3.521}, {"text": "that you input in PyTorch need to be on", "start": 1425.039, "duration": 3.76}, {"text": "the same device as the model itself. If", "start": 1426.32, "duration": 4.96}, {"text": "you think about it, um, if you have your", "start": 1428.799, "duration": 4.481}, {"text": "model on a GPU and all of your data on", "start": 1431.28, "duration": 4.72}, {"text": "the CPU, then you'll have to route all", "start": 1433.28, "duration": 4.56}, {"text": "the data to the GPU, do the calculation,", "start": 1436.0, "duration": 2.88}, {"text": "then route it back. This will be", "start": 1437.84, "duration": 2.719}, {"text": "incredibly inefficient and it might even", "start": 1438.88, "duration": 2.799}, {"text": "give you an error. I'm not sure how", "start": 1440.559, "duration": 3.761}, {"text": "PyTorch handles this if you give it a", "start": 1441.679, "duration": 6.081}, {"text": "mismatched input model pair. But to", "start": 1444.32, "duration": 5.44}, {"text": "avoid anything happening in terms of", "start": 1447.76, "duration": 3.6}, {"text": "mismatch, we're just going to do it", "start": 1449.76, "duration": 5.12}, {"text": "properly to begin with. So let's say", "start": 1451.36, "duration": 6.16}, {"text": "once again if CUDA um we're going to", "start": 1454.88, "duration": 4.159}, {"text": "just duplicate this because we need it", "start": 1457.52, "duration": 6.88}, {"text": "twice. Um tab tab and then if not CUDA", "start": 1459.039, "duration": 6.88}, {"text": "then we do nothing once again. So we", "start": 1464.4, "duration": 3.44}, {"text": "don't need to do anything. So let's look", "start": 1465.919, "duration": 5.76}, {"text": "the input needs to go to device zero", "start": 1467.84, "duration": 5.92}, {"text": "because it starts on device zero in the", "start": 1471.679, "duration": 3.681}, {"text": "forward pass. It goes to the first layer", "start": 1473.76, "duration": 4.799}, {"text": "of the first uh GPU. And then the target", "start": 1475.36, "duration": 5.84}, {"text": "though if you remember is only ever seen", "start": 1478.559, "duration": 5.041}, {"text": "by part two, right? targets is here and", "start": 1481.2, "duration": 3.839}, {"text": "then part one never has it because it", "start": 1483.6, "duration": 2.88}, {"text": "doesn't calculate the loss. So it", "start": 1485.039, "duration": 4.0}, {"text": "actually goes straight to the second GPU", "start": 1486.48, "duration": 5.6}, {"text": "and this is done here. Now let's go to", "start": 1489.039, "duration": 5.52}, {"text": "the next todo in our training loop. All", "start": 1492.08, "duration": 4.24}, {"text": "right here it says device switch. What", "start": 1494.559, "duration": 3.521}, {"text": "does it mean by device switch? What it", "start": 1496.32, "duration": 6.32}, {"text": "means is that if we are using a GPU then", "start": 1498.08, "duration": 7.44}, {"text": "once we get our activations from part", "start": 1502.64, "duration": 6.32}, {"text": "one which is the output of the forward", "start": 1505.52, "duration": 5.84}, {"text": "so return self.net X then we need to", "start": 1508.96, "duration": 4.88}, {"text": "move this to the second GPU if we are", "start": 1511.36, "duration": 6.96}, {"text": "using GPUs. So let's do that right now.", "start": 1513.84, "duration": 6.88}, {"text": "If CUDA", "start": 1518.32, "duration": 4.959}, {"text": "we'll do hidden", "start": 1520.72, "duration": 5.28}, {"text": "dot to I think we have it in our", "start": 1523.279, "duration": 5.121}, {"text": "clipboard and then we'll change it to uh", "start": 1526.0, "duration": 4.24}, {"text": "torch.cuda.device", "start": 1528.4, "duration": 4.56}, {"text": "1. Okay, now that this is done and then", "start": 1530.24, "duration": 5.12}, {"text": "we'll also do something called retain", "start": 1532.96, "duration": 4.0}, {"text": "grad. So I'm just going to show you guys", "start": 1535.36, "duration": 3.12}, {"text": "right now what I mean. I'm going to do", "start": 1536.96, "duration": 4.48}, {"text": "hidden. retain_grad", "start": 1538.48, "duration": 5.76}, {"text": "and what is this doing?", "start": 1541.44, "duration": 6.719}, {"text": "So um here you can see I have a print", "start": 1544.24, "duration": 8.319}, {"text": "statement which prints the requires grad", "start": 1548.159, "duration": 8.321}, {"text": "element of hidden the grad itself if um", "start": 1552.559, "duration": 7.441}, {"text": "whether or not it's none and then the", "start": 1556.48, "duration": 6.4}, {"text": "grad itself. So what we need to know is", "start": 1560.0, "duration": 5.44}, {"text": "that in pietorrch", "start": 1562.88, "duration": 5.76}, {"text": "there are certain tensors which have", "start": 1565.44, "duration": 5.28}, {"text": "requires grad equals true and certain", "start": 1568.64, "duration": 6.0}, {"text": "that don't. As a general rule any neural", "start": 1570.72, "duration": 7.36}, {"text": "network parameter has grad requires grad", "start": 1574.64, "duration": 6.159}, {"text": "equals to true because you want to store", "start": 1578.08, "duration": 5.52}, {"text": "the gradient such that you can update", "start": 1580.799, "duration": 5.36}, {"text": "all of the weights during the backward", "start": 1583.6, "duration": 4.959}, {"text": "pass. And to that end hidden actually", "start": 1586.159, "duration": 4.081}, {"text": "will also have requires grad equals", "start": 1588.559, "duration": 4.0}, {"text": "true. So I just realized we've done the", "start": 1590.24, "duration": 4.24}, {"text": "implementation. So let me first run this", "start": 1592.559, "duration": 4.0}, {"text": "once and then I'll explain what hidden", "start": 1594.48, "duration": 4.48}, {"text": "retain grad does. So let's get out of", "start": 1596.559, "duration": 6.641}, {"text": "the torch interpreter and we will run my", "start": 1598.96, "duration": 6.24}, {"text": "work and if we get a bug that will be", "start": 1603.2, "duration": 3.359}, {"text": "cool because we can debug it in real", "start": 1605.2, "duration": 2.719}, {"text": "time. But I don't know. I'm kind of", "start": 1606.559, "duration": 3.201}, {"text": "confident. So let's see what happens. Oh", "start": 1607.919, "duration": 5.921}, {"text": "okay. Ah, this is amazing because I kind", "start": 1609.76, "duration": 8.48}, {"text": "of left this bug as a booby trap because", "start": 1613.84, "duration": 7.68}, {"text": "if we look at manual.py", "start": 1618.24, "duration": 4.799}, {"text": "in", "start": 1621.52, "duration": 4.639}, {"text": "our solution, what do we have? Oh,", "start": 1623.039, "duration": 4.161}, {"text": "actually no, let's not look at the", "start": 1626.159, "duration": 2.4}, {"text": "solution yet because I don't want to", "start": 1627.2, "duration": 4.079}, {"text": "spoil it. If we look at the monolith, we", "start": 1628.559, "duration": 5.281}, {"text": "have optimizer equals optim atom", "start": 1631.279, "duration": 5.52}, {"text": "model.parameters and that's it. So first", "start": 1633.84, "duration": 4.959}, {"text": "of all we know that we are missing", "start": 1636.799, "duration": 3.041}, {"text": "something because we don't actually", "start": 1638.799, "duration": 3.441}, {"text": "select the par parameters but also", "start": 1639.84, "duration": 4.4}, {"text": "there's something else. So first of all", "start": 1642.24, "duration": 4.24}, {"text": "let's just add the parameters because um", "start": 1644.24, "duration": 3.6}, {"text": "currently we're just giving it the n", "start": 1646.48, "duration": 2.88}, {"text": "sequential itself and the optimizer", "start": 1647.84, "duration": 3.36}, {"text": "doesn't know what to do with it. So", "start": 1649.36, "duration": 3.12}, {"text": "where are we actually initializing our", "start": 1651.2, "duration": 3.76}, {"text": "optimizer right here. So let's do", "start": 1652.48, "duration": 4.4}, {"text": "dotparameters", "start": 1654.96, "duration": 3.92}, {"text": "and we need to call this and then we", "start": 1656.88, "duration": 4.88}, {"text": "also need to convert these into lists", "start": 1658.88, "duration": 4.56}, {"text": "both of them because we're adding them", "start": 1661.76, "duration": 4.48}, {"text": "and um yeah you can't actually I would", "start": 1663.44, "duration": 4.16}, {"text": "like to see what the error is because I", "start": 1666.24, "duration": 3.039}, {"text": "didn't ever check this for myself but", "start": 1667.6, "duration": 4.0}, {"text": "you can't add parameters", "start": 1669.279, "duration": 4.081}, {"text": "um between because they're generators", "start": 1671.6, "duration": 3.76}, {"text": "you can't add them to each other but now", "start": 1673.36, "duration": 3.439}, {"text": "if we convert these generators into", "start": 1675.36, "duration": 2.96}, {"text": "lists which will just iterate over the", "start": 1676.799, "duration": 3.841}, {"text": "entire generator and then give us all of", "start": 1678.32, "duration": 4.64}, {"text": "the output parameters of each of these", "start": 1680.64, "duration": 4.639}, {"text": "two neural networks works then it should", "start": 1682.96, "duration": 4.4}, {"text": "work", "start": 1685.279, "duration": 4.161}, {"text": "unless we do another thing wrong. Okay,", "start": 1687.36, "duration": 3.919}, {"text": "I have a comma here. Why do we have a", "start": 1689.44, "duration": 4.0}, {"text": "comma here? I don't actually know why.", "start": 1691.279, "duration": 3.52}, {"text": "Ah, it's because the comma should be", "start": 1693.44, "duration": 3.68}, {"text": "there. Okay,", "start": 1694.799, "duration": 4.721}, {"text": "let's see. All right, so it works now.", "start": 1697.12, "duration": 5.2}, {"text": "And so the optimizer is fine. Now this", "start": 1699.52, "duration": 5.279}, {"text": "is the the kind of trap that's in the", "start": 1702.32, "duration": 5.68}, {"text": "code. So we can first of all see", "start": 1704.799, "duration": 4.48}, {"text": "something really cool which is the fact", "start": 1708.0, "duration": 4.799}, {"text": "that the final loss here is the same as", "start": 1709.279, "duration": 5.76}, {"text": "the loss for monolith. So let's look if", "start": 1712.799, "duration": 4.961}, {"text": "you forgot um it's the same. And in", "start": 1715.039, "duration": 4.481}, {"text": "terms of the time we can run it once", "start": 1717.76, "duration": 3.2}, {"text": "more time once more to see how long it", "start": 1719.52, "duration": 3.2}, {"text": "takes. I wouldn't really bet I wouldn't", "start": 1720.96, "duration": 3.52}, {"text": "really put too much um weight into the", "start": 1722.72, "duration": 3.04}, {"text": "timing here because once again you can", "start": 1724.48, "duration": 4.16}, {"text": "see here it was actually 0.1 seconds", "start": 1725.76, "duration": 5.36}, {"text": "faster than the first run. And my guess", "start": 1728.64, "duration": 6.32}, {"text": "is that it's cached some values in my", "start": 1731.12, "duration": 6.4}, {"text": "Mac and makes it go slightly faster the", "start": 1734.96, "duration": 5.04}, {"text": "second time. But in any case, what's", "start": 1737.52, "duration": 3.68}, {"text": "worth noting is that since we do have", "start": 1740.0, "duration": 3.6}, {"text": "the manual seed set up to 42, then", "start": 1741.2, "duration": 4.32}, {"text": "everything is great in terms of the", "start": 1743.6, "duration": 3.76}, {"text": "reproducibility.", "start": 1745.52, "duration": 4.8}, {"text": "But more important than that, let's come", "start": 1747.36, "duration": 4.72}, {"text": "back to these print statements. So let's", "start": 1750.32, "duration": 5.04}, {"text": "see what happened here. We had let's", "start": 1752.08, "duration": 5.839}, {"text": "first try it with retain grade grad", "start": 1755.36, "duration": 3.919}, {"text": "equals false because you don't even know", "start": 1757.919, "duration": 3.041}, {"text": "what this means yet. So we'll just see", "start": 1759.279, "duration": 5.76}, {"text": "what the what the um output is without", "start": 1760.96, "duration": 8.079}, {"text": "anything colluding it. So first of all", "start": 1765.039, "duration": 6.88}, {"text": "we have print hidden. grad which is", "start": 1769.039, "duration": 5.041}, {"text": "true. And the reason why this is true is", "start": 1771.919, "duration": 7.201}, {"text": "because when we have a output to of a", "start": 1774.08, "duration": 7.52}, {"text": "neural network module in the", "start": 1779.12, "duration": 7.52}, {"text": "computational graph the output of a", "start": 1781.6, "duration": 10.16}, {"text": "calculation which is itself created by", "start": 1786.64, "duration": 7.279}, {"text": "parameters which require grad will also", "start": 1791.76, "duration": 5.36}, {"text": "require grad. This is just how um the", "start": 1793.919, "duration": 5.681}, {"text": "require grad inherits from its children", "start": 1797.12, "duration": 4.48}, {"text": "essentially in PyTorch. So this will", "start": 1799.6, "duration": 4.48}, {"text": "always be true for hidden. We don't have", "start": 1801.6, "duration": 4.16}, {"text": "to worry about that. And what this means", "start": 1804.08, "duration": 6.24}, {"text": "is that when we do loss backward,", "start": 1805.76, "duration": 7.36}, {"text": "we don't have to worry about the", "start": 1810.32, "duration": 6.079}, {"text": "gradients not flowing from part two to", "start": 1813.12, "duration": 6.4}, {"text": "part one. And now I'm going to check", "start": 1816.399, "duration": 4.801}, {"text": "something which I don't actually know if", "start": 1819.52, "duration": 3.92}, {"text": "it'll work, but I'm still going to try", "start": 1821.2, "duration": 5.12}, {"text": "it. And what I'm going to try is to say", "start": 1823.44, "duration": 5.119}, {"text": "hidden.requires", "start": 1826.32, "duration": 3.76}, {"text": "grad equals false and see what happens.", "start": 1828.559, "duration": 2.72}, {"text": "I haven't tried this myself, but it just", "start": 1830.08, "duration": 4.079}, {"text": "came into my mind. In theory, this", "start": 1831.279, "duration": 5.441}, {"text": "should break the connection between part", "start": 1834.159, "duration": 5.52}, {"text": "one and part two um because now we will", "start": 1836.72, "duration": 5.439}, {"text": "no longer be storing the gradients um", "start": 1839.679, "duration": 4.401}, {"text": "between the two. But let's see what", "start": 1842.159, "duration": 3.921}, {"text": "actually happens in the code. Okay,", "start": 1844.08, "duration": 3.28}, {"text": "let's see what we'll see what it says.", "start": 1846.08, "duration": 2.64}, {"text": "Runtime error. You can only change", "start": 1847.36, "duration": 2.88}, {"text": "required to grad flags of leaf", "start": 1848.72, "duration": 2.959}, {"text": "variables. If you want to use a computer", "start": 1850.24, "duration": 2.48}, {"text": "variable in subgraph that doesn't", "start": 1851.679, "duration": 3.6}, {"text": "require differentiences, use ver nograd", "start": 1852.72, "duration": 7.4}, {"text": "equals ver detach. Okay. Um", "start": 1855.279, "duration": 4.841}, {"text": "I see. Okay. Um", "start": 1860.32, "duration": 4.44}, {"text": "essentially it's telling us that we", "start": 1866.799, "duration": 3.521}, {"text": "should detach hidden which will remove", "start": 1868.0, "duration": 3.52}, {"text": "it from the computational graph. And", "start": 1870.32, "duration": 3.359}, {"text": "let's see what happens now. I think it", "start": 1871.52, "duration": 6.44}, {"text": "will give us an error. Okay.", "start": 1873.679, "duration": 4.281}, {"text": "So the error we actually got is", "start": 1884.64, "duration": 4.399}, {"text": "incorrect because I forgot to change", "start": 1886.32, "duration": 5.52}, {"text": "hidden equals hidden. Instead of saying", "start": 1889.039, "duration": 4.88}, {"text": "hidden requires grad because um we want", "start": 1891.84, "duration": 4.8}, {"text": "to change the entire tensor. And here", "start": 1893.919, "duration": 6.961}, {"text": "okay we can see that exactly as we pro", "start": 1896.64, "duration": 6.399}, {"text": "prophesized", "start": 1900.88, "duration": 4.399}, {"text": "um the training loop is messed up. What", "start": 1903.039, "duration": 5.52}, {"text": "I'm trying to say here is that the back", "start": 1905.279, "duration": 5.76}, {"text": "um the loss is not propagating backwards", "start": 1908.559, "duration": 7.521}, {"text": "from our model in um part two to part", "start": 1911.039, "duration": 7.52}, {"text": "one. And this is because we detach the", "start": 1916.08, "duration": 7.36}, {"text": "model here. And now if I print hidden", "start": 1918.559, "duration": 7.36}, {"text": "requires grad, what happened? Um", "start": 1923.44, "duration": 4.56}, {"text": "hidden.requires grad, we'll see that it", "start": 1925.919, "duration": 6.321}, {"text": "should be false. And this means that", "start": 1928.0, "duration": 6.32}, {"text": "when we actually do back propagation,", "start": 1932.24, "duration": 3.6}, {"text": "there's no learning happening in part", "start": 1934.32, "duration": 3.68}, {"text": "one. So let's just run this once and see", "start": 1935.84, "duration": 4.88}, {"text": "what what it says. Um the Bula object is", "start": 1938.0, "duration": 4.32}, {"text": "not callable. That is that is true. It's", "start": 1940.72, "duration": 3.6}, {"text": "not callable.", "start": 1942.32, "duration": 4.0}, {"text": "Okay, it's still not callable. Did it", "start": 1944.32, "duration": 3.92}, {"text": "not update my code? Print", "start": 1946.32, "duration": 4.92}, {"text": "hidden.requires_grad.", "start": 1948.24, "duration": 3.0}, {"text": "Um okay, boom. Yeah, I I printed way too", "start": 1951.279, "duration": 4.64}, {"text": "many times. You can see that's false.", "start": 1954.159, "duration": 4.161}, {"text": "And at this point, what this means is", "start": 1955.919, "duration": 3.441}, {"text": "that somewhere in the computational", "start": 1958.32, "duration": 2.479}, {"text": "graph, PyTorch is kind of just fumbling", "start": 1959.36, "duration": 2.72}, {"text": "around. Doesn't really know what to do", "start": 1960.799, "duration": 3.12}, {"text": "because we detached. And when we detach", "start": 1962.08, "duration": 5.04}, {"text": "a tensor, um, it loses its gradients and", "start": 1963.919, "duration": 5.441}, {"text": "sets requires grad equals to false as", "start": 1967.12, "duration": 3.84}, {"text": "you see here. So, this was just a mini", "start": 1969.36, "duration": 3.039}, {"text": "experiment, which is really cool. Now,", "start": 1970.96, "duration": 3.36}, {"text": "let's get back to the subject of step", "start": 1972.399, "duration": 5.361}, {"text": "one, which was if we remind ourselves to", "start": 1974.32, "duration": 6.0}, {"text": "calculate with part one and part two and", "start": 1977.76, "duration": 6.32}, {"text": "get back this final loss. But even", "start": 1980.32, "duration": 6.4}, {"text": "though requires grad equals true in this", "start": 1984.08, "duration": 6.16}, {"text": "case what we notice is that we have our", "start": 1986.72, "duration": 6.559}, {"text": "gradient um equal to none which means", "start": 1990.24, "duration": 5.12}, {"text": "it's false and also we have no hidden", "start": 1993.279, "duration": 4.88}, {"text": "grad and this is because after the", "start": 1995.36, "duration": 6.64}, {"text": "backward pass python uh rather pytorch", "start": 1998.159, "duration": 6.64}, {"text": "no longer needs the gradients of", "start": 2002.0, "duration": 4.88}, {"text": "intermediate activations which hidden is", "start": 2004.799, "duration": 4.401}, {"text": "in this case. So it will empty them to", "start": 2006.88, "duration": 4.24}, {"text": "save memory which is really good. But", "start": 2009.2, "duration": 4.24}, {"text": "what we need to know is that when we", "start": 2011.12, "duration": 4.24}, {"text": "actually do pipeline parallelism here,", "start": 2013.44, "duration": 4.719}, {"text": "we will be moving um with a", "start": 2015.36, "duration": 4.4}, {"text": "communication primitive which is just", "start": 2018.159, "duration": 5.12}, {"text": "called dist. send and disc.receive in", "start": 2019.76, "duration": 6.08}, {"text": "the message par passing interface which", "start": 2023.279, "duration": 6.24}, {"text": "is just the python pietorch sorry way of", "start": 2025.84, "duration": 5.52}, {"text": "communicating between different models", "start": 2029.519, "duration": 6.481}, {"text": "and devices. In this case,", "start": 2031.36, "duration": 7.76}, {"text": "we will need to send our hidden", "start": 2036.0, "duration": 5.12}, {"text": "activations between the two models", "start": 2039.12, "duration": 3.76}, {"text": "because the activations are used in", "start": 2041.12, "duration": 4.159}, {"text": "order to calculate the partial", "start": 2042.88, "duration": 4.32}, {"text": "derivative of the loss with respect to", "start": 2045.279, "duration": 5.201}, {"text": "those activations aka inputs so that we", "start": 2047.2, "duration": 5.52}, {"text": "can propagate them back uh so that we", "start": 2050.48, "duration": 4.0}, {"text": "can propagate the locks backwards", "start": 2052.72, "duration": 5.199}, {"text": "through the graph. And even though we", "start": 2054.48, "duration": 6.48}, {"text": "don't need to set retain grad to true", "start": 2057.919, "duration": 5.281}, {"text": "for this network to learn, it will just", "start": 2060.96, "duration": 4.719}, {"text": "print what the model is actually sending", "start": 2063.2, "duration": 6.0}, {"text": "backwards. So let's come um here. And", "start": 2065.679, "duration": 5.121}, {"text": "now we can see that once we say ret", "start": 2069.2, "duration": 3.52}, {"text": "retain gradients true, it actually", "start": 2070.8, "duration": 6.24}, {"text": "retains it in the um hidden.grad", "start": 2072.72, "duration": 8.24}, {"text": "element um of our tensor and it is just", "start": 2077.04, "duration": 5.52}, {"text": "here. So first of all, you can see", "start": 2080.96, "duration": 3.36}, {"text": "hidden grad is not none. So it's true.", "start": 2082.56, "duration": 3.599}, {"text": "And this tensor are the gradients of", "start": 2084.32, "duration": 4.88}, {"text": "hidden which in the pipeline parallelism", "start": 2086.159, "duration": 5.601}, {"text": "that we'll be implementing very soon", "start": 2089.2, "duration": 5.6}, {"text": "will be sent backwards from part two to", "start": 2091.76, "duration": 5.28}, {"text": "part one. And if you want to know what's", "start": 2094.8, "duration": 4.16}, {"text": "sent forwards from part two to part one", "start": 2097.04, "duration": 4.64}, {"text": "then we can just print hidden itself.", "start": 2098.96, "duration": 4.24}, {"text": "So", "start": 2101.68, "duration": 3.6}, {"text": "we can see that this is the vector that", "start": 2103.2, "duration": 4.0}, {"text": "is sent in the forward pass. It's the", "start": 2105.28, "duration": 4.079}, {"text": "actual activations of hidden just this", "start": 2107.2, "duration": 6.24}, {"text": "because um we can see that when we input", "start": 2109.359, "duration": 5.441}, {"text": "our hidden vector into part two these", "start": 2113.44, "duration": 3.04}, {"text": "are the activations of itself. The input", "start": 2114.8, "duration": 4.319}, {"text": "to the second model and then the", "start": 2116.48, "duration": 5.119}, {"text": "gradient which is in the backwards", "start": 2119.119, "duration": 4.401}, {"text": "direction is something else because the", "start": 2121.599, "duration": 4.881}, {"text": "gradients are the same dimension as our", "start": 2123.52, "duration": 7.12}, {"text": "um hidden um input activations but they", "start": 2126.48, "duration": 7.2}, {"text": "are instead telling the model how to", "start": 2130.64, "duration": 4.24}, {"text": "change the weights of our model in order", "start": 2133.68, "duration": 3.04}, {"text": "to decrease the loss. And one last thing", "start": 2134.88, "duration": 3.6}, {"text": "that I'm going to do is calculate the", "start": 2136.72, "duration": 3.44}, {"text": "shape just to show you guys that the", "start": 2138.48, "duration": 4.4}, {"text": "gradients and the model itself have the", "start": 2140.16, "duration": 4.08}, {"text": "same shape. So let's look at this real", "start": 2142.88, "duration": 4.719}, {"text": "quick together. Um 32128 32128. So there", "start": 2144.24, "duration": 6.32}, {"text": "you go. We have now implemented the step", "start": 2147.599, "duration": 7.76}, {"text": "one manual which all it does is make on", "start": 2150.56, "duration": 8.96}, {"text": "a CPU two fake models and splits them up", "start": 2155.359, "duration": 7.841}, {"text": "um in the middle and then does the exact", "start": 2159.52, "duration": 5.839}, {"text": "same loose uh does the exact same loop", "start": 2163.2, "duration": 4.48}, {"text": "sorry as manual.py which is right here.", "start": 2165.359, "duration": 4.24}, {"text": "And if you actually have a CUDA GPU then", "start": 2167.68, "duration": 4.72}, {"text": "you can do this for real with all of the", "start": 2169.599, "duration": 4.641}, {"text": "device management which is really cool.", "start": 2172.4, "duration": 4.4}, {"text": "So if you do then please let us know how", "start": 2174.24, "duration": 4.24}, {"text": "that goes in the comment section. And", "start": 2176.8, "duration": 5.84}, {"text": "now we can move on to step two. So now", "start": 2178.48, "duration": 6.08}, {"text": "that we've done the manual we can see", "start": 2182.64, "duration": 3.68}, {"text": "that even on one machine you still have", "start": 2184.56, "duration": 3.36}, {"text": "to manage the handoff of the activation", "start": 2186.32, "duration": 3.84}, {"text": "and the gradients. And just to highlight", "start": 2187.92, "duration": 4.24}, {"text": "where that happens um the handoff of the", "start": 2190.16, "duration": 3.84}, {"text": "activations happens here. And then in", "start": 2192.16, "duration": 3.52}, {"text": "fact the handoff of the gradients is", "start": 2194.0, "duration": 4.16}, {"text": "done by backward. So we don't really", "start": 2195.68, "duration": 4.08}, {"text": "have to handle that. So I should", "start": 2198.16, "duration": 4.32}, {"text": "probably modify the outline to rectify", "start": 2199.76, "duration": 5.52}, {"text": "that error. And now we've done that,", "start": 2202.48, "duration": 6.639}, {"text": "let's talk about distributed basics. So", "start": 2205.28, "duration": 6.0}, {"text": "in this stage of the course, we're not", "start": 2209.119, "duration": 3.681}, {"text": "so much going to be implementing", "start": 2211.28, "duration": 5.04}, {"text": "pipeline parallelism as implementing the", "start": 2212.8, "duration": 6.799}, {"text": "communication primitives which are in", "start": 2216.32, "duration": 5.759}, {"text": "this case the send tensor and the", "start": 2219.599, "duration": 5.201}, {"text": "receive tensor functions so that we can", "start": 2222.079, "duration": 5.361}, {"text": "actually communicate between our", "start": 2224.8, "duration": 5.44}, {"text": "different GPUs and send the weights", "start": 2227.44, "duration": 4.56}, {"text": "between them. Okay. Okay, so the", "start": 2230.24, "duration": 3.359}, {"text": "distributed basics comprises three", "start": 2232.0, "duration": 3.44}, {"text": "things. Rank, world size and process", "start": 2233.599, "duration": 3.841}, {"text": "group. Let's go from bottom to top. The", "start": 2235.44, "duration": 5.04}, {"text": "rank is just the ID of your GPU. So if", "start": 2237.44, "duration": 5.44}, {"text": "we have four GPUs, we have ID 0123.", "start": 2240.48, "duration": 3.68}, {"text": "That's simple. The world size is the", "start": 2242.88, "duration": 3.199}, {"text": "number of GPUs. Very simple as well. But", "start": 2244.16, "duration": 4.72}, {"text": "then there's also the process group", "start": 2246.079, "duration": 6.161}, {"text": "which is um used in PyTorch in order to", "start": 2248.88, "duration": 7.76}, {"text": "establish the group which inside of", "start": 2252.24, "duration": 6.8}, {"text": "which we will have our communication. So", "start": 2256.64, "duration": 4.88}, {"text": "this may or may not make sense as of", "start": 2259.04, "duration": 5.92}, {"text": "right now but once we implement it then", "start": 2261.52, "duration": 7.12}, {"text": "it will be much clearer. So let's come", "start": 2264.96, "duration": 8.96}, {"text": "and open up our step two comps and get", "start": 2268.64, "duration": 8.56}, {"text": "into the implementation. So let's look", "start": 2273.92, "duration": 4.56}, {"text": "at the function that we have. First of", "start": 2277.2, "duration": 4.0}, {"text": "all we have first of all an init", "start": 2278.48, "duration": 5.2}, {"text": "distributed which initiize initializes", "start": 2281.2, "duration": 4.32}, {"text": "the distributed process group. Once", "start": 2283.68, "duration": 5.6}, {"text": "again, this is the stage where we have", "start": 2285.52, "duration": 7.76}, {"text": "to call init process group and", "start": 2289.28, "duration": 6.319}, {"text": "initialize the communication or the", "start": 2293.28, "duration": 4.0}, {"text": "conference call if you'd like to say it", "start": 2295.599, "duration": 3.921}, {"text": "that way. And then if we come back to", "start": 2297.28, "duration": 4.24}, {"text": "step two comms, we read the state", "start": 2299.52, "duration": 4.079}, {"text": "directly um from the environment", "start": 2301.52, "duration": 5.44}, {"text": "variables set by torch run as well. And", "start": 2303.599, "duration": 5.281}, {"text": "this is already done for us because it's", "start": 2306.96, "duration": 3.36}, {"text": "not really interesting to mount this on", "start": 2308.88, "duration": 4.479}, {"text": "yourself on your own. So what is torch", "start": 2310.32, "duration": 6.0}, {"text": "run? First of all, if I just come here", "start": 2313.359, "duration": 6.0}, {"text": "and say UV run torch run, we'll", "start": 2316.32, "duration": 4.64}, {"text": "obviously get an error because we", "start": 2319.359, "duration": 4.401}, {"text": "haven't given any arguments. But torch", "start": 2320.96, "duration": 8.48}, {"text": "run is how you execute distributed code", "start": 2323.76, "duration": 8.24}, {"text": "in PyTorch. Which is to say, as soon as", "start": 2329.44, "duration": 4.48}, {"text": "you have more than one GPU, you can no", "start": 2332.0, "duration": 5.44}, {"text": "longer run a single file main.py across", "start": 2333.92, "duration": 5.439}, {"text": "all of your GPUs. you actually need to", "start": 2337.44, "duration": 5.76}, {"text": "run one one um process of each file for", "start": 2339.359, "duration": 6.72}, {"text": "each device. So what torch run does in", "start": 2343.2, "duration": 7.36}, {"text": "in essence is it creates a copy of your", "start": 2346.079, "duration": 6.0}, {"text": "script for every single GPU and then", "start": 2350.56, "duration": 4.24}, {"text": "runs on that GPU. Okay. And this is why", "start": 2352.079, "duration": 5.201}, {"text": "it actually gives us a local rank. So", "start": 2354.8, "duration": 4.64}, {"text": "when you think of step two coms which is", "start": 2357.28, "duration": 4.0}, {"text": "going to give us our communication what", "start": 2359.44, "duration": 4.08}, {"text": "you should really think of is four", "start": 2361.28, "duration": 4.96}, {"text": "copies of this file for the four GPUs if", "start": 2363.52, "duration": 4.319}, {"text": "we're going to use this example which we", "start": 2366.24, "duration": 5.28}, {"text": "are and for this reason", "start": 2367.839, "duration": 6.481}, {"text": "we'll get local rank from 0 to three", "start": 2371.52, "duration": 6.079}, {"text": "inside of this init init distributed", "start": 2374.32, "duration": 6.799}, {"text": "method and furthermore we get our world", "start": 2377.599, "duration": 5.361}, {"text": "size and our rank and as the comment", "start": 2381.119, "duration": 4.72}, {"text": "says here once again it's set by torch", "start": 2382.96, "duration": 4.56}, {"text": "run. So these environment variables are", "start": 2385.839, "duration": 3.28}, {"text": "initialized by torch itself. We don't", "start": 2387.52, "duration": 3.68}, {"text": "have to worry about that. But now this", "start": 2389.119, "duration": 4.0}, {"text": "is done. We still have work to do which", "start": 2391.2, "duration": 5.28}, {"text": "is to say um set up our device. If we're", "start": 2393.119, "duration": 4.72}, {"text": "using a GPU then we need to set our", "start": 2396.48, "duration": 4.639}, {"text": "device to CUDA. If not then it should be", "start": 2397.839, "duration": 5.841}, {"text": "CPU. So the best way to do this is", "start": 2401.119, "duration": 5.201}, {"text": "device equals", "start": 2403.68, "duration": 5.64}, {"text": "CUDA", "start": 2406.32, "duration": 3.0}, {"text": "if", "start": 2409.44, "duration": 4.48}, {"text": "torch.CUDA CUDA", "start": 2410.96, "duration": 8.399}, {"text": "is available or I think we just do", "start": 2413.92, "duration": 10.159}, {"text": "is unavailable. There you go. And then", "start": 2419.359, "duration": 8.641}, {"text": "it will be CPU otherwise.", "start": 2424.079, "duration": 6.161}, {"text": "So one thing that I didn't mention is", "start": 2428.0, "duration": 6.72}, {"text": "that we should specify the device itself", "start": 2430.24, "duration": 6.8}, {"text": "and we have local rank. So very", "start": 2434.72, "duration": 4.48}, {"text": "conveniently we can just specify local", "start": 2437.04, "duration": 5.36}, {"text": "rank here and we also need to add a", "start": 2439.2, "duration": 5.919}, {"text": "colon. So in essence when you have a", "start": 2442.4, "duration": 6.16}, {"text": "device and multiple GPUs in PyTorch you", "start": 2445.119, "duration": 5.841}, {"text": "just specify which device it is by CUDA", "start": 2448.56, "duration": 5.2}, {"text": "comma 0 for the zero GPU CUDA comma uh", "start": 2450.96, "duration": 5.04}, {"text": "sorry CUDA colon one for the one GPU. So", "start": 2453.76, "duration": 3.599}, {"text": "we're just going to do this and in this", "start": 2456.0, "duration": 2.56}, {"text": "code we're just going to assume that you", "start": 2457.359, "duration": 3.76}, {"text": "have four GPUs. Um we can also add a", "start": 2458.56, "duration": 4.72}, {"text": "condition if the device count is less", "start": 2461.119, "duration": 3.841}, {"text": "than or equal to four. If it's less than", "start": 2463.28, "duration": 3.52}, {"text": "four actually then return an error but", "start": 2464.96, "duration": 3.44}, {"text": "let's just keep the code simple for now", "start": 2466.8, "duration": 4.08}, {"text": "and then we'll initialize the group. So", "start": 2468.4, "duration": 6.32}, {"text": "once again it's going to be dist.init", "start": 2470.88, "duration": 5.199}, {"text": "process group. So let's look at the", "start": 2474.72, "duration": 4.0}, {"text": "let's look at the method um description", "start": 2476.079, "duration": 4.161}, {"text": "here", "start": 2478.72, "duration": 5.119}, {"text": "and see what it says. Okay, this doesn't", "start": 2480.24, "duration": 8.4}, {"text": "seem too um easy to read. I don't know", "start": 2483.839, "duration": 7.841}, {"text": "where the uh there you go. It went. So", "start": 2488.64, "duration": 4.88}, {"text": "it initialized the default distributed", "start": 2491.68, "duration": 3.28}, {"text": "process group. This will also initialize", "start": 2493.52, "duration": 3.2}, {"text": "a distributed package. There are two", "start": 2494.96, "duration": 2.96}, {"text": "main ways to initialize a process group", "start": 2496.72, "duration": 3.04}, {"text": "etc etc. We already know how to do this.", "start": 2497.92, "duration": 4.159}, {"text": "Um in essence okay it doesn't have any", "start": 2499.76, "duration": 4.72}, {"text": "examples but in essence this is what's", "start": 2502.079, "duration": 4.721}, {"text": "going to establish our communication. Um", "start": 2504.48, "duration": 3.359}, {"text": "you can think of it as the conference", "start": 2506.8, "duration": 3.92}, {"text": "call between our different GPUs. So the", "start": 2507.839, "duration": 6.641}, {"text": "back end will depend on the type of", "start": 2510.72, "duration": 5.76}, {"text": "devices that we're using. So here we", "start": 2514.48, "duration": 5.52}, {"text": "actually need to check again if we have", "start": 2516.48, "duration": 5.839}, {"text": "our uh device if if we have GPUs", "start": 2520.0, "duration": 4.0}, {"text": "available and then if not we're going to", "start": 2522.319, "duration": 4.081}, {"text": "do something else. So here and then else", "start": 2524.0, "duration": 3.599}, {"text": "and we're going to do a different type", "start": 2526.4, "duration": 3.919}, {"text": "of dist distributed um init process", "start": 2527.599, "duration": 6.081}, {"text": "group call. So", "start": 2530.319, "duration": 5.76}, {"text": "if we have GPUs, which is the case here,", "start": 2533.68, "duration": 4.399}, {"text": "then we're going to use the NCCL back", "start": 2536.079, "duration": 3.921}, {"text": "end, which stands for the Nvidia", "start": 2538.079, "duration": 3.441}, {"text": "collective communications library. And", "start": 2540.0, "duration": 3.359}, {"text": "this is just how GPUs communicate with", "start": 2541.52, "duration": 4.559}, {"text": "each other. And then the second argument", "start": 2543.359, "duration": 6.561}, {"text": "is what the init method, which we're not", "start": 2546.079, "duration": 5.76}, {"text": "going to use. We're going to use the", "start": 2549.92, "duration": 3.76}, {"text": "first way, which is to specify store,", "start": 2551.839, "duration": 3.681}, {"text": "rank, and world size. And store here is", "start": 2553.68, "duration": 4.48}, {"text": "the type of communication um back end", "start": 2555.52, "duration": 6.4}, {"text": "that we're using. So, we just input rank", "start": 2558.16, "duration": 6.24}, {"text": "and world size here as we already got", "start": 2561.92, "duration": 5.439}, {"text": "them from torch.run and then world size.", "start": 2564.4, "duration": 4.48}, {"text": "Boom.", "start": 2567.359, "duration": 2.801}, {"text": "And then I'm just going to copy paste", "start": 2568.88, "duration": 3.6}, {"text": "this here except we're going to use", "start": 2570.16, "duration": 4.72}, {"text": "glue. So, if you look back at the", "start": 2572.48, "duration": 3.52}, {"text": "different types of communication", "start": 2574.88, "duration": 3.04}, {"text": "backends that are supported, we have", "start": 2576.0, "duration": 6.8}, {"text": "MPI, glue, NCCL, UCCC, XCCL.", "start": 2577.92, "duration": 7.36}, {"text": "Notably, you'll see that MPS is not", "start": 2582.8, "duration": 4.16}, {"text": "supported. So metal performance shaders", "start": 2585.28, "duration": 5.36}, {"text": "which is used in silicon max. Um so even", "start": 2586.96, "duration": 5.92}, {"text": "me using my Mac I was unable to try this", "start": 2590.64, "duration": 5.12}, {"text": "on the GPU because we don't actually", "start": 2592.88, "duration": 4.32}, {"text": "support distributed communication yet in", "start": 2595.76, "duration": 4.559}, {"text": "PyTorch on MPS. So in this case even if", "start": 2597.2, "duration": 5.52}, {"text": "you have a Mac silicon chip you still", "start": 2600.319, "duration": 5.28}, {"text": "have to use a CPU with glue which is the", "start": 2602.72, "duration": 5.84}, {"text": "communication um platform that we have", "start": 2605.599, "duration": 6.161}, {"text": "to establish here. Okay. So now this is", "start": 2608.56, "duration": 4.559}, {"text": "actually everything we need to do in", "start": 2611.76, "duration": 2.72}, {"text": "order to initialize this distributed", "start": 2613.119, "duration": 3.361}, {"text": "process group. So let's give oursel a", "start": 2614.48, "duration": 4.48}, {"text": "pat on the back and we just return rank", "start": 2616.48, "duration": 4.639}, {"text": "world and size uh sorry rank world size", "start": 2618.96, "duration": 5.52}, {"text": "and device for later reference. And then", "start": 2621.119, "duration": 5.921}, {"text": "now in pipeline columns we have a few", "start": 2624.48, "duration": 6.72}, {"text": "things that we need to check. So", "start": 2627.04, "duration": 6.079}, {"text": "first of all, we have our init method", "start": 2631.2, "duration": 4.56}, {"text": "and we want to come ahead and save our", "start": 2633.119, "duration": 7.841}, {"text": "rank as well as our world size because", "start": 2635.76, "duration": 7.359}, {"text": "this is what you do in an init in method", "start": 2640.96, "duration": 5.52}, {"text": "in um in a few words. And then okay,", "start": 2643.119, "duration": 5.601}, {"text": "that's weird how it did world twice.", "start": 2646.48, "duration": 4.72}, {"text": "Anyways, there you go. And then what", "start": 2648.72, "duration": 5.92}, {"text": "we're going to do with these values is", "start": 2651.2, "duration": 5.6}, {"text": "say whether or not we are the last or", "start": 2654.64, "duration": 3.36}, {"text": "the first GPU because it's very", "start": 2656.8, "duration": 3.68}, {"text": "important. Why is this important? For", "start": 2658.0, "duration": 5.839}, {"text": "example, the first GPU does not send any", "start": 2660.48, "duration": 5.599}, {"text": "gradients to a previous GPU because", "start": 2663.839, "duration": 5.041}, {"text": "there is no GPU before it. And the last", "start": 2666.079, "duration": 5.201}, {"text": "GPU does not send any activations to", "start": 2668.88, "duration": 4.08}, {"text": "another GPU because there's nothing", "start": 2671.28, "duration": 6.319}, {"text": "after it. So we'll just say self.last.", "start": 2672.96, "duration": 6.08}, {"text": "I forget what we said in the official", "start": 2677.599, "duration": 2.881}, {"text": "library, but it doesn't really matter.", "start": 2679.04, "duration": 4.16}, {"text": "Uh, equals", "start": 2680.48, "duration": 5.52}, {"text": "self. It doesn't equal self. U, so if", "start": 2683.2, "duration": 5.44}, {"text": "you want to check if the GPU is the last", "start": 2686.0, "duration": 7.599}, {"text": "one, we'll just do um self do world size", "start": 2688.64, "duration": 10.56}, {"text": "and um then we'll take self.rank", "start": 2693.599, "duration": 8.081}, {"text": "and we'll say is this the is this the", "start": 2699.2, "duration": 4.96}, {"text": "same as self.orld size minus one. So if", "start": 2701.68, "duration": 4.639}, {"text": "we have four GPUs and the rank is three,", "start": 2704.16, "duration": 4.4}, {"text": "then three equals 4 minus one. And this", "start": 2706.319, "duration": 5.681}, {"text": "is when we want self.last to be true.", "start": 2708.56, "duration": 5.84}, {"text": "And I did say that we don't want to look", "start": 2712.0, "duration": 5.44}, {"text": "at um the coms.py sheet, but I really", "start": 2714.4, "duration": 4.48}, {"text": "want to quickly check what we called it.", "start": 2717.44, "duration": 3.679}, {"text": "Uh we called it previous rank. Okay. Uh", "start": 2718.88, "duration": 4.56}, {"text": "no, we didn't call it that.", "start": 2721.119, "duration": 5.601}, {"text": "Okay, we did call it that. So I just", "start": 2723.44, "duration": 5.52}, {"text": "realized we don't want to use self.last", "start": 2726.72, "duration": 4.8}, {"text": "just yet from seeing that. We're instead", "start": 2728.96, "duration": 4.639}, {"text": "gonna call them something else. And then", "start": 2731.52, "duration": 4.88}, {"text": "this will be much more useful. So", "start": 2733.599, "duration": 7.52}, {"text": "self.pre rank is going to be the value", "start": 2736.4, "duration": 8.4}, {"text": "of self.rank", "start": 2741.119, "duration": 10.2}, {"text": "minus 1 if self.rank", "start": 2744.8, "duration": 6.519}, {"text": "not equals", "start": 2751.599, "duration": 5.121}, {"text": "zero else none. So this is how we're", "start": 2753.52, "duration": 5.76}, {"text": "going to check if for example you're the", "start": 2756.72, "duration": 4.48}, {"text": "first GPU. If the previous rank is none,", "start": 2759.28, "duration": 3.039}, {"text": "that means that there's nothing before", "start": 2761.2, "duration": 5.119}, {"text": "you. And then now we can check with this", "start": 2762.319, "duration": 6.721}, {"text": "condition here. So sorry for that mixup,", "start": 2766.319, "duration": 6.641}, {"text": "but um it will just be self.orldus one", "start": 2769.04, "duration": 8.4}, {"text": "or um self.rank", "start": 2772.96, "duration": 7.84}, {"text": "if or self.rank + one", "start": 2777.44, "duration": 6.24}, {"text": "if the following condition is true. If", "start": 2780.8, "duration": 4.96}, {"text": "we have", "start": 2783.68, "duration": 5.36}, {"text": "this self.orld world size.", "start": 2785.76, "duration": 7.52}, {"text": "Um, shoot. Sorry for this um, blooper,", "start": 2789.04, "duration": 6.64}, {"text": "guys. I really need to copy this", "start": 2793.28, "duration": 7.2}, {"text": "instead. So, there you go. And since I", "start": 2795.68, "duration": 6.08}, {"text": "confused you guys slightly, let's just", "start": 2800.48, "duration": 2.8}, {"text": "go over what we just did. So, the", "start": 2801.76, "duration": 3.12}, {"text": "previous rank, oh, we need to call this", "start": 2803.28, "duration": 3.6}, {"text": "next rank.", "start": 2804.88, "duration": 4.16}, {"text": "The previous rank is equal to self rank", "start": 2806.88, "duration": 3.36}, {"text": "minus one, which makes sense. If you", "start": 2809.04, "duration": 2.559}, {"text": "rank one, then your previous rank is", "start": 2810.24, "duration": 2.96}, {"text": "zero. If you're not the LA, if you're", "start": 2811.599, "duration": 3.76}, {"text": "not the first GPU, otherwise it's none.", "start": 2813.2, "duration": 4.24}, {"text": "Here the next rank is the self.rank plus", "start": 2815.359, "duration": 5.76}, {"text": "one. If you're not the last GPU", "start": 2817.44, "duration": 6.96}, {"text": "um so we need to change this to not else", "start": 2821.119, "duration": 5.521}, {"text": "it's none. Okay so this is all set up", "start": 2824.4, "duration": 5.199}, {"text": "really well. Now we will come and", "start": 2826.64, "duration": 4.719}, {"text": "implement these four primitives. What", "start": 2829.599, "duration": 3.601}, {"text": "are the four primitives? First one is", "start": 2831.359, "duration": 3.681}, {"text": "send forward send of activations next", "start": 2833.2, "duration": 3.6}, {"text": "GPU. Next one is receive activations", "start": 2835.04, "duration": 4.079}, {"text": "from the previous GPU. After that we", "start": 2836.8, "duration": 4.48}, {"text": "have send gradients back to previous GPU", "start": 2839.119, "duration": 6.321}, {"text": "and receive gradients from the next GPU.", "start": 2841.28, "duration": 5.6}, {"text": "So what you notice here is that every", "start": 2845.44, "duration": 4.48}, {"text": "single function has a receive and a send", "start": 2846.88, "duration": 4.239}, {"text": "because if you send something to", "start": 2849.92, "duration": 2.72}, {"text": "somebody then you also need to have the", "start": 2851.119, "duration": 4.24}, {"text": "corresponding receive function and for", "start": 2852.64, "duration": 4.4}, {"text": "send forward and send backward. It's", "start": 2855.359, "duration": 3.521}, {"text": "really simple. We just come here and do", "start": 2857.04, "duration": 5.84}, {"text": "dist send and it will send a tensor sync", "start": 2858.88, "duration": 6.959}, {"text": "send a tensor synchronously and all we", "start": 2862.88, "duration": 4.959}, {"text": "need to specify is a tensor itself that", "start": 2865.839, "duration": 3.441}, {"text": "we want to send which is a argument of", "start": 2867.839, "duration": 2.561}, {"text": "our function so we don't even need to", "start": 2869.28, "duration": 2.64}, {"text": "specify it but more importantly we need", "start": 2870.4, "duration": 4.88}, {"text": "to specify the destination so that is", "start": 2871.92, "duration": 5.76}, {"text": "simply the ID of the GPU that we want to", "start": 2875.28, "duration": 6.559}, {"text": "send it to and let's just come here", "start": 2877.68, "duration": 9.04}, {"text": "input our tensor and then input self", "start": 2881.839, "duration": 7.441}, {"text": "next rank since we're sending to the", "start": 2886.72, "duration": 4.32}, {"text": "next GPU.", "start": 2889.28, "duration": 3.839}, {"text": "And one thing that I'll say right now is", "start": 2891.04, "duration": 3.44}, {"text": "that of course if you called this on the", "start": 2893.119, "duration": 3.281}, {"text": "last GPU, you'll get an error, but we're", "start": 2894.48, "duration": 4.16}, {"text": "going to add the conditional statement", "start": 2896.4, "duration": 4.32}, {"text": "not here, but in the function that calls", "start": 2898.64, "duration": 4.0}, {"text": "it. It will just make the logic slightly", "start": 2900.72, "duration": 4.639}, {"text": "simpler. So, and it will also make the", "start": 2902.64, "duration": 4.4}, {"text": "the code more readable. So, we're just", "start": 2905.359, "duration": 3.681}, {"text": "going to do that. With that being said,", "start": 2907.04, "duration": 3.6}, {"text": "we have finished our first function. So,", "start": 2909.04, "duration": 4.319}, {"text": "that's great. Receiving is slightly more", "start": 2910.64, "duration": 4.4}, {"text": "complicated. You'll see that we have not", "start": 2913.359, "duration": 4.72}, {"text": "only a sh a shape device but also a", "start": 2915.04, "duration": 5.92}, {"text": "dtype. So three parameters and this is", "start": 2918.079, "duration": 6.801}, {"text": "because we need to specify the tensor", "start": 2920.96, "duration": 5.76}, {"text": "which we are going to receive the data", "start": 2924.88, "duration": 4.239}, {"text": "onto. If you ever use maloc and c this", "start": 2926.72, "duration": 6.08}, {"text": "is kind of like a really tuned down", "start": 2929.119, "duration": 4.96}, {"text": "version of what we have to do with maloc", "start": 2932.8, "duration": 3.279}, {"text": "which is to say like specify a block in", "start": 2934.079, "duration": 5.441}, {"text": "memory. So we'll return a tensor um", "start": 2936.079, "duration": 6.561}, {"text": "first of all because when we receive a", "start": 2939.52, "duration": 6.12}, {"text": "tensor", "start": 2942.64, "duration": 3.0}, {"text": "from receive forward we want to actually", "start": 2945.839, "duration": 3.28}, {"text": "return it back to the function that", "start": 2948.079, "duration": 2.081}, {"text": "called it so they can actually get the", "start": 2949.119, "duration": 2.561}, {"text": "tensor. When we send something we don't", "start": 2950.16, "duration": 3.199}, {"text": "need to return anything because we're", "start": 2951.68, "duration": 3.52}, {"text": "just sending it a value. But when we", "start": 2953.359, "duration": 3.041}, {"text": "when we receive something we need to", "start": 2955.2, "duration": 3.28}, {"text": "return that value that we received. So", "start": 2956.4, "duration": 5.52}, {"text": "what we do here is tensor equals torch", "start": 2958.48, "duration": 5.76}, {"text": "dozer.", "start": 2961.92, "duration": 4.08}, {"text": "So we just want to initialize a vector", "start": 2964.24, "duration": 3.28}, {"text": "of zeros. You could do also torch ones", "start": 2966.0, "duration": 3.44}, {"text": "or torch.rand. It actually does not", "start": 2967.52, "duration": 7.039}, {"text": "matter. And then we will put the shape,", "start": 2969.44, "duration": 7.6}, {"text": "the shape,", "start": 2974.559, "duration": 7.76}, {"text": "the device, and the data type.", "start": 2977.04, "duration": 9.039}, {"text": "Okay. And though this will mean that the", "start": 2982.319, "duration": 5.441}, {"text": "um tensor will be mapped onto the proper", "start": 2986.079, "duration": 2.881}, {"text": "device. If you're using a GPU, it'll be", "start": 2987.76, "duration": 3.12}, {"text": "mapped on the GPU and then it will also", "start": 2988.96, "duration": 4.56}, {"text": "receive the correct data type.", "start": 2990.88, "duration": 5.28}, {"text": "But so far we all we did was return a", "start": 2993.52, "duration": 4.96}, {"text": "tensor of zeros. So now we need to do", "start": 2996.16, "duration": 4.72}, {"text": "distress", "start": 2998.48, "duration": 4.8}, {"text": "receive and what are the arguments to", "start": 3000.88, "duration": 3.76}, {"text": "this function? We have the tense that we", "start": 3003.28, "duration": 4.559}, {"text": "just made. So tensor", "start": 3004.64, "duration": 4.959}, {"text": "and then the source. So where is the", "start": 3007.839, "duration": 5.52}, {"text": "source? Since we're receiving forward,", "start": 3009.599, "duration": 4.96}, {"text": "we have to receive from the person", "start": 3013.359, "duration": 3.681}, {"text": "that's previous to us. So self previous", "start": 3014.559, "duration": 4.8}, {"text": "rank. All right. So now we have two", "start": 3017.04, "duration": 4.559}, {"text": "functions down. Let's go down to send", "start": 3019.359, "duration": 4.561}, {"text": "backward. This one will send gradients", "start": 3021.599, "duration": 4.161}, {"text": "back to the previous GPU. So we need to", "start": 3023.92, "duration": 4.8}, {"text": "send to the previous rank and in fact", "start": 3025.76, "duration": 5.28}, {"text": "this function is almost identical except", "start": 3028.72, "duration": 6.56}, {"text": "we will change next rank to pre rank. So", "start": 3031.04, "duration": 5.68}, {"text": "you can see that these functions are", "start": 3035.28, "duration": 3.36}, {"text": "yeah kind of mirrors of each other and", "start": 3036.72, "duration": 3.839}, {"text": "we can also just copy paste this to", "start": 3038.64, "duration": 5.439}, {"text": "receive range from the next GPU. So um", "start": 3040.559, "duration": 6.081}, {"text": "if I'm not mistaken what we need to", "start": 3044.079, "duration": 5.28}, {"text": "change here is just previous rank to", "start": 3046.64, "duration": 4.479}, {"text": "next rank and we'll go over it once to", "start": 3049.359, "duration": 3.281}, {"text": "make sure that we are doing it properly.", "start": 3051.119, "duration": 3.361}, {"text": "So what's happening here is that in", "start": 3052.64, "duration": 3.52}, {"text": "receive backwards we're receiving from", "start": 3054.48, "duration": 3.68}, {"text": "the next GPU we initialize our tensor to", "start": 3056.16, "duration": 4.959}, {"text": "zeros uh receive it from the previous GP", "start": 3058.16, "duration": 5.04}, {"text": "from the next GPU sorry and then we", "start": 3061.119, "duration": 4.081}, {"text": "return that tensor. So just like that", "start": 3063.2, "duration": 4.159}, {"text": "we've actually already implemented coms.", "start": 3065.2, "duration": 4.399}, {"text": "So you should be proud of yourself cuz", "start": 3067.359, "duration": 5.76}, {"text": "we're already two steps into our course.", "start": 3069.599, "duration": 5.681}, {"text": "And one thing that I wanted to note is", "start": 3073.119, "duration": 3.841}, {"text": "that if you look at the type hint of", "start": 3075.28, "duration": 4.559}, {"text": "these distributed protocols or", "start": 3076.96, "duration": 5.04}, {"text": "primitives, you say it says it says", "start": 3079.839, "duration": 4.24}, {"text": "receive the tensor synchrony both for", "start": 3082.0, "duration": 5.119}, {"text": "this. receive and sends a tensor", "start": 3084.079, "duration": 4.561}, {"text": "synchronously.", "start": 3087.119, "duration": 4.48}, {"text": "So when it says this, of course, you", "start": 3088.64, "duration": 4.64}, {"text": "think that there's an asynchronous", "start": 3091.599, "duration": 4.401}, {"text": "method, which there is.", "start": 3093.28, "duration": 5.039}, {"text": "And this is what's used in production", "start": 3096.0, "duration": 4.24}, {"text": "because when you have asynchronous", "start": 3098.319, "duration": 4.0}, {"text": "processes then you can essentially", "start": 3100.24, "duration": 4.24}, {"text": "overlap computation with comp uh", "start": 3102.319, "duration": 4.24}, {"text": "computation with communication more", "start": 3104.48, "duration": 4.4}, {"text": "effectively but it also imp it also", "start": 3106.559, "duration": 4.321}, {"text": "introduces much more complexity. So for", "start": 3108.88, "duration": 3.76}, {"text": "the purposes of this course, we're only", "start": 3110.88, "duration": 3.76}, {"text": "going to use synchronous oper uh synch", "start": 3112.64, "duration": 5.199}, {"text": "synchronous operations, sorry. And what", "start": 3114.64, "duration": 5.919}, {"text": "this means is that when we call receive", "start": 3117.839, "duration": 4.321}, {"text": "backward,", "start": 3120.559, "duration": 4.881}, {"text": "as soon as we reach this method, the", "start": 3122.16, "duration": 6.48}, {"text": "code will not go anywhere until it", "start": 3125.44, "duration": 5.44}, {"text": "actually gets the tensor that it is", "start": 3128.64, "duration": 4.16}, {"text": "supposed to receive. So what you can", "start": 3130.88, "duration": 4.0}, {"text": "imagine is that if we're running with", "start": 3132.8, "duration": 5.92}, {"text": "two GPUs and we're waiting for GPU 1, so", "start": 3134.88, "duration": 6.719}, {"text": "the first GPU to do a computation and", "start": 3138.72, "duration": 5.2}, {"text": "then we need to receive the forward from", "start": 3141.599, "duration": 4.161}, {"text": "GPU 1, we're going to be waiting here", "start": 3143.92, "duration": 4.8}, {"text": "the entire time at distop receive in the", "start": 3145.76, "duration": 8.559}, {"text": "second GPU until the first GPU sends it.", "start": 3148.72, "duration": 8.879}, {"text": "And once again I want to emphasize that", "start": 3154.319, "duration": 5.201}, {"text": "here I don't know why we have this in", "start": 3157.599, "duration": 4.24}, {"text": "the top here", "start": 3159.52, "duration": 5.12}, {"text": "we have two copies of this file one copy", "start": 3161.839, "duration": 4.561}, {"text": "with local rank zero and one copy with", "start": 3164.64, "duration": 4.64}, {"text": "local rank one because torch one makes a", "start": 3166.4, "duration": 5.199}, {"text": "copy of each script for distributed", "start": 3169.28, "duration": 4.96}, {"text": "training or inference whatever you want", "start": 3171.599, "duration": 5.041}, {"text": "to do. So just to be extremely clear on", "start": 3174.24, "duration": 4.0}, {"text": "this point as soon as we run something", "start": 3176.64, "duration": 4.4}, {"text": "with torch run let's say two process", "start": 3178.24, "duration": 4.48}, {"text": "nodes which is two devices that could be", "start": 3181.04, "duration": 5.039}, {"text": "two GPUs any script that is implemented", "start": 3182.72, "duration": 6.72}, {"text": "as its argument such as these coms will", "start": 3186.079, "duration": 6.561}, {"text": "be run in two separate instances once", "start": 3189.44, "duration": 5.119}, {"text": "for the first device and another for the", "start": 3192.64, "duration": 4.24}, {"text": "second device with local rank zero for", "start": 3194.559, "duration": 5.201}, {"text": "device the first one and local rank one", "start": 3196.88, "duration": 5.12}, {"text": "for the second device and when we come", "start": 3199.76, "duration": 3.839}, {"text": "down here let's say that we're running", "start": 3202.0, "duration": 5.28}, {"text": "the part one and the part to sharded MLP", "start": 3203.599, "duration": 7.921}, {"text": "in the entire first part. The second GPU", "start": 3207.28, "duration": 6.799}, {"text": "is simply stuck at this receive forward", "start": 3211.52, "duration": 4.64}, {"text": "function because it's waiting to get the", "start": 3214.079, "duration": 4.081}, {"text": "activations from the first GPU so that", "start": 3216.16, "duration": 4.24}, {"text": "it can compute its own activations on", "start": 3218.16, "duration": 4.8}, {"text": "its part of the model which means that", "start": 3220.4, "duration": 6.64}, {"text": "until the first model computes the GPU", "start": 3222.96, "duration": 6.32}, {"text": "and calls send forward to do this disc", "start": 3227.04, "duration": 4.0}, {"text": "send to next rank which in this case", "start": 3229.28, "duration": 3.6}, {"text": "will be one because we have zero then", "start": 3231.04, "duration": 4.24}, {"text": "the next rank will be one. This entire", "start": 3232.88, "duration": 5.28}, {"text": "time the GPU1 is just waiting here to", "start": 3235.28, "duration": 4.559}, {"text": "receive the tensor from the previous", "start": 3238.16, "duration": 3.679}, {"text": "rank. So nothing's happening. It's just", "start": 3239.839, "duration": 3.52}, {"text": "waiting because it's synchronous. As", "start": 3241.839, "duration": 4.0}, {"text": "soon as it does receive that um value", "start": 3243.359, "duration": 5.681}, {"text": "from the first GPU, then it will", "start": 3245.839, "duration": 4.72}, {"text": "complete this function and then return", "start": 3249.04, "duration": 3.36}, {"text": "the tensor and then start its own", "start": 3250.559, "duration": 5.04}, {"text": "activation um forward pass. So you can", "start": 3252.4, "duration": 5.28}, {"text": "do the exact same logic or reasoning", "start": 3255.599, "duration": 4.161}, {"text": "here for the backward set of uh", "start": 3257.68, "duration": 5.439}, {"text": "distributed communication primitives but", "start": 3259.76, "duration": 4.88}, {"text": "in that case it's the reverse order.", "start": 3263.119, "duration": 3.121}, {"text": "It's that the first GPU has to wait for", "start": 3264.64, "duration": 4.32}, {"text": "the second GPU to give it its gradients.", "start": 3266.24, "duration": 5.04}, {"text": "So there you go. I once again want to", "start": 3268.96, "duration": 4.56}, {"text": "emphasize this because it's not easy to", "start": 3271.28, "duration": 4.559}, {"text": "think about distributed code because you", "start": 3273.52, "duration": 4.72}, {"text": "have to picture two instances of any", "start": 3275.839, "duration": 4.48}, {"text": "script if you have two processes or if", "start": 3278.24, "duration": 3.119}, {"text": "you have even more you have to picture", "start": 3280.319, "duration": 3.121}, {"text": "four happening simultaneously. So I hope", "start": 3281.359, "duration": 4.48}, {"text": "this makes sense. So now that we have", "start": 3283.44, "duration": 5.2}, {"text": "comms done, let's go back to our", "start": 3285.839, "duration": 6.801}, {"text": "syllabus and see what's next. So we're", "start": 3288.64, "duration": 6.08}, {"text": "still in distributed basics, but now we", "start": 3292.64, "duration": 4.08}, {"text": "have a lab which is to spawn two", "start": 3294.72, "duration": 4.399}, {"text": "processes on GPU or CPU and ping pong a", "start": 3296.72, "duration": 4.399}, {"text": "tensor. And this is the this is the", "start": 3299.119, "duration": 4.321}, {"text": "command that we're going to use to run", "start": 3301.119, "duration": 9.041}, {"text": "that. So let's come and open our um", "start": 3303.44, "duration": 10.8}, {"text": "step three which is called ping pong.", "start": 3310.16, "duration": 6.08}, {"text": "Okay, so you can see it's pretty bare", "start": 3314.24, "duration": 3.28}, {"text": "bones. So we unfortunately have to do a", "start": 3316.24, "duration": 3.44}, {"text": "lot of implementation here. But let me", "start": 3317.52, "duration": 4.64}, {"text": "give you an introduction to what we need", "start": 3319.68, "duration": 4.72}, {"text": "to do and it's pretty well documented", "start": 3322.16, "duration": 4.88}, {"text": "here in the dock string. It says send a", "start": 3324.4, "duration": 5.84}, {"text": "tensor from device rank zero to device", "start": 3327.04, "duration": 5.44}, {"text": "rank one and print to verify that you", "start": 3330.24, "duration": 6.079}, {"text": "actually sent it. So we can see that I", "start": 3332.48, "duration": 7.28}, {"text": "already have the conference call uh", "start": 3336.319, "duration": 5.76}, {"text": "establishment which is just incorre", "start": 3339.76, "duration": 4.64}, {"text": "distributed which will set up the", "start": 3342.079, "duration": 3.681}, {"text": "collective communication between these", "start": 3344.4, "duration": 6.0}, {"text": "two devices. And this will return rank", "start": 3345.76, "duration": 7.28}, {"text": "world size and the device that we're on", "start": 3350.4, "duration": 5.679}, {"text": "just as we set it up if you recall here", "start": 3353.04, "duration": 4.96}, {"text": "which is important.", "start": 3356.079, "duration": 4.0}, {"text": "And then we're going to use this to", "start": 3358.0, "duration": 4.48}, {"text": "decide which device that we're on and", "start": 3360.079, "duration": 5.681}, {"text": "output the corresponding um value. But", "start": 3362.48, "duration": 5.839}, {"text": "before we do that, let's just see what", "start": 3365.76, "duration": 4.799}, {"text": "happens if I print these values. So", "start": 3368.319, "duration": 5.601}, {"text": "print rank world size and device. And", "start": 3370.559, "duration": 4.8}, {"text": "we're once again going to use torch run", "start": 3373.92, "duration": 4.48}, {"text": "here. So torch run, sorry, UV run torch", "start": 3375.359, "duration": 4.24}, {"text": "run because we didn't activate the", "start": 3378.4, "duration": 2.64}, {"text": "virtual environment. Torch run on its", "start": 3379.599, "duration": 3.921}, {"text": "own doesn't work. But if I do UV torch", "start": 3381.04, "duration": 6.16}, {"text": "UV run torch run with the following", "start": 3383.52, "duration": 5.52}, {"text": "parameters. So you can see here we have", "start": 3387.2, "duration": 3.919}, {"text": "n pros per node and all this means is", "start": 3389.04, "duration": 4.96}, {"text": "that how many GPUs do you have or CPU", "start": 3391.119, "duration": 4.801}, {"text": "processes. So we have two in this case", "start": 3394.0, "duration": 3.68}, {"text": "and then we need to specify the file", "start": 3395.92, "duration": 3.28}, {"text": "that we want to run. You can see here", "start": 3397.68, "duration": 3.04}, {"text": "that this is the training script. So I'm", "start": 3399.2, "duration": 3.599}, {"text": "going to come into my work and step", "start": 3400.72, "duration": 4.639}, {"text": "three is what we're going to run. So", "start": 3402.799, "duration": 5.841}, {"text": "just to verify here we have two GPUs and", "start": 3405.359, "duration": 5.041}, {"text": "or two virtual GPUs. In our case we have", "start": 3408.64, "duration": 3.439}, {"text": "CPUs. And you might be wondering what's", "start": 3410.4, "duration": 3.6}, {"text": "nodes that's the number of computers", "start": 3412.079, "duration": 3.841}, {"text": "that you have or number of clusters. So", "start": 3414.0, "duration": 4.0}, {"text": "maybe you have two clusters of two GPUs.", "start": 3415.92, "duration": 3.679}, {"text": "In that case, you would do number of", "start": 3418.0, "duration": 3.2}, {"text": "nodes two and number of processes per", "start": 3419.599, "duration": 3.441}, {"text": "node two, which gives you four GPUs in", "start": 3421.2, "duration": 3.04}, {"text": "total. But we don't have that in this", "start": 3423.04, "duration": 2.48}, {"text": "case. So we're just going to run torch", "start": 3424.24, "duration": 5.76}, {"text": "run. And it looks like ah I realized", "start": 3425.52, "duration": 8.319}, {"text": "that we don't have this working because", "start": 3430.0, "duration": 5.28}, {"text": "we're in the wrong we're in a different", "start": 3433.839, "duration": 4.401}, {"text": "directory. So instead I need to say", "start": 3435.28, "duration": 6.16}, {"text": "step2.com. So this is good um because we", "start": 3438.24, "duration": 5.599}, {"text": "will change this for the course. So, by", "start": 3441.44, "duration": 4.0}, {"text": "the time that you've read this, it would", "start": 3443.839, "duration": 4.96}, {"text": "actually be step three um underscore", "start": 3445.44, "duration": 5.44}, {"text": "comps.", "start": 3448.799, "duration": 3.441}, {"text": "Sorry, step two underscore comps. I", "start": 3450.88, "duration": 4.479}, {"text": "can't read. And now it should work.", "start": 3452.24, "duration": 6.68}, {"text": "Let's see what happens.", "start": 3455.359, "duration": 3.561}, {"text": "Okay, guys, we got a bug. That's cool", "start": 3459.44, "duration": 4.639}, {"text": "because now we can deb debug it", "start": 3462.16, "duration": 7.76}, {"text": "together. So, what went wrong? Oh, okay.", "start": 3464.079, "duration": 7.04}, {"text": "No, that's not the error. What is", "start": 3469.92, "duration": 2.159}, {"text": "actually is there? Let's look at the", "start": 3471.119, "duration": 4.44}, {"text": "error, guys. together.", "start": 3472.079, "duration": 3.48}, {"text": "As you can see, torch run error logs are", "start": 3477.52, "duration": 8.24}, {"text": "not the simplest to read. Ah, okay. So,", "start": 3481.599, "duration": 6.641}, {"text": "we have a bug. For some reason, it's", "start": 3485.76, "duration": 5.12}, {"text": "trying to initialize a process with", "start": 3488.24, "duration": 5.839}, {"text": "nickel even though we have a CPU. So,", "start": 3490.88, "duration": 6.08}, {"text": "why is that happening? Let's come and", "start": 3494.079, "duration": 5.76}, {"text": "see because I think I have the logic", "start": 3496.96, "duration": 6.639}, {"text": "reversed. Then device equals", "start": 3499.839, "duration": 9.041}, {"text": "CUDA if torch dot is available and then", "start": 3503.599, "duration": 8.801}, {"text": "this we will init the process group with", "start": 3508.88, "duration": 8.16}, {"text": "nickel if torch.cuda is available else", "start": 3512.4, "duration": 7.52}, {"text": "dist with glue and rank world size.", "start": 3517.04, "duration": 6.0}, {"text": "Actually this does look correct. Let me", "start": 3519.92, "duration": 9.6}, {"text": "just um ah of course the fact is that", "start": 3523.04, "duration": 9.519}, {"text": "with this type of um conditional", "start": 3529.52, "duration": 6.0}, {"text": "statement it will still call this um if", "start": 3532.559, "duration": 5.28}, {"text": "I'm not mistaken. So I think we need to", "start": 3535.52, "duration": 7.12}, {"text": "just make this a um make this a strict", "start": 3537.839, "duration": 6.72}, {"text": "conditional statement and this may fix", "start": 3542.64, "duration": 3.919}, {"text": "the bug. If it doesn't then we'll try", "start": 3544.559, "duration": 3.201}, {"text": "something else. But let's see what", "start": 3546.559, "duration": 3.921}, {"text": "happens now. No. Okay, that was not", "start": 3547.76, "duration": 5.359}, {"text": "there. I think then I've misread what's", "start": 3550.48, "duration": 5.04}, {"text": "happening here.", "start": 3553.119, "duration": 5.44}, {"text": "Okay. No, it is better now. We have glue", "start": 3555.52, "duration": 5.12}, {"text": "and then", "start": 3558.559, "duration": 5.601}, {"text": "what else is happening?", "start": 3560.64, "duration": 6.0}, {"text": "Right. I can see now our error was quite", "start": 3564.16, "duration": 4.56}, {"text": "obvious is that we didn't wrap these", "start": 3566.64, "duration": 4.719}, {"text": "strings around the torch device call. So", "start": 3568.72, "duration": 4.639}, {"text": "literally our device was the CUDA string", "start": 3571.359, "duration": 6.881}, {"text": "and it wasn't the uh CUDA torch.", "start": 3573.359, "duration": 10.0}, {"text": "CPU. So now if I fix this, it should", "start": 3578.24, "duration": 9.2}, {"text": "work. Let's try one last time. Okay, now", "start": 3583.359, "duration": 5.841}, {"text": "I finally realized the other error is", "start": 3587.44, "duration": 4.8}, {"text": "that we need to use keyword arguments to", "start": 3589.2, "duration": 6.399}, {"text": "establish these values of rank and world", "start": 3592.24, "duration": 6.8}, {"text": "size because they're not the actual", "start": 3595.599, "duration": 7.041}, {"text": "second and third arguments in the code.", "start": 3599.04, "duration": 8.64}, {"text": "So we'll say rank equals rank and then", "start": 3602.64, "duration": 7.52}, {"text": "world", "start": 3607.68, "duration": 4.48}, {"text": "underscore skies equals world size.", "start": 3610.16, "duration": 4.08}, {"text": "Okay, now I promise this will work.", "start": 3612.16, "duration": 5.12}, {"text": "Please work. There you go. Okay, so we", "start": 3614.24, "duration": 5.76}, {"text": "have a working version of the code, but", "start": 3617.28, "duration": 3.76}, {"text": "now we need to actually implement the", "start": 3620.0, "duration": 2.96}, {"text": "ping pong itself. But what I wanted to", "start": 3621.04, "duration": 6.319}, {"text": "point out is that we now get a strange", "start": 3622.96, "duration": 7.28}, {"text": "input which is to say", "start": 3627.359, "duration": 6.321}, {"text": "the two values are", "start": 3630.24, "duration": 7.04}, {"text": "printing one on top of the next. So if", "start": 3633.68, "duration": 5.84}, {"text": "you look here, we'll have rank in this", "start": 3637.28, "duration": 7.68}, {"text": "case is one for GPU number two and then", "start": 3639.52, "duration": 8.16}, {"text": "the world size is two and the device is", "start": 3644.96, "duration": 8.399}, {"text": "CPU and then for world size with rank", "start": 3647.68, "duration": 8.879}, {"text": "zero it's 02 CPU. So you might be", "start": 3653.359, "duration": 4.96}, {"text": "wondering why aren't these printed", "start": 3656.559, "duration": 3.841}, {"text": "nicely and they're kind of interled.", "start": 3658.319, "duration": 3.52}, {"text": "It's because well if we try this one", "start": 3660.4, "duration": 3.12}, {"text": "more time we might get it happening. So", "start": 3661.839, "duration": 3.921}, {"text": "you can see even here that we have oh no", "start": 3663.52, "duration": 4.559}, {"text": "this this time we get the same result", "start": 3665.76, "duration": 4.0}, {"text": "right you can see now we have slightly", "start": 3668.079, "duration": 3.681}, {"text": "different result it's because when you", "start": 3669.76, "duration": 4.64}, {"text": "have this distributed framework with the", "start": 3671.76, "duration": 4.88}, {"text": "way that the memory and the buffers are", "start": 3674.4, "duration": 4.0}, {"text": "loading this information it's not", "start": 3676.64, "duration": 3.36}, {"text": "actually deterministic so finally I got", "start": 3678.4, "duration": 4.32}, {"text": "it um this time we actually first", "start": 3680.0, "duration": 6.48}, {"text": "printed zero to and CPU which is for", "start": 3682.72, "duration": 7.28}, {"text": "device zero of rank zero and then one to", "start": 3686.48, "duration": 5.52}, {"text": "CPU which is for device of rank one and", "start": 3690.0, "duration": 3.44}, {"text": "this is just to show you", "start": 3692.0, "duration": 3.52}, {"text": "When you do this, sometimes the print", "start": 3693.44, "duration": 5.52}, {"text": "statements are working in a strange or", "start": 3695.52, "duration": 6.24}, {"text": "asynchronous way. And this is because", "start": 3698.96, "duration": 4.8}, {"text": "we're running distributed code. And", "start": 3701.76, "duration": 3.92}, {"text": "lastly, I want to once again highlight", "start": 3703.76, "duration": 4.24}, {"text": "the fact that we are running two copies", "start": 3705.68, "duration": 4.72}, {"text": "of this file. So we have one copy", "start": 3708.0, "duration": 4.24}, {"text": "running on device 0 and one copy running", "start": 3710.4, "duration": 5.199}, {"text": "on device two. Sorry, device one. And", "start": 3712.24, "duration": 5.2}, {"text": "for this reason, we get two print", "start": 3715.599, "duration": 3.361}, {"text": "statements even though there's only one", "start": 3717.44, "duration": 3.359}, {"text": "print statement here. And I have never", "start": 3718.96, "duration": 3.92}, {"text": "tried this myself, but if I think if if", "start": 3720.799, "duration": 4.081}, {"text": "we run just the file on its own, we'll", "start": 3722.88, "duration": 4.239}, {"text": "probably get an error because um we need", "start": 3724.88, "duration": 3.679}, {"text": "to use torch one when we're running", "start": 3727.119, "duration": 2.72}, {"text": "distributed code. But let's just see", "start": 3728.559, "duration": 3.04}, {"text": "what happens um to make sure that it", "start": 3729.839, "duration": 4.881}, {"text": "does give us an error.", "start": 3731.599, "duration": 4.401}, {"text": "All right. Yeah, it doesn't know where", "start": 3734.72, "duration": 4.48}, {"text": "the rank is because right if we come", "start": 3736.0, "duration": 6.64}, {"text": "here in coms.py, the rank is initialized", "start": 3739.2, "duration": 6.24}, {"text": "by torch run. Okay. So now we have this", "start": 3742.64, "duration": 5.52}, {"text": "done. Let's actually check what device", "start": 3745.44, "duration": 6.639}, {"text": "that we're on. So if device equals", "start": 3748.16, "duration": 6.159}, {"text": "equals zero, we'll do something. And", "start": 3752.079, "duration": 4.24}, {"text": "then I don't want to do it three times.", "start": 3754.319, "duration": 4.161}, {"text": "If device equals equals 1, then we want", "start": 3756.319, "duration": 4.0}, {"text": "to do something else. So in this case,", "start": 3758.48, "duration": 3.2}, {"text": "the function says send a tensor from", "start": 3760.319, "duration": 3.361}, {"text": "device rank zero to device rank one and", "start": 3761.68, "duration": 5.28}, {"text": "print to verify. Okay, so what we need", "start": 3763.68, "duration": 8.159}, {"text": "to do first of all is a tensor creation,", "start": 3766.96, "duration": 6.0}, {"text": "right? We want to actually make the", "start": 3771.839, "duration": 7.28}, {"text": "tensor. So we'll just do tensor equals", "start": 3772.96, "duration": 8.399}, {"text": "torch", "start": 3779.119, "duration": 5.361}, {"text": "rand of three. And what does that give", "start": 3781.359, "duration": 4.561}, {"text": "us? Actually, we'll just see very soon.", "start": 3784.48, "duration": 3.68}, {"text": "It'll just be a random tensor of three", "start": 3785.92, "duration": 5.199}, {"text": "values. And then", "start": 3788.16, "duration": 6.72}, {"text": "we're going to say um tensor. So one", "start": 3791.119, "duration": 5.121}, {"text": "thing I forgot to do is actually", "start": 3794.88, "duration": 5.199}, {"text": "initialize our communication um class,", "start": 3796.24, "duration": 5.839}, {"text": "which is pipeline comms. Without doing", "start": 3800.079, "duration": 3.921}, {"text": "this, then we won't be able to actually", "start": 3802.079, "duration": 4.24}, {"text": "perform the communication. This is the", "start": 3804.0, "duration": 4.079}, {"text": "class which gives us gives us our", "start": 3806.319, "duration": 3.441}, {"text": "primitives. So we need to give it the", "start": 3808.079, "duration": 4.401}, {"text": "rank and the world size.", "start": 3809.76, "duration": 5.52}, {"text": "Boom. And then now we're going to do", "start": 3812.48, "duration": 4.639}, {"text": "communication", "start": 3815.28, "duration": 3.92}, {"text": "dot", "start": 3817.119, "duration": 5.44}, {"text": "send forward.", "start": 3819.2, "duration": 5.119}, {"text": "And then", "start": 3822.559, "duration": 3.681}, {"text": "we're going to send a tensor. And since", "start": 3824.319, "duration": 3.28}, {"text": "this does not return anything, then we", "start": 3826.24, "duration": 3.839}, {"text": "we don't need to return anything um", "start": 3827.599, "duration": 4.561}, {"text": "because that's that's not what we do", "start": 3830.079, "duration": 3.601}, {"text": "when we're sending. We don't We don't", "start": 3832.16, "duration": 3.679}, {"text": "want to capture any values. We only want", "start": 3833.68, "duration": 6.24}, {"text": "receiving. And we'll say print", "start": 3835.839, "duration": 7.601}, {"text": "sent tensor", "start": 3839.92, "duration": 6.08}, {"text": "tensor. So this will only once again", "start": 3843.44, "duration": 5.28}, {"text": "work if local rank is equal to zero.", "start": 3846.0, "duration": 5.28}, {"text": "Okay. I just realized I put device", "start": 3848.72, "duration": 4.8}, {"text": "instead of rank. So this should be rank.", "start": 3851.28, "duration": 6.48}, {"text": "And then now we will do", "start": 3853.52, "duration": 8.48}, {"text": "first of all uh communication dot", "start": 3857.76, "duration": 7.599}, {"text": "receive forward of the tensor but we", "start": 3862.0, "duration": 5.44}, {"text": "need to know its shape. So the problem", "start": 3865.359, "duration": 5.68}, {"text": "here is that when we run this code on", "start": 3867.44, "duration": 6.879}, {"text": "the device of local rank one it will not", "start": 3871.039, "duration": 5.841}, {"text": "know what the shape is since we have", "start": 3874.319, "duration": 6.8}, {"text": "initialized it in the rank equals zero.", "start": 3876.88, "duration": 6.479}, {"text": "So let's just take this out of um let's", "start": 3881.119, "duration": 4.161}, {"text": "just take this out of the oh no we don't", "start": 3883.359, "duration": 3.921}, {"text": "want to take it out of the the if", "start": 3885.28, "duration": 3.839}, {"text": "because the whole point is that we only", "start": 3887.28, "duration": 5.44}, {"text": "want it to exist on the device zero and", "start": 3889.119, "duration": 5.361}, {"text": "then set it to device one. So instead", "start": 3892.72, "duration": 2.639}, {"text": "we're just going to assume that we know", "start": 3894.48, "duration": 3.52}, {"text": "the shape which is set to three. So what", "start": 3895.359, "duration": 4.24}, {"text": "again once what are once again the", "start": 3898.0, "duration": 4.16}, {"text": "arguments here? I forgot uh the shape", "start": 3899.599, "duration": 4.801}, {"text": "which is three and then the device which", "start": 3902.16, "duration": 6.159}, {"text": "is device and we already have that as a", "start": 3904.4, "duration": 7.84}, {"text": "parameter of the return to init distri", "start": 3908.319, "duration": 6.24}, {"text": "distributed and then dtype is optional.", "start": 3912.24, "duration": 5.76}, {"text": "So now we can say print", "start": 3914.559, "duration": 6.8}, {"text": "received tensor and then the tensor and", "start": 3918.0, "duration": 4.96}, {"text": "now if you run this it should actually", "start": 3921.359, "duration": 5.401}, {"text": "work which is amazing.", "start": 3922.96, "duration": 3.8}, {"text": "What did we do? I think we still have", "start": 3927.599, "duration": 2.96}, {"text": "something that I wanted. Ah, yeah. We're", "start": 3929.119, "duration": 3.2}, {"text": "not actually using torch run. Of course,", "start": 3930.559, "duration": 3.04}, {"text": "guys, we need to use torch run when we", "start": 3932.319, "duration": 3.52}, {"text": "want to run distributed code. Okay, I", "start": 3933.599, "duration": 3.2}, {"text": "keep on saying we're not going to get", "start": 3935.839, "duration": 2.321}, {"text": "errors, but we do end up getting an", "start": 3936.799, "duration": 3.841}, {"text": "error. And it's because we never", "start": 3938.16, "duration": 4.399}, {"text": "actually Okay, first of all, we never", "start": 3940.64, "duration": 3.679}, {"text": "actually captured the tensor, and I", "start": 3942.559, "duration": 4.321}, {"text": "think that's the error.", "start": 3944.319, "duration": 5.121}, {"text": "Yeah, it is. Okay.", "start": 3946.88, "duration": 4.88}, {"text": "So, first of all, you can see we set the", "start": 3949.44, "duration": 4.879}, {"text": "tensor of this", "start": 3951.76, "duration": 5.52}, {"text": "and then we received it here. And this", "start": 3954.319, "duration": 6.561}, {"text": "is only possible because we have", "start": 3957.28, "duration": 5.2}, {"text": "implemented descend forward and receive", "start": 3960.88, "duration": 3.28}, {"text": "forward. Otherwise, it wouldn't work.", "start": 3962.48, "duration": 3.28}, {"text": "And we can actually just change these to", "start": 3964.16, "duration": 2.639}, {"text": "backward because we're not actually", "start": 3965.76, "duration": 4.48}, {"text": "doing any backward passes right now. Um,", "start": 3966.799, "duration": 7.361}, {"text": "it should be the same. Let's see.", "start": 3970.24, "duration": 5.92}, {"text": "Yeah, it should be the same except the", "start": 3974.16, "duration": 3.84}, {"text": "one problem is that the direction will", "start": 3976.16, "duration": 3.439}, {"text": "be flipped. So, if we do send backward,", "start": 3978.0, "duration": 2.799}, {"text": "then we'll have to change the device", "start": 3979.599, "duration": 2.48}, {"text": "check. So, let's change this to", "start": 3980.799, "duration": 2.721}, {"text": "backward. I'll show you. We should get", "start": 3982.079, "duration": 2.321}, {"text": "an error here because this should be", "start": 3983.52, "duration": 3.2}, {"text": "rank one and rank zero, right? But if we", "start": 3984.4, "duration": 3.919}, {"text": "change this to rank one and this to rank", "start": 3986.72, "duration": 3.44}, {"text": "zero, then now we're sending it", "start": 3988.319, "duration": 4.72}, {"text": "backward. So it actually works. And this", "start": 3990.16, "duration": 4.8}, {"text": "is really amazing to see. We receive the", "start": 3993.039, "duration": 4.481}, {"text": "tensor of this and we sent the tensor of", "start": 3994.96, "duration": 4.639}, {"text": "this. So this is the ping pong example", "start": 3997.52, "duration": 4.079}, {"text": "which establishes the communication", "start": 3999.599, "duration": 5.041}, {"text": "which we need to use when we actually", "start": 4001.599, "duration": 5.281}, {"text": "have our pipeline parallelism um", "start": 4004.64, "duration": 4.32}, {"text": "splitting up the models and sending the", "start": 4006.88, "duration": 3.76}, {"text": "data between them. Something I really", "start": 4008.96, "duration": 4.24}, {"text": "want to quickly introduce before we move", "start": 4010.64, "duration": 4.56}, {"text": "on to the next point though is something", "start": 4013.2, "duration": 4.8}, {"text": "called torch.distributed.", "start": 4015.2, "duration": 5.919}, {"text": "And what this does is it forces as we", "start": 4018.0, "duration": 6.4}, {"text": "can see here the processes to wait until", "start": 4021.119, "duration": 5.761}, {"text": "the entire group enters this function.", "start": 4024.4, "duration": 4.08}, {"text": "So this is mostly useful for", "start": 4026.88, "duration": 4.239}, {"text": "asynchronous code where you have many", "start": 4028.48, "duration": 5.76}, {"text": "processes happening in different uh", "start": 4031.119, "duration": 6.161}, {"text": "speeds and you want them to all reach a", "start": 4034.24, "duration": 5.28}, {"text": "certain point before they move on to the", "start": 4037.28, "duration": 5.279}, {"text": "next one. So this isn't extremely useful", "start": 4039.52, "duration": 4.319}, {"text": "for synchronous code but it's still", "start": 4042.559, "duration": 4.24}, {"text": "interesting to implement it and see what", "start": 4043.839, "duration": 5.041}, {"text": "the effect is. So the first thing I want", "start": 4046.799, "duration": 4.721}, {"text": "to try with this is to put it within one", "start": 4048.88, "duration": 4.88}, {"text": "of the conditional statements of the", "start": 4051.52, "duration": 4.24}, {"text": "different device ranks and see what", "start": 4053.76, "duration": 5.12}, {"text": "happens. So if we just go UV run torch", "start": 4055.76, "duration": 5.279}, {"text": "run, I think this is what we want. And", "start": 4058.88, "duration": 4.8}, {"text": "what this will do is it will cause the", "start": 4061.039, "duration": 5.681}, {"text": "code to not um terminate. And why is", "start": 4063.68, "duration": 5.119}, {"text": "this the case? It's because once again,", "start": 4066.72, "duration": 4.8}, {"text": "as we read it says it waits until the", "start": 4068.799, "duration": 4.481}, {"text": "whole group enters the function, which", "start": 4071.52, "duration": 3.92}, {"text": "in this case is these two GPUs or these", "start": 4073.28, "duration": 4.16}, {"text": "two devices. In our case, it's two CPU", "start": 4075.44, "duration": 4.8}, {"text": "cores because while we're not splitting", "start": 4077.44, "duration": 4.48}, {"text": "on GPUs, we're splitting on the cores of", "start": 4080.24, "duration": 4.879}, {"text": "our CPU. But if I now come, you can see", "start": 4081.92, "duration": 4.399}, {"text": "that this code will never actually", "start": 4085.119, "duration": 3.121}, {"text": "terminate because it's waiting forever", "start": 4086.319, "duration": 6.0}, {"text": "for the first core of our CPU to enter", "start": 4088.24, "duration": 6.16}, {"text": "this um barrier. But it it's actually", "start": 4092.319, "duration": 4.4}, {"text": "not going to get it because it's only", "start": 4094.4, "duration": 4.48}, {"text": "happening if you're on rank one. Now, if", "start": 4096.719, "duration": 3.921}, {"text": "we duplicate this and put it down here,", "start": 4098.88, "duration": 3.6}, {"text": "we'll see that the c the code will run", "start": 4100.64, "duration": 4.24}, {"text": "as expected. So, let's terminate this", "start": 4102.48, "duration": 5.679}, {"text": "process and go through it. Boom. So as", "start": 4104.88, "duration": 4.799}, {"text": "soon as the two devices reach this", "start": 4108.159, "duration": 5.12}, {"text": "barrier then um they will continue on.", "start": 4109.679, "duration": 4.961}, {"text": "So this is really good once again for", "start": 4113.279, "duration": 3.04}, {"text": "synchronizing asynchronous code but", "start": 4114.64, "duration": 3.28}, {"text": "since our code is already synchronous it", "start": 4116.319, "duration": 3.201}, {"text": "doesn't really make a difference. And", "start": 4117.92, "duration": 3.279}, {"text": "one last thing I'll show you is that if", "start": 4119.52, "duration": 3.6}, {"text": "we put this before the conditional", "start": 4121.199, "duration": 3.761}, {"text": "statements then both of them will see it", "start": 4123.12, "duration": 4.4}, {"text": "at that point and the code will also run", "start": 4124.96, "duration": 4.56}, {"text": "properly. So if you're ever looking into", "start": 4127.52, "duration": 4.239}, {"text": "implementing these primitives in an", "start": 4129.52, "duration": 4.48}, {"text": "asynchronous way, then this is a cool", "start": 4131.759, "duration": 4.161}, {"text": "method that Torch implements which", "start": 4134.0, "duration": 4.239}, {"text": "allows you to temporarily make your code", "start": 4135.92, "duration": 5.279}, {"text": "synchronous. So there you go. Now we", "start": 4138.239, "duration": 5.281}, {"text": "have done step three. So we're 60%", "start": 4141.199, "duration": 4.48}, {"text": "through the steps in this course. I will", "start": 4143.52, "duration": 6.4}, {"text": "note though that the step five um is the", "start": 4145.679, "duration": 6.721}, {"text": "longest. So let's not get too happy. But", "start": 4149.92, "duration": 5.04}, {"text": "now that we've done this, we are going", "start": 4152.4, "duration": 7.439}, {"text": "to first create the model which will be", "start": 4154.96, "duration": 7.279}, {"text": "sharted along the amount of device that", "start": 4159.839, "duration": 4.96}, {"text": "we have. So in this class which is step", "start": 4162.239, "duration": 6.321}, {"text": "four.py, this is once again um mirrored", "start": 4164.799, "duration": 6.161}, {"text": "here. So you saw it's only 35 lines.", "start": 4168.56, "duration": 3.36}, {"text": "This is going to be really simple to", "start": 4170.96, "duration": 3.44}, {"text": "implement and really quick except the", "start": 4171.92, "duration": 4.0}, {"text": "input parameters here are slightly", "start": 4174.4, "duration": 3.04}, {"text": "different.", "start": 4175.92, "duration": 5.439}, {"text": "uh as opposed to the step one where we", "start": 4177.44, "duration": 6.08}, {"text": "had the part one and part two. You can", "start": 4181.359, "duration": 3.761}, {"text": "see here the net method only has dim and", "start": 4183.52, "duration": 3.199}, {"text": "depth but in here the net method has", "start": 4185.12, "duration": 3.28}, {"text": "hidden dim total layers rank and road", "start": 4186.719, "duration": 3.6}, {"text": "size because we need to know which GPU", "start": 4188.4, "duration": 5.279}, {"text": "we're on in order to determine um how", "start": 4190.319, "duration": 6.241}, {"text": "many layers that we need to make. Okay,", "start": 4193.679, "duration": 5.201}, {"text": "so first of all calculate how many", "start": 4196.56, "duration": 3.679}, {"text": "layers this GPU is responsible for. And", "start": 4198.88, "duration": 3.68}, {"text": "this is pretty easy. um we'll say number", "start": 4200.239, "duration": 8.081}, {"text": "of layers is equal to the total layers", "start": 4202.56, "duration": 8.56}, {"text": "divided by the world size. So the number", "start": 4208.32, "duration": 5.76}, {"text": "of GPUs that we have divided by so the", "start": 4211.12, "duration": 4.32}, {"text": "number of layers we have divided by GP.", "start": 4214.08, "duration": 2.96}, {"text": "So if we have 16 layers and four GPUs", "start": 4215.44, "duration": 4.799}, {"text": "then it's four layers per GPU. Um let's", "start": 4217.04, "duration": 5.6}, {"text": "just call this number of layers. And", "start": 4220.239, "duration": 5.041}, {"text": "then what we want to do is build the", "start": 4222.64, "duration": 4.64}, {"text": "stack of local layers. So this is going", "start": 4225.28, "duration": 3.439}, {"text": "to look really similar to what we did", "start": 4227.28, "duration": 3.68}, {"text": "earlier. So because of that I'm just", "start": 4228.719, "duration": 4.401}, {"text": "going to come into my copy paste history", "start": 4230.96, "duration": 3.84}, {"text": "and paste it here. Of course we'll have", "start": 4233.12, "duration": 3.92}, {"text": "to change a few things. So we'll say for", "start": 4234.8, "duration": 4.64}, {"text": "i in range", "start": 4237.04, "duration": 3.679}, {"text": "number of layers because we've already", "start": 4239.44, "duration": 3.52}, {"text": "calculated it. So even before we", "start": 4240.719, "duration": 3.841}, {"text": "actually did the division um and one", "start": 4242.96, "duration": 2.719}, {"text": "thing I just realized is that we should", "start": 4244.56, "duration": 3.599}, {"text": "make this integer division instead of um", "start": 4245.679, "duration": 5.441}, {"text": "float division. We will take the dim. So", "start": 4248.159, "duration": 4.641}, {"text": "here for some reason we have a we have", "start": 4251.12, "duration": 4.48}, {"text": "it set to uh hidden dim but we'll just", "start": 4252.8, "duration": 4.96}, {"text": "change that to dim. The total layers.", "start": 4255.6, "duration": 4.72}, {"text": "Okay. So this is all set up. But one", "start": 4257.76, "duration": 5.12}, {"text": "thing that we need to know and don't", "start": 4260.32, "duration": 5.04}, {"text": "don't uh forget is the fact that in this", "start": 4262.88, "duration": 4.319}, {"text": "case when we had part one and part two", "start": 4265.36, "duration": 3.92}, {"text": "we had two copies of it. So we could", "start": 4267.199, "duration": 4.401}, {"text": "hardcode the logic. But in this case we", "start": 4269.28, "duration": 4.32}, {"text": "just have one model. So we need to have", "start": 4271.6, "duration": 3.68}, {"text": "a conditional statement which will check", "start": 4273.6, "duration": 4.559}, {"text": "in essence is this the last model? Is", "start": 4275.28, "duration": 5.12}, {"text": "this the last GPU? And if that's the", "start": 4278.159, "duration": 4.241}, {"text": "case then we need to add the head. Um so", "start": 4280.4, "duration": 3.279}, {"text": "that's one thing that we need to know.", "start": 4282.4, "duration": 2.96}, {"text": "And then if it's the last GPU we also", "start": 4283.679, "duration": 4.48}, {"text": "need to add the loss function. So this", "start": 4285.36, "duration": 4.48}, {"text": "is what every single model gets. It", "start": 4288.159, "duration": 5.601}, {"text": "always is the same. But if we are self,", "start": 4289.84, "duration": 6.0}, {"text": "so if self oh we don't even need self", "start": 4293.76, "duration": 5.04}, {"text": "self. We just have the input uh as rank.", "start": 4295.84, "duration": 6.0}, {"text": "If rank is equal equal to world size", "start": 4298.8, "duration": 5.6}, {"text": "minus one, then we need to do everything", "start": 4301.84, "duration": 6.8}, {"text": "required for the last model which is", "start": 4304.4, "duration": 8.319}, {"text": "just to say self um not self but rather", "start": 4308.64, "duration": 5.76}, {"text": "layers.append", "start": 4312.719, "duration": 6.121}, {"text": "append a nn sequential", "start": 4314.4, "duration": 4.44}, {"text": "of", "start": 4319.04, "duration": 4.4}, {"text": "what size well it's going to be size dim", "start": 4321.12, "duration": 4.0}, {"text": "to two and right now there's a bug in", "start": 4323.44, "duration": 3.36}, {"text": "our code", "start": 4325.12, "duration": 3.92}, {"text": "I want you to see if you can find it", "start": 4326.8, "duration": 4.56}, {"text": "it's on line 14 so maybe that's a hint I", "start": 4329.04, "duration": 4.88}, {"text": "mean it's a pretty obvious hint um so if", "start": 4331.36, "duration": 4.4}, {"text": "you found it congrats what we need to", "start": 4333.92, "duration": 3.84}, {"text": "realize is that right now we have our", "start": 4335.76, "duration": 4.72}, {"text": "selfnet initialization before we", "start": 4337.76, "duration": 4.08}, {"text": "actually finish adding all the layers", "start": 4340.48, "duration": 2.88}, {"text": "because if we do add this layer then it", "start": 4341.84, "duration": 2.96}, {"text": "will never included in the model. So we", "start": 4343.36, "duration": 3.92}, {"text": "need to move it down. Um if you are not", "start": 4344.8, "duration": 4.16}, {"text": "on the last GPU then it doesn't make a", "start": 4347.28, "duration": 2.72}, {"text": "difference. But if you're on the last", "start": 4348.96, "duration": 3.36}, {"text": "GPU then it does. And then now we'll", "start": 4350.0, "duration": 4.48}, {"text": "also just initialize our loss function.", "start": 4352.32, "duration": 7.68}, {"text": "self.loss fn equals nn.", "start": 4354.48, "duration": 7.759}, {"text": "Entropy loss and we need to call it", "start": 4360.0, "duration": 4.56}, {"text": "because it's a function. Boom. So this", "start": 4362.239, "duration": 5.281}, {"text": "is everything um that we require for", "start": 4364.56, "duration": 5.36}, {"text": "building the local stack of layers. Once", "start": 4367.52, "duration": 5.04}, {"text": "again, the number of layers will be four", "start": 4369.92, "duration": 7.44}, {"text": "instead of 16 if we are from GPU 0 to", "start": 4372.56, "duration": 6.639}, {"text": "GPU 2, but then it'll be five for the", "start": 4377.36, "duration": 5.44}, {"text": "last GPU because we do have the um", "start": 4379.199, "duration": 6.881}, {"text": "classification head here. And now let's", "start": 4382.8, "duration": 6.64}, {"text": "implement forward. So first of all,", "start": 4386.08, "duration": 5.68}, {"text": "every single", "start": 4389.44, "duration": 5.04}, {"text": "model that is on every single GPU needs", "start": 4391.76, "duration": 4.8}, {"text": "to calculate activations. But now we", "start": 4394.48, "duration": 5.04}, {"text": "need to say are we calculating the loss", "start": 4396.56, "duration": 6.159}, {"text": "because in that case um we need to", "start": 4399.52, "duration": 6.0}, {"text": "return that if we're the last device. So", "start": 4402.719, "duration": 4.801}, {"text": "first of all we want to return x in any", "start": 4405.52, "duration": 4.8}, {"text": "case but in this case we want to modify", "start": 4407.52, "duration": 8.32}, {"text": "x. So we'll first check if self", "start": 4410.32, "duration": 7.2}, {"text": "dot", "start": 4415.84, "duration": 3.839}, {"text": "rank", "start": 4417.52, "duration": 6.08}, {"text": "is equal to", "start": 4419.679, "duration": 5.441}, {"text": "world", "start": 4423.6, "duration": 3.84}, {"text": "underscore size minus one. So I just", "start": 4425.12, "duration": 5.039}, {"text": "realized we do definitely need to store", "start": 4427.44, "duration": 5.279}, {"text": "these values in our init method. So", "start": 4430.159, "duration": 5.121}, {"text": "let's come here and say self.rank rank", "start": 4432.719, "duration": 7.041}, {"text": "equals rank and self dot world size", "start": 4435.28, "duration": 6.24}, {"text": "equals", "start": 4439.76, "duration": 3.2}, {"text": "world size. Give me that type in there", "start": 4441.52, "duration": 6.24}, {"text": "you go. If that's the case then we want", "start": 4442.96, "duration": 11.84}, {"text": "to calculate the loss. So self.loss fn", "start": 4447.76, "duration": 8.88}, {"text": "is", "start": 4454.8, "duration": 4.399}, {"text": "e on x and targets. So that's the the", "start": 4456.64, "duration": 5.12}, {"text": "parameters that uh", "start": 4459.199, "duration": 4.641}, {"text": "the cross entropy loss takes. You can", "start": 4461.76, "duration": 5.76}, {"text": "see that huh one second let's look once", "start": 4463.84, "duration": 7.28}, {"text": "again right so we have loss of the input", "start": 4467.52, "duration": 6.32}, {"text": "and the targets as our cross entropy", "start": 4471.12, "duration": 4.0}, {"text": "loss so we want to do the same thing", "start": 4473.84, "duration": 5.92}, {"text": "here if that's the case and we just take", "start": 4475.12, "duration": 8.88}, {"text": "that over x which are the activations so", "start": 4479.76, "duration": 7.6}, {"text": "you can see that now we will calculate", "start": 4484.0, "duration": 7.44}, {"text": "loss if it is um the last value and then", "start": 4487.36, "duration": 5.839}, {"text": "if it isn't we just do the activations", "start": 4491.44, "duration": 3.2}, {"text": "and then pass it on to the next. And", "start": 4493.199, "duration": 4.0}, {"text": "this is how the forward method works. So", "start": 4494.64, "duration": 4.88}, {"text": "now you can see that we have made this", "start": 4497.199, "duration": 4.48}, {"text": "file. I think that it's six lines", "start": 4499.52, "duration": 6.24}, {"text": "shorter because we have less comments.", "start": 4501.679, "duration": 5.761}, {"text": "But let's just make sure. Yeah, there's", "start": 4505.76, "duration": 3.76}, {"text": "quite a few comments here. One thing", "start": 4507.44, "duration": 4.0}, {"text": "that I just see here is that targets is", "start": 4509.52, "duration": 4.08}, {"text": "not none as a check that you can add. So", "start": 4511.44, "duration": 5.84}, {"text": "target should always be none if you are", "start": 4513.6, "duration": 5.36}, {"text": "sorry. Target should always not be none.", "start": 4517.28, "duration": 3.439}, {"text": "should be something if you're last GPU.", "start": 4518.96, "duration": 6.0}, {"text": "But you can also just add and target", "start": 4520.719, "duration": 6.881}, {"text": "is not none in this case to make sure", "start": 4524.96, "duration": 4.0}, {"text": "that you actually have targets to", "start": 4527.6, "duration": 3.44}, {"text": "calculate and not just put them in as", "start": 4528.96, "duration": 5.279}, {"text": "none. So here we've now implemented the", "start": 4531.04, "duration": 5.28}, {"text": "model which is going to be used in our", "start": 4534.239, "duration": 5.361}, {"text": "scheduler. And now we're finally at the", "start": 4536.32, "duration": 4.96}, {"text": "stage we're going to be implementing the", "start": 4539.6, "duration": 4.24}, {"text": "pipeline parallelism given that we've", "start": 4541.28, "duration": 5.68}, {"text": "added all of our primitive uh functions", "start": 4543.84, "duration": 5.12}, {"text": "and the communication primitives that we", "start": 4546.96, "duration": 3.759}, {"text": "need in order to communicate different", "start": 4548.96, "duration": 4.56}, {"text": "between different GPUs. So this is", "start": 4550.719, "duration": 5.201}, {"text": "really good news. Let's come into our", "start": 4553.52, "duration": 3.52}, {"text": "outline. I don't know why we got out of", "start": 4555.92, "duration": 4.4}, {"text": "there. And let's learn what the naive", "start": 4557.04, "duration": 6.08}, {"text": "solution is. So if we come here, we're", "start": 4560.32, "duration": 5.919}, {"text": "going to have some some code and let's", "start": 4563.12, "duration": 5.36}, {"text": "talk about naive model parallelism. So", "start": 4566.239, "duration": 4.641}, {"text": "naive model parallelism or pipeline", "start": 4568.48, "duration": 4.4}, {"text": "parallelism, this is the same the same", "start": 4570.88, "duration": 4.72}, {"text": "word in in in effect is the most", "start": 4572.88, "duration": 5.12}, {"text": "straightway of implementing pipeline", "start": 4575.6, "duration": 4.4}, {"text": "parallel training. What we do is we", "start": 4578.0, "duration": 3.28}, {"text": "split our model into multiple parts and", "start": 4580.0, "duration": 2.8}, {"text": "assign each one to a GPU as we've", "start": 4581.28, "duration": 3.439}, {"text": "already discussed. And then we train", "start": 4582.8, "duration": 3.359}, {"text": "inserting communication steps at the", "start": 4584.719, "duration": 3.041}, {"text": "boundaries when we've split the model.", "start": 4586.159, "duration": 4.241}, {"text": "So I'll already kind of give a teaser.", "start": 4587.76, "duration": 4.08}, {"text": "This is the boundary where we have the", "start": 4590.4, "duration": 4.24}, {"text": "communication and then we only use node", "start": 4591.84, "duration": 4.8}, {"text": "to node communication which is sending", "start": 4594.64, "duration": 3.92}, {"text": "and receiving and don't need any", "start": 4596.64, "duration": 3.2}, {"text": "collective communication privileence", "start": 4598.56, "duration": 2.48}, {"text": "which is really nice. So, if you've ever", "start": 4599.84, "duration": 2.96}, {"text": "heard of an all gather, this is how you", "start": 4601.04, "duration": 3.04}, {"text": "can synchronize gradients between", "start": 4602.8, "duration": 3.359}, {"text": "different models. And this requires many", "start": 4604.08, "duration": 3.28}, {"text": "different GPUs to communicate", "start": 4606.159, "duration": 3.201}, {"text": "collectively. But because we're only", "start": 4607.36, "duration": 3.44}, {"text": "sending and receiving, it only requires", "start": 4609.36, "duration": 4.4}, {"text": "two GPUs. And what this means is that we", "start": 4610.8, "duration": 4.879}, {"text": "don't have to worry about um", "start": 4613.76, "duration": 4.08}, {"text": "disynchroniz or unsynchronization errors", "start": 4615.679, "duration": 4.881}, {"text": "or weird things happening because we're", "start": 4617.84, "duration": 4.24}, {"text": "sending things and receiving things", "start": 4620.56, "duration": 4.56}, {"text": "between many different GPUs. So, let's", "start": 4622.08, "duration": 4.88}, {"text": "go through the entire stage. First of", "start": 4625.12, "duration": 3.36}, {"text": "all, starting with the forward pass, we", "start": 4626.96, "duration": 4.16}, {"text": "compute intermediate on GP1. So this is", "start": 4628.48, "duration": 5.36}, {"text": "what's actually happening here. Um, and", "start": 4631.12, "duration": 4.48}, {"text": "then we transfer the resulting tensor to", "start": 4633.84, "duration": 3.68}, {"text": "GP2. This is the send operation here,", "start": 4635.6, "duration": 3.76}, {"text": "but it's also a corresponding receive", "start": 4637.52, "duration": 4.159}, {"text": "operation on this GPU. And once we", "start": 4639.36, "duration": 5.2}, {"text": "receive it, GPU GPU2 computes the loss", "start": 4641.679, "duration": 6.0}, {"text": "of the model in this stage here. And all", "start": 4644.56, "duration": 5.76}, {"text": "the while it's caching its activations", "start": 4647.679, "duration": 5.761}, {"text": "so that it can compute the backward pass", "start": 4650.32, "duration": 5.839}, {"text": "faster. And then during the backward", "start": 4653.44, "duration": 5.12}, {"text": "pass, okay, we're going to be going", "start": 4656.159, "duration": 4.241}, {"text": "downwards. So the GPU calculates the", "start": 4658.56, "duration": 4.0}, {"text": "derivatives of loss with respect to its", "start": 4660.4, "duration": 4.08}, {"text": "weights in the input. So the loss is", "start": 4662.56, "duration": 3.119}, {"text": "calculated with respect to its own", "start": 4664.48, "duration": 3.44}, {"text": "weights W4 and W3. But then it's also", "start": 4665.679, "duration": 3.361}, {"text": "calculated with respect to the", "start": 4667.92, "duration": 5.44}, {"text": "activations which are given to it by the", "start": 4669.04, "duration": 7.28}, {"text": "first model. And then GP1 once it once", "start": 4673.36, "duration": 5.68}, {"text": "it gets those um values those gradients", "start": 4676.32, "duration": 4.32}, {"text": "completes the backward pass by", "start": 4679.04, "duration": 4.48}, {"text": "calculating the derivatives of the loss", "start": 4680.64, "duration": 6.16}, {"text": "with respect to its own weight and", "start": 4683.52, "duration": 4.88}, {"text": "that's once again based on the gradients", "start": 4686.8, "duration": 3.919}, {"text": "that it was sent. So this is a very good", "start": 4688.4, "duration": 4.799}, {"text": "diagram by Simon Bow which explains this", "start": 4690.719, "duration": 4.881}, {"text": "entire process for G two GPUs, GPU 1 and", "start": 4693.199, "duration": 6.641}, {"text": "GPU 2. And this is a pebble graph which", "start": 4695.6, "duration": 6.8}, {"text": "will show us also three things which we", "start": 4699.84, "duration": 4.8}, {"text": "want to address throughout this course.", "start": 4702.4, "duration": 4.319}, {"text": "So first of all there's low GPU", "start": 4704.64, "duration": 4.32}, {"text": "utilization because at any given point", "start": 4706.719, "duration": 4.241}, {"text": "only one of the GPUs is being used. The", "start": 4708.96, "duration": 4.88}, {"text": "other GPU is waiting. So right now GPU 2", "start": 4710.96, "duration": 4.4}, {"text": "is doing something and GPU one is", "start": 4713.84, "duration": 3.28}, {"text": "waiting and now GPU 2 is doing nothing", "start": 4715.36, "duration": 3.839}, {"text": "and GPU 1 is doing something. And then", "start": 4717.12, "duration": 4.64}, {"text": "there's also no interle of communication", "start": 4719.199, "duration": 4.48}, {"text": "and computation because of this waiting.", "start": 4721.76, "duration": 3.52}, {"text": "But if you look at some of the more", "start": 4723.679, "duration": 4.161}, {"text": "advanced pipeline algorithms, you do", "start": 4725.28, "duration": 4.16}, {"text": "have multiple GPUs doing work at the", "start": 4727.84, "duration": 3.839}, {"text": "same time. This is possible if you", "start": 4729.44, "duration": 4.4}, {"text": "overlap communication with computation.", "start": 4731.679, "duration": 3.52}, {"text": "And then also has a high memory demand.", "start": 4733.84, "duration": 2.399}, {"text": "And why does it have a high memory", "start": 4735.199, "duration": 4.721}, {"text": "demand? This is because GPU one has to", "start": 4736.239, "duration": 5.92}, {"text": "store the activations for the entire", "start": 4739.92, "duration": 5.52}, {"text": "model in its um internal state while it", "start": 4742.159, "duration": 7.601}, {"text": "waits until GPU 2 will give us give it", "start": 4745.44, "duration": 6.4}, {"text": "the um gradients for the backward path.", "start": 4749.76, "duration": 4.72}, {"text": "So you can see this cache is full until", "start": 4751.84, "duration": 4.8}, {"text": "GPU 2 returns back with the gradients.", "start": 4754.48, "duration": 3.759}, {"text": "And this means it has a high memory", "start": 4756.64, "duration": 4.48}, {"text": "demand and we're kind of not actually", "start": 4758.239, "duration": 4.881}, {"text": "taking advantage of the fact that we're", "start": 4761.12, "duration": 4.32}, {"text": "having many GPUs because we still have", "start": 4763.12, "duration": 4.48}, {"text": "to store the cache the activations for", "start": 4765.44, "duration": 4.88}, {"text": "the entire model on GPU1 while we wait", "start": 4767.6, "duration": 5.36}, {"text": "for GPU 2 to get back to us. So these", "start": 4770.32, "duration": 8.08}, {"text": "are pretty unidal circumstances. And", "start": 4772.96, "duration": 7.44}, {"text": "even though these are un ideal, we're", "start": 4778.4, "duration": 3.52}, {"text": "still going to start off with the naive", "start": 4780.4, "duration": 3.92}, {"text": "model parallelism implementation because", "start": 4781.92, "duration": 5.2}, {"text": "it is quite intuitive and easy to", "start": 4784.32, "duration": 4.72}, {"text": "implement. And before we go and", "start": 4787.12, "duration": 4.64}, {"text": "implement the naive solution, I realized", "start": 4789.04, "duration": 5.76}, {"text": "that we didn't show you the graph of the", "start": 4791.76, "duration": 5.52}, {"text": "computation itself. So as we're going to", "start": 4794.8, "duration": 4.32}, {"text": "use four devices, you can see this is", "start": 4797.28, "duration": 4.16}, {"text": "how it's structured. This is not too", "start": 4799.12, "duration": 3.92}, {"text": "different from the pebble graph that we", "start": 4801.44, "duration": 4.0}, {"text": "had below except that it's having four", "start": 4803.04, "duration": 4.32}, {"text": "instead of two devices. But this graph", "start": 4805.44, "duration": 3.68}, {"text": "is more interesting in my opinion", "start": 4807.36, "duration": 4.48}, {"text": "because it's showing you essentially the", "start": 4809.12, "duration": 5.119}, {"text": "idle time as well as the order of the", "start": 4811.84, "duration": 4.24}, {"text": "forward and backward calls across the", "start": 4814.239, "duration": 3.601}, {"text": "four devices. So you can see that it's", "start": 4816.08, "duration": 3.68}, {"text": "mapped by color to each device. The", "start": 4817.84, "duration": 4.0}, {"text": "first device passes the batch through", "start": 4819.76, "duration": 3.76}, {"text": "the forward pass of its layers. The", "start": 4821.84, "duration": 3.68}, {"text": "second does the same, third, the fourth,", "start": 4823.52, "duration": 3.44}, {"text": "and then the fourth right away does the", "start": 4825.52, "duration": 4.24}, {"text": "backward of the loss that it calculated", "start": 4826.96, "duration": 4.96}, {"text": "at the end of the last forward pass. And", "start": 4829.76, "duration": 4.32}, {"text": "then passes all of those gradients in", "start": 4831.92, "duration": 4.319}, {"text": "between each step. So in between these", "start": 4834.08, "duration": 4.24}, {"text": "steps here we pass activations and then", "start": 4836.239, "duration": 3.44}, {"text": "in between these steps here we pass", "start": 4838.32, "duration": 3.28}, {"text": "gradients. This is just standard back", "start": 4839.679, "duration": 3.601}, {"text": "propagation. And then here we have the", "start": 4841.6, "duration": 3.92}, {"text": "optimizer step because we need to um", "start": 4843.28, "duration": 4.879}, {"text": "essentially synchronize this and do them", "start": 4845.52, "duration": 4.0}, {"text": "all at the same time so we don't have", "start": 4848.159, "duration": 4.321}, {"text": "any weird weight um differences between", "start": 4849.52, "duration": 5.12}, {"text": "the model weights. Of course the weights", "start": 4852.48, "duration": 3.92}, {"text": "are going to be different but any weird", "start": 4854.64, "duration": 3.599}, {"text": "synchronization errors when we update", "start": 4856.4, "duration": 2.88}, {"text": "the weights. We want to do the", "start": 4858.239, "duration": 2.801}, {"text": "optimizing step at the same time. And", "start": 4859.28, "duration": 3.52}, {"text": "then one other thing that I want to note", "start": 4861.04, "duration": 5.52}, {"text": "once again is you can see the massive", "start": 4862.8, "duration": 7.2}, {"text": "gap in between the forward call on the", "start": 4866.56, "duration": 5.04}, {"text": "first device on its batch and the", "start": 4870.0, "duration": 3.52}, {"text": "backward call. Right? There are in this", "start": 4871.6, "duration": 4.24}, {"text": "case six time steps between those two", "start": 4873.52, "duration": 3.92}, {"text": "devices. But if you increase the number", "start": 4875.84, "duration": 4.24}, {"text": "of GPUs and you'll increase the distance", "start": 4877.44, "duration": 5.04}, {"text": "between the forward and the backward. So", "start": 4880.08, "duration": 4.48}, {"text": "this means that for the entire six time", "start": 4882.48, "duration": 3.92}, {"text": "steps here and until this backward pass", "start": 4884.56, "duration": 5.28}, {"text": "is completed that the first device is", "start": 4886.4, "duration": 6.0}, {"text": "storing those cache activations here.", "start": 4889.84, "duration": 4.08}, {"text": "And this is once again just to", "start": 4892.4, "duration": 3.2}, {"text": "illustrate the point of the naive", "start": 4893.92, "duration": 4.799}, {"text": "solution being naive. And now that we've", "start": 4895.6, "duration": 6.079}, {"text": "done that we were initially going to go", "start": 4898.719, "duration": 4.721}, {"text": "ahead and implement the naive model", "start": 4901.679, "duration": 3.761}, {"text": "parallelism right away. But I realized", "start": 4903.44, "duration": 4.239}, {"text": "that it's much better if we include an", "start": 4905.44, "duration": 5.279}, {"text": "extra step. Step five now is going to be", "start": 4907.679, "duration": 5.441}, {"text": "um calculating main or rather completing", "start": 4910.719, "duration": 4.48}, {"text": "the main function. So it's going to be", "start": 4913.12, "duration": 4.079}, {"text": "not too difficult at all but we're just", "start": 4915.199, "duration": 3.361}, {"text": "going to do this so that we understand", "start": 4917.199, "duration": 4.081}, {"text": "how the orchestration of the pipeline is", "start": 4918.56, "duration": 5.44}, {"text": "going to occur and then just give you a", "start": 4921.28, "duration": 5.359}, {"text": "sneak peek. We have the naive pipeline", "start": 4924.0, "duration": 3.679}, {"text": "step function that we're going to have", "start": 4926.639, "duration": 2.801}, {"text": "to implement here and then the other two", "start": 4927.679, "duration": 4.081}, {"text": "which are going to conclude our course.", "start": 4929.44, "duration": 5.6}, {"text": "So that's what the next few functions", "start": 4931.76, "duration": 5.04}, {"text": "that we need to complete are going to", "start": 4935.04, "duration": 3.92}, {"text": "be. So, let's look at what our to-dos", "start": 4936.8, "duration": 5.28}, {"text": "are. First of all, as a quick rundown of", "start": 4938.96, "duration": 7.199}, {"text": "what main.py is, it's very similar to if", "start": 4942.08, "duration": 7.76}, {"text": "we come back and check manual, it's very", "start": 4946.159, "duration": 6.801}, {"text": "similar to this for loop here, except we", "start": 4949.84, "duration": 4.799}, {"text": "just implement it in a separate function", "start": 4952.96, "duration": 3.679}, {"text": "so that we have a better separation of", "start": 4954.639, "duration": 3.921}, {"text": "concerns. So, we have the same things", "start": 4956.639, "duration": 3.121}, {"text": "going on here with the batch size,", "start": 4958.56, "duration": 2.4}, {"text": "hidden dimension, total layers, and", "start": 4959.76, "duration": 4.08}, {"text": "steps. It's all the same except now we", "start": 4960.96, "duration": 4.16}, {"text": "do want to set up the distributed", "start": 4963.84, "duration": 3.04}, {"text": "environment. So, this is pretty simple.", "start": 4965.12, "duration": 3.44}, {"text": "We'll just do this right away before we", "start": 4966.88, "duration": 3.92}, {"text": "explain the rest of the code. And all we", "start": 4968.56, "duration": 5.2}, {"text": "do is call init distributed just as we", "start": 4970.8, "duration": 6.16}, {"text": "did with uh comms, not comms, but rather", "start": 4973.76, "duration": 6.56}, {"text": "ping pong right here. So the exact same", "start": 4976.96, "duration": 5.679}, {"text": "call and then also the exact same call", "start": 4980.32, "duration": 3.839}, {"text": "for communication. So communication", "start": 4982.639, "duration": 3.281}, {"text": "equals pipeline comms of rank and world", "start": 4984.159, "duration": 3.441}, {"text": "size. In this case, we're calling it", "start": 4985.92, "duration": 3.36}, {"text": "com. So just a little semantic", "start": 4987.6, "duration": 4.24}, {"text": "difference, but otherwise it's the same", "start": 4989.28, "duration": 4.08}, {"text": "function and class at the end of the", "start": 4991.84, "duration": 4.319}, {"text": "day. So we'll do this. And already step", "start": 4993.36, "duration": 5.279}, {"text": "one is done. So let's give ourselves a", "start": 4996.159, "duration": 5.52}, {"text": "paddle back. And then going on there's", "start": 4998.639, "duration": 4.321}, {"text": "something interesting that's happening", "start": 5001.679, "duration": 2.96}, {"text": "here. We still have the same manual seat", "start": 5002.96, "duration": 4.8}, {"text": "set to 42. However, let's read what this", "start": 5004.639, "duration": 5.281}, {"text": "says. Each rank needs to skip the random", "start": 5007.76, "duration": 4.24}, {"text": "numbers used by previous ranks. And then", "start": 5009.92, "duration": 4.56}, {"text": "we have this", "start": 5012.0, "duration": 5.44}, {"text": "strange range value which is equal to", "start": 5014.48, "duration": 6.0}, {"text": "rank times the total layers of world", "start": 5017.44, "duration": 4.4}, {"text": "size which is just the number of layers", "start": 5020.48, "duration": 4.0}, {"text": "that each model contains within our", "start": 5021.84, "duration": 5.28}, {"text": "pipeline parallel system multiplied by", "start": 5024.48, "duration": 5.44}, {"text": "two. And just to give you the answer on", "start": 5027.12, "duration": 4.32}, {"text": "why there's the two here because that's", "start": 5029.92, "duration": 3.2}, {"text": "where we can start off as the simplest", "start": 5031.44, "duration": 4.48}, {"text": "explanation. It's just because for every", "start": 5033.12, "duration": 5.36}, {"text": "single n.linear linear we need to", "start": 5035.92, "duration": 5.44}, {"text": "initialize both a weight and a bias for", "start": 5038.48, "duration": 5.6}, {"text": "the fully connected layer. So what we're", "start": 5041.36, "duration": 5.839}, {"text": "trying to do here is to say okay in", "start": 5044.08, "duration": 6.0}, {"text": "order to get the most similar results", "start": 5047.199, "duration": 7.201}, {"text": "possible to manual and um yeah just", "start": 5050.08, "duration": 6.559}, {"text": "manual and monolith right where we had", "start": 5054.4, "duration": 7.279}, {"text": "the same seed. The thing is since those", "start": 5056.639, "duration": 7.361}, {"text": "functions initialize their parameters", "start": 5061.679, "duration": 7.121}, {"text": "all in one um list or rather in one go", "start": 5064.0, "duration": 7.6}, {"text": "the random number generator will be", "start": 5068.8, "duration": 5.359}, {"text": "different because in this case what", "start": 5071.6, "duration": 4.96}, {"text": "we're going to do is instead if we come", "start": 5074.159, "duration": 5.52}, {"text": "back to model in step four we're going", "start": 5076.56, "duration": 5.2}, {"text": "to initialate it in initiate initialize", "start": 5079.679, "duration": 4.161}, {"text": "it sorry in four separate processes. So", "start": 5081.76, "duration": 4.16}, {"text": "every single time we're going to do a", "start": 5083.84, "duration": 4.399}, {"text": "for loop essentially from 0 to three and", "start": 5085.92, "duration": 4.319}, {"text": "then of course the last layer we add the", "start": 5088.239, "duration": 3.361}, {"text": "classification head. Where is the", "start": 5090.239, "duration": 3.361}, {"text": "classification head? Right here. But any", "start": 5091.6, "duration": 4.4}, {"text": "case, what I'm trying to say is that the", "start": 5093.6, "duration": 4.72}, {"text": "random number generator will only reach", "start": 5096.0, "duration": 7.36}, {"text": "four in the charted instance, whereas it", "start": 5098.32, "duration": 6.8}, {"text": "will go all the way up to 16 for", "start": 5103.36, "duration": 5.2}, {"text": "monolith and eight um for the first", "start": 5105.12, "duration": 6.48}, {"text": "eight in the part one of manual and then", "start": 5108.56, "duration": 5.52}, {"text": "from 8 to 15 or whatever for the second", "start": 5111.6, "duration": 6.16}, {"text": "part. And for a random number generator,", "start": 5114.08, "duration": 5.68}, {"text": "this is actually a differencemaker,", "start": 5117.76, "duration": 4.8}, {"text": "which is to say, you need to order um", "start": 5119.76, "duration": 4.479}, {"text": "the the generation of those random", "start": 5122.56, "duration": 4.72}, {"text": "numbers such that you skip the first", "start": 5124.239, "duration": 5.121}, {"text": "four. If you're in the second layer, you", "start": 5127.28, "duration": 3.84}, {"text": "skip the first eight that are generated", "start": 5129.36, "duration": 4.24}, {"text": "if you're in the third layer. Or if", "start": 5131.12, "duration": 4.24}, {"text": "you're on the last layer, you skip the", "start": 5133.6, "duration": 4.639}, {"text": "first 12 um models. So if I'm just going", "start": 5135.36, "duration": 4.16}, {"text": "to give you an example, this is going to", "start": 5138.239, "duration": 3.44}, {"text": "be the best way to illustrate it. For", "start": 5139.52, "duration": 5.92}, {"text": "rank zero, which is the first GPU, we", "start": 5141.679, "duration": 7.52}, {"text": "say for i in range zero and this means", "start": 5145.44, "duration": 5.84}, {"text": "that we don't actually consume any", "start": 5149.199, "duration": 3.681}, {"text": "random number states. So what's", "start": 5151.28, "duration": 2.8}, {"text": "happening here is that we are just", "start": 5152.88, "duration": 3.04}, {"text": "generating one random number which will", "start": 5154.08, "duration": 3.68}, {"text": "as it says here consume a state and", "start": 5155.92, "duration": 6.64}, {"text": "because the first GPU is the same um", "start": 5157.76, "duration": 7.76}, {"text": "same way for initializing the values as", "start": 5162.56, "duration": 6.639}, {"text": "the uh first GPU in our monolith which", "start": 5165.52, "duration": 5.84}, {"text": "is the only GPU then we can keep it the", "start": 5169.199, "duration": 3.761}, {"text": "same but as soon as we reach the second", "start": 5171.36, "duration": 4.16}, {"text": "GPU right which will have rank one it'll", "start": 5172.96, "duration": 10.4}, {"text": "be for i in range 1 * 16 / 4 so 1 * 4 *", "start": 5175.52, "duration": 11.04}, {"text": "2 because when we initialize an n linear", "start": 5183.36, "duration": 4.4}, {"text": "we initialize both the weights and the", "start": 5186.56, "duration": 3.76}, {"text": "biases. So for the first layer we'll do", "start": 5187.76, "duration": 6.399}, {"text": "um 1 * 4 * 2 which gives 8 and this", "start": 5190.32, "duration": 6.08}, {"text": "means that we'll consume eight random", "start": 5194.159, "duration": 5.52}, {"text": "numbers which would represent the first", "start": 5196.4, "duration": 5.839}, {"text": "eight um nn linear or the first four nn", "start": 5199.679, "duration": 3.921}, {"text": "linear weights and biases that we would", "start": 5202.239, "duration": 4.801}, {"text": "have initialized for the first GPU. So", "start": 5203.6, "duration": 5.36}, {"text": "all of this ramble is just to say that", "start": 5207.04, "duration": 4.639}, {"text": "we need to consume RNG states to skip", "start": 5208.96, "duration": 4.0}, {"text": "the random numbers used by previous", "start": 5211.679, "duration": 2.641}, {"text": "ranks since we're not actually", "start": 5212.96, "duration": 3.279}, {"text": "initializing the model all on one GPU.", "start": 5214.32, "duration": 4.08}, {"text": "We're initializ we're initializing it on", "start": 5216.239, "duration": 4.081}, {"text": "separate processes which cannot talk", "start": 5218.4, "duration": 4.4}, {"text": "with each other at all. So that's the", "start": 5220.32, "duration": 4.96}, {"text": "entire explanation here. And then one", "start": 5222.8, "duration": 3.839}, {"text": "other thing worth noting is that since", "start": 5225.28, "duration": 4.16}, {"text": "the same file is duplicated on four", "start": 5226.639, "duration": 4.961}, {"text": "instances when we want to print for", "start": 5229.44, "duration": 4.719}, {"text": "logging we need to only print for one of", "start": 5231.6, "duration": 5.119}, {"text": "the GPUs. So we just default to say if", "start": 5234.159, "duration": 4.161}, {"text": "rank equals equals zero then we'll print", "start": 5236.719, "duration": 3.601}, {"text": "start or else you'll print starting", "start": 5238.32, "duration": 5.04}, {"text": "micro um run four times which is just", "start": 5240.32, "duration": 5.359}, {"text": "unnecessary and verbose. So this is", "start": 5243.36, "duration": 3.6}, {"text": "another thing we're noting when you're", "start": 5245.679, "duration": 4.321}, {"text": "doing a pipeline um distributed run.", "start": 5246.96, "duration": 5.44}, {"text": "Okay. So the next thing we want to do is", "start": 5250.0, "duration": 4.0}, {"text": "just initialize initialize the sharded", "start": 5252.4, "duration": 3.6}, {"text": "model and that's going to be very simple", "start": 5254.0, "duration": 4.0}, {"text": "because we just call sharted MLP and", "start": 5256.0, "duration": 4.96}, {"text": "then we put in the dimensions and the", "start": 5258.0, "duration": 5.84}, {"text": "arguments u which all should be given to", "start": 5260.96, "duration": 6.32}, {"text": "us right now. So total layers is going", "start": 5263.84, "duration": 5.12}, {"text": "to be actually this one. We need to be", "start": 5267.28, "duration": 4.8}, {"text": "careful because it is not just total", "start": 5268.96, "duration": 5.92}, {"text": "layers. It is rather total layers or", "start": 5272.08, "duration": 3.76}, {"text": "actually no I think we do the", "start": 5274.88, "duration": 2.48}, {"text": "computation. Let's just make sure that", "start": 5275.84, "duration": 3.28}, {"text": "we do the computation. Yeah, we do the", "start": 5277.36, "duration": 3.12}, {"text": "computation total layers divide world", "start": 5279.12, "duration": 2.48}, {"text": "size inside of there. So we don't want", "start": 5280.48, "duration": 2.96}, {"text": "to do it a second time. All right. Um", "start": 5281.6, "duration": 3.92}, {"text": "why isn't dim working? H it's because", "start": 5283.44, "duration": 4.239}, {"text": "it's actually called hidden dim here.", "start": 5285.52, "duration": 4.48}, {"text": "And then what's next? Rank. All right.", "start": 5287.679, "duration": 4.161}, {"text": "Where is rank? We have that. And then", "start": 5290.0, "duration": 4.32}, {"text": "world size. So everything is very simple", "start": 5291.84, "duration": 5.2}, {"text": "in this case. And then now let's go", "start": 5294.32, "duration": 4.64}, {"text": "down. We initialize the optimizer here.", "start": 5297.04, "duration": 7.04}, {"text": "And whereas before with manual we had to", "start": 5298.96, "duration": 7.36}, {"text": "do the list of parameters. So let's find", "start": 5304.08, "duration": 4.72}, {"text": "that here. List of part one plus list of", "start": 5306.32, "duration": 7.52}, {"text": "part two. In this case, since our device", "start": 5308.8, "duration": 7.2}, {"text": "only exists on itself, we're not", "start": 5313.84, "duration": 3.839}, {"text": "actually doing the stupid manual thing", "start": 5316.0, "duration": 4.239}, {"text": "where we split two GPUs on the same", "start": 5317.679, "duration": 5.52}, {"text": "device. We only need to give the atom", "start": 5320.239, "duration": 4.721}, {"text": "optimizer here the parameters of this", "start": 5323.199, "duration": 3.281}, {"text": "model, not all models because we're only", "start": 5324.96, "duration": 2.8}, {"text": "concerned with optimizing on the", "start": 5326.48, "duration": 4.8}, {"text": "specific device that each um GPU is", "start": 5327.76, "duration": 5.12}, {"text": "working with or rather on the specific", "start": 5331.28, "duration": 4.48}, {"text": "model that each device has. Okay. And", "start": 5332.88, "duration": 4.799}, {"text": "then what's what's going on here? It", "start": 5335.76, "duration": 4.08}, {"text": "says only rank zero loads of data, which", "start": 5337.679, "duration": 3.281}, {"text": "is true. And we're just going to call", "start": 5339.84, "duration": 3.68}, {"text": "this fixed input. So if rank equals", "start": 5340.96, "duration": 6.64}, {"text": "equals zero, we'll do um torch.rand", "start": 5343.52, "duration": 6.48}, {"text": "rand n", "start": 5347.6, "duration": 5.76}, {"text": "and then the parameters are size. So we", "start": 5350.0, "duration": 7.04}, {"text": "just take the batch size which is 32 and", "start": 5353.36, "duration": 7.6}, {"text": "then the hidden dim which is 128.", "start": 5357.04, "duration": 8.84}, {"text": "And let's do that really quickly.", "start": 5360.96, "duration": 4.92}, {"text": "Okay. And I don't think it requires", "start": 5366.32, "duration": 4.56}, {"text": "anything else.", "start": 5368.4, "duration": 6.08}, {"text": "Okay. And then in the other case when", "start": 5370.88, "duration": 5.44}, {"text": "it's else we still want to set fixed", "start": 5374.48, "duration": 4.96}, {"text": "input so we don't get any errors. And", "start": 5376.32, "duration": 5.6}, {"text": "what we're going to do here", "start": 5379.44, "duration": 5.759}, {"text": "is set the fixed input to be the same as", "start": 5381.92, "duration": 5.759}, {"text": "the batch size. And now coming down here", "start": 5385.199, "duration": 5.601}, {"text": "to step five, it says only the last rank", "start": 5387.679, "duration": 6.401}, {"text": "needs the targets to calculate the loss.", "start": 5390.8, "duration": 7.04}, {"text": "And this is also true. So in this case", "start": 5394.08, "duration": 5.68}, {"text": "if the rank equals equals world size", "start": 5397.84, "duration": 3.92}, {"text": "minus one we want the model to learn to", "start": 5399.76, "duration": 4.32}, {"text": "classify these random vectors and", "start": 5401.76, "duration": 4.64}, {"text": "therefore we're going to do torch.rand", "start": 5404.08, "duration": 4.559}, {"text": "int because we just want to class", "start": 5406.4, "duration": 5.12}, {"text": "between one and zero and one sorry. So", "start": 5408.639, "duration": 5.04}, {"text": "zero and then two because it is the", "start": 5411.52, "duration": 6.32}, {"text": "upper um exclusive on the upper bound.", "start": 5413.679, "duration": 7.921}, {"text": "And we should put this in a", "start": 5417.84, "duration": 6.399}, {"text": "um set of brackets that we can also give", "start": 5421.6, "duration": 6.079}, {"text": "it the number of sorry rather the size", "start": 5424.239, "duration": 8.081}, {"text": "of the uh the size of the vector that we", "start": 5427.679, "duration": 6.721}, {"text": "want. And I just realized it says that", "start": 5432.32, "duration": 5.2}, {"text": "we should not um input them in brackets.", "start": 5434.4, "duration": 5.04}, {"text": "We should rather just do 02 batch size.", "start": 5437.52, "duration": 3.76}, {"text": "Okay. And then fix target. Otherwise,", "start": 5439.44, "duration": 4.88}, {"text": "it's just none. Very nice.", "start": 5441.28, "duration": 4.64}, {"text": "And one other thing I just realized I", "start": 5444.32, "duration": 4.48}, {"text": "minced is that since this is of course a", "start": 5445.92, "duration": 6.239}, {"text": "CUDA compatible course, we need to move", "start": 5448.8, "duration": 6.24}, {"text": "these onto the device if we are using", "start": 5452.159, "duration": 5.681}, {"text": "GPUs. So whenever you're moving things", "start": 5455.04, "duration": 5.199}, {"text": "onto device or thinking about whether or", "start": 5457.84, "duration": 4.72}, {"text": "not you need to, it's always the models", "start": 5460.239, "duration": 4.241}, {"text": "and the tensors that need to be moved", "start": 5462.56, "duration": 5.76}, {"text": "onto a device. if you are using", "start": 5464.48, "duration": 5.759}, {"text": "something that is not the default which", "start": 5468.32, "duration": 4.319}, {"text": "is of course CPU's default otherwise you", "start": 5470.239, "duration": 5.041}, {"text": "need to have that here and if you", "start": 5472.639, "duration": 4.241}, {"text": "remember the device is returned by this", "start": 5475.28, "duration": 3.52}, {"text": "init distributed method so that's all", "start": 5476.88, "duration": 5.52}, {"text": "fine and dandy and in fact right now", "start": 5478.8, "duration": 5.04}, {"text": "that's everything that we needed to do", "start": 5482.4, "duration": 4.239}, {"text": "in order to run main.py pi. Now, if I", "start": 5483.84, "duration": 5.68}, {"text": "come down here to show you how the", "start": 5486.639, "duration": 5.441}, {"text": "actual runner training loop works, it's", "start": 5489.52, "duration": 6.4}, {"text": "almost identical to the one we set up in", "start": 5492.08, "duration": 7.599}, {"text": "manual, but the only difference is line", "start": 5495.92, "duration": 7.68}, {"text": "50. So, let's actually come and open up", "start": 5499.679, "duration": 6.721}, {"text": "coms, not coms, but rather manual.py, so", "start": 5503.6, "duration": 5.119}, {"text": "I can show you guys the difference.", "start": 5506.4, "duration": 6.08}, {"text": "So if we come here we can observe okay", "start": 5508.719, "duration": 5.601}, {"text": "there was this retain grad stuff which", "start": 5512.48, "duration": 3.6}, {"text": "once again we had to do because we were", "start": 5514.32, "duration": 4.08}, {"text": "manually uh doing pipeline parallelism", "start": 5516.08, "duration": 4.639}, {"text": "but more importantly I want to I want to", "start": 5518.4, "duration": 4.96}, {"text": "look at the loss here. So these are the", "start": 5520.719, "duration": 5.681}, {"text": "two lines that are only one line here", "start": 5523.36, "duration": 5.92}, {"text": "and furthermore we can see that the", "start": 5526.4, "duration": 6.48}, {"text": "optimizer step in this case is the same.", "start": 5529.28, "duration": 5.439}, {"text": "So we don't have any differences with", "start": 5532.88, "duration": 4.16}, {"text": "the optimizer step and this is because", "start": 5534.719, "duration": 5.44}, {"text": "um we just optimize at the very end of", "start": 5537.04, "duration": 6.88}, {"text": "the pipeline and it's the same whether", "start": 5540.159, "duration": 6.241}, {"text": "or not you're doing any special pipeline", "start": 5543.92, "duration": 4.64}, {"text": "parallelism or not. It has to be synced", "start": 5546.4, "duration": 4.88}, {"text": "across all devices anyways. But what the", "start": 5548.56, "duration": 6.8}, {"text": "difference is is in this case we get our", "start": 5551.28, "duration": 6.08}, {"text": "loss from part two and then call loss", "start": 5555.36, "duration": 4.64}, {"text": "backward. But because we have to", "start": 5557.36, "duration": 5.12}, {"text": "orchestrate all the communications in", "start": 5560.0, "duration": 5.76}, {"text": "pipeline parallelism manually, then we", "start": 5562.48, "duration": 6.4}, {"text": "don't call loss. Within this training", "start": 5565.76, "duration": 6.24}, {"text": "loop, we instead call the pipeline step", "start": 5568.88, "duration": 5.04}, {"text": "function, which if I just look at the", "start": 5572.0, "duration": 4.719}, {"text": "dock string, it does a single training", "start": 5573.92, "duration": 5.279}, {"text": "step using the naiveuler.", "start": 5576.719, "duration": 4.401}, {"text": "So it's inside of the naiveuler that", "start": 5579.199, "duration": 3.681}, {"text": "will actually do the backward pass. Um,", "start": 5581.12, "duration": 4.32}, {"text": "and then here we just return loss as the", "start": 5582.88, "duration": 7.2}, {"text": "final output of this main.py run.", "start": 5585.44, "duration": 6.4}, {"text": "However, what I didn't code which is", "start": 5590.08, "duration": 4.0}, {"text": "better in terms of semantics is the fact", "start": 5591.84, "duration": 4.96}, {"text": "that the loss is only returned if we're", "start": 5594.08, "duration": 4.559}, {"text": "on the last device because only the last", "start": 5596.8, "duration": 3.839}, {"text": "device can compute the loss. So", "start": 5598.639, "duration": 4.241}, {"text": "otherwise the knife pipeline step", "start": 5600.639, "duration": 4.161}, {"text": "actually just returns none. Which is", "start": 5602.88, "duration": 5.92}, {"text": "because if you are a device that is not", "start": 5604.8, "duration": 6.72}, {"text": "the last one then you actually don't", "start": 5608.8, "duration": 4.399}, {"text": "return anything within the training", "start": 5611.52, "duration": 5.119}, {"text": "loop. You just pass your forward um", "start": 5613.199, "duration": 5.44}, {"text": "activations to the next device or you", "start": 5616.639, "duration": 3.281}, {"text": "pass your gradients to the previous", "start": 5618.639, "duration": 3.04}, {"text": "device and then the last device is only", "start": 5619.92, "duration": 3.36}, {"text": "one that's telling us what the loss is", "start": 5621.679, "duration": 4.081}, {"text": "in order to measure the training", "start": 5623.28, "duration": 4.879}, {"text": "progress. So with that being said, we", "start": 5625.76, "duration": 7.919}, {"text": "need to check if world size minus one", "start": 5628.159, "duration": 8.241}, {"text": "is equal. So not that but rather if rank", "start": 5633.679, "duration": 4.96}, {"text": "is equal to world size minus one. If", "start": 5636.4, "duration": 5.04}, {"text": "rank minus one is equal to world size", "start": 5638.639, "duration": 6.0}, {"text": "then we will do this one and then else", "start": 5641.44, "duration": 5.68}, {"text": "it will just be the exact same thing", "start": 5644.639, "duration": 5.281}, {"text": "except it does not return anything. So", "start": 5647.12, "duration": 5.119}, {"text": "we won't even capture it. So there's no", "start": 5649.92, "duration": 3.36}, {"text": "difference between what we had", "start": 5652.239, "duration": 2.881}, {"text": "previously and what we have now. But it", "start": 5653.28, "duration": 4.08}, {"text": "is just semantically better because we", "start": 5655.12, "duration": 4.32}, {"text": "want only to return and capture this", "start": 5657.36, "duration": 3.2}, {"text": "loss when we're on the last device.", "start": 5659.44, "duration": 4.239}, {"text": "Otherwise, we just do a training step.", "start": 5660.56, "duration": 5.84}, {"text": "So we can't actually test this yet", "start": 5663.679, "duration": 4.881}, {"text": "because we actually haven't implemented", "start": 5666.4, "duration": 4.16}, {"text": "this yet since this is from step", "start": 5668.56, "duration": 6.24}, {"text": "sixuler. So now the logic step is to", "start": 5670.56, "duration": 6.72}, {"text": "come ahead and open up step", "start": 5674.8, "duration": 4.96}, {"text": "six_chedul.py I and see what we need to", "start": 5677.28, "duration": 5.439}, {"text": "implement in the naive pipeline step. So", "start": 5679.76, "duration": 4.399}, {"text": "we already read the dock string here but", "start": 5682.719, "duration": 3.52}, {"text": "let's look at the todos. First of all we", "start": 5684.159, "duration": 4.0}, {"text": "need to receive input from the previous", "start": 5686.239, "duration": 4.641}, {"text": "stage if we're not the first stage. Then", "start": 5688.159, "duration": 4.08}, {"text": "we want to forward the batch to the", "start": 5690.88, "duration": 4.24}, {"text": "model. Send output to next stage if not", "start": 5692.239, "duration": 4.96}, {"text": "the last stage. And then that's all", "start": 5695.12, "duration": 3.84}, {"text": "forward. And then now look at the", "start": 5697.199, "duration": 3.841}, {"text": "backward pass. If we're the last stage", "start": 5698.96, "duration": 3.36}, {"text": "we want to compute loss and call", "start": 5701.04, "duration": 6.08}, {"text": "backward on it. Then uh if not we're not", "start": 5702.32, "duration": 6.24}, {"text": "if we're not the last stage. So for any", "start": 5707.12, "duration": 2.72}, {"text": "if we're any other stage, we're going to", "start": 5708.56, "duration": 2.8}, {"text": "receive the gradient from the next stage", "start": 5709.84, "duration": 4.16}, {"text": "and call backward and then send the", "start": 5711.36, "duration": 3.92}, {"text": "gradient to previous stage if not the", "start": 5714.0, "duration": 3.52}, {"text": "first stage and then return loss if last", "start": 5715.28, "duration": 5.439}, {"text": "stage else none. So if we come back to", "start": 5717.52, "duration": 6.96}, {"text": "main, the reason once again why I have", "start": 5720.719, "duration": 5.601}, {"text": "loss equals this when we're the last one", "start": 5724.48, "duration": 3.759}, {"text": "and nothing else otherwise is because", "start": 5726.32, "duration": 5.52}, {"text": "this will return none anyways. So there", "start": 5728.239, "duration": 6.96}, {"text": "really is nothing to return. And I", "start": 5731.84, "duration": 5.2}, {"text": "didn't look actually I didn't explain", "start": 5735.199, "duration": 3.281}, {"text": "the last part of main.py. So let's", "start": 5737.04, "duration": 4.72}, {"text": "really quickly do that. If rank is zero", "start": 5738.48, "duration": 5.84}, {"text": "then we will just print train complete", "start": 5741.76, "duration": 5.12}, {"text": "the the time it took and the final loss.", "start": 5744.32, "duration": 6.0}, {"text": "Whereas before we didn't check um and we", "start": 5746.88, "duration": 5.2}, {"text": "just printed it. Once again since we", "start": 5750.32, "duration": 3.2}, {"text": "don't want to print this four times for", "start": 5752.08, "duration": 4.4}, {"text": "every single model um then we have to", "start": 5753.52, "duration": 5.52}, {"text": "just uh print it once. And now I just", "start": 5756.48, "duration": 5.12}, {"text": "saw another error here is that if rank", "start": 5759.04, "duration": 4.88}, {"text": "equals world size is minus one. We can", "start": 5761.6, "duration": 4.639}, {"text": "only do this because just as a reminder,", "start": 5763.92, "duration": 6.4}, {"text": "we only have the loss for the last model", "start": 5766.239, "duration": 6.96}, {"text": "and otherwise the loss does not um have", "start": 5770.32, "duration": 6.24}, {"text": "any existence for models before it. And", "start": 5773.199, "duration": 5.281}, {"text": "one last thing as well is that since", "start": 5776.56, "duration": 4.0}, {"text": "we're just returning a scalar loss, then", "start": 5778.48, "duration": 3.759}, {"text": "it's not a tensor. So we should actually", "start": 5780.56, "duration": 4.24}, {"text": "remove this item call here. Okay. And", "start": 5782.239, "duration": 4.4}, {"text": "then the very last thing is that for", "start": 5784.8, "duration": 3.919}, {"text": "every single group you want to destroy", "start": 5786.639, "duration": 4.161}, {"text": "the process group which just ends the", "start": 5788.719, "duration": 4.721}, {"text": "communication um conference call between", "start": 5790.8, "duration": 5.439}, {"text": "those different devices. Okay. So let's", "start": 5793.44, "duration": 5.92}, {"text": "now jump into step step six schedule. So", "start": 5796.239, "duration": 6.561}, {"text": "if coms.rank is zero then we will", "start": 5799.36, "duration": 5.359}, {"text": "receive the batch data directly or else", "start": 5802.8, "duration": 3.359}, {"text": "we'll receive the activations from the", "start": 5804.719, "duration": 3.041}, {"text": "previous layer. Let's close this since", "start": 5806.159, "duration": 3.201}, {"text": "we no longer need that. And we can", "start": 5807.76, "duration": 3.919}, {"text": "actually just uncomment this because it", "start": 5809.36, "duration": 4.4}, {"text": "already has some of the logic for us. So", "start": 5811.679, "duration": 5.04}, {"text": "let's just go like this and have the", "start": 5813.76, "duration": 6.64}, {"text": "first condition here and then else uh", "start": 5816.719, "duration": 6.561}, {"text": "right there. Okay. So first of all we'll", "start": 5820.4, "duration": 4.96}, {"text": "say input", "start": 5823.28, "duration": 5.359}, {"text": "data equals batch. So the batch is just", "start": 5825.36, "duration": 6.4}, {"text": "that exactly. If we come here um the", "start": 5828.639, "duration": 7.121}, {"text": "fixed input is just the torch.rand n of", "start": 5831.76, "duration": 7.2}, {"text": "the batch as hidden mentioned. Okay. So", "start": 5835.76, "duration": 6.479}, {"text": "that is pretty simple and if not we need", "start": 5838.96, "duration": 5.92}, {"text": "to use coms to receive the data. So", "start": 5842.239, "duration": 5.041}, {"text": "coms.receive", "start": 5844.88, "duration": 4.88}, {"text": "forward is the method and it requires", "start": 5847.28, "duration": 4.72}, {"text": "shape device and DT type. DT type being", "start": 5849.76, "duration": 5.12}, {"text": "optional but we do want to include the", "start": 5852.0, "duration": 4.96}, {"text": "shape. So we have to set the shape which", "start": 5854.88, "duration": 4.0}, {"text": "I haven't done yet and we'll do that", "start": 5856.96, "duration": 4.239}, {"text": "just right here.", "start": 5858.88, "duration": 4.72}, {"text": "Okay. Shape", "start": 5861.199, "duration": 7.04}, {"text": "is going to be what? shape equals we're", "start": 5863.6, "duration": 7.76}, {"text": "just going to take the batch here and", "start": 5868.239, "duration": 6.641}, {"text": "say shape equals batch by hidden dim", "start": 5871.36, "duration": 4.799}, {"text": "because that is the shape of the", "start": 5874.88, "duration": 3.359}, {"text": "temperature that sorry the tensor itself", "start": 5876.159, "duration": 3.841}, {"text": "and you may be confused because in this", "start": 5878.239, "duration": 4.48}, {"text": "case we have batch already equal to", "start": 5880.0, "duration": 6.159}, {"text": "batch by hidden dim right because if you", "start": 5882.719, "duration": 5.281}, {"text": "look at this naming convention here we", "start": 5886.159, "duration": 3.841}, {"text": "have fixed input is batch size by hidden", "start": 5888.0, "duration": 5.119}, {"text": "dim but this is a tensor of random", "start": 5890.0, "duration": 4.88}, {"text": "integers of batch by hidden dim whereas", "start": 5893.119, "duration": 5.281}, {"text": "here if it's not rank equal equals zero", "start": 5894.88, "duration": 7.12}, {"text": "then as I mentioned before we will take", "start": 5898.4, "duration": 6.08}, {"text": "fixed input and just set it to batch", "start": 5902.0, "duration": 4.32}, {"text": "size because if you remember in ping", "start": 5904.48, "duration": 3.52}, {"text": "pong right we actually don't know the", "start": 5906.32, "duration": 3.04}, {"text": "batch size if we're on a different", "start": 5908.0, "duration": 2.32}, {"text": "another tensor because they're", "start": 5909.36, "duration": 3.6}, {"text": "completely separate processes but if we", "start": 5910.32, "duration": 5.12}, {"text": "just send this um as the fixed input", "start": 5912.96, "duration": 3.759}, {"text": "which is technically incorrect right", "start": 5915.44, "duration": 2.48}, {"text": "because this is not an input this is", "start": 5916.719, "duration": 2.641}, {"text": "just telling us what the batch size is", "start": 5917.92, "duration": 2.799}, {"text": "as long as we know that we're doing this", "start": 5919.36, "duration": 3.44}, {"text": "weird trick where we're setting the", "start": 5920.719, "duration": 4.801}, {"text": "batch size as the fixed input when it's", "start": 5922.8, "duration": 5.2}, {"text": "not the first device then this is fine", "start": 5925.52, "duration": 4.639}, {"text": "and what it allows us to do is know the", "start": 5928.0, "duration": 5.92}, {"text": "shape of each device such that we can", "start": 5930.159, "duration": 6.401}, {"text": "create a tensor of zeros within our", "start": 5933.92, "duration": 4.799}, {"text": "communication pipelines comm so that we", "start": 5936.56, "duration": 5.28}, {"text": "can receive that tensor itself. Okay. So", "start": 5938.719, "duration": 6.48}, {"text": "now that we have the shape and have", "start": 5941.84, "duration": 7.04}, {"text": "received that forward tensor um we", "start": 5945.199, "duration": 6.721}, {"text": "should actually call this the input and", "start": 5948.88, "duration": 5.6}, {"text": "we'll just use the same name input data.", "start": 5951.92, "duration": 3.92}, {"text": "Now we can do the forward pass through", "start": 5954.48, "duration": 3.84}, {"text": "the model. So this is pretty standard.", "start": 5955.84, "duration": 4.879}, {"text": "We just do where's the model? Um it's", "start": 5958.32, "duration": 5.04}, {"text": "it's just called model and then we will", "start": 5960.719, "duration": 6.161}, {"text": "just input data. Boom. And what we what", "start": 5963.36, "duration": 4.56}, {"text": "do we want to call this? We want to call", "start": 5966.88, "duration": 5.2}, {"text": "this output. So already we have the", "start": 5967.92, "duration": 7.92}, {"text": "forward pass done and dusted. So pretty", "start": 5972.08, "duration": 6.8}, {"text": "well. And then now we need to say if", "start": 5975.84, "duration": 4.879}, {"text": "we're not in the last stage, let's send", "start": 5978.88, "duration": 3.759}, {"text": "the output to the next stage. So this is", "start": 5980.719, "duration": 4.881}, {"text": "important as well. Because if we just", "start": 5982.639, "duration": 4.721}, {"text": "calculate the output for each stage,", "start": 5985.6, "duration": 3.76}, {"text": "then without sending it, we'll only get", "start": 5987.36, "duration": 3.359}, {"text": "up to the first GPU because it will", "start": 5989.36, "duration": 6.16}, {"text": "never send its data. So let's say if", "start": 5990.719, "duration": 8.48}, {"text": "model dot rank", "start": 5995.52, "duration": 7.679}, {"text": "is not equal to world size, model.world", "start": 5999.199, "duration": 7.0}, {"text": "size", "start": 6003.199, "duration": 3.0}, {"text": "u minus one,", "start": 6006.88, "duration": 5.04}, {"text": "then we will send the output to the next", "start": 6009.36, "duration": 6.799}, {"text": "stage. So, we're going to do a coms dot", "start": 6011.92, "duration": 8.0}, {"text": "send forward. Oh, I spelled comms wrong.", "start": 6016.159, "duration": 6.321}, {"text": "Send forward.", "start": 6019.92, "duration": 4.88}, {"text": "And this is going to be on the output.", "start": 6022.48, "duration": 4.159}, {"text": "And once again, the send does not return", "start": 6024.8, "duration": 3.52}, {"text": "anything. So, we don't need to capture", "start": 6026.639, "duration": 4.721}, {"text": "any um return type. It's just none. So", "start": 6028.32, "duration": 6.0}, {"text": "already this one is also out of the way.", "start": 6031.36, "duration": 6.16}, {"text": "And there's two things that I just", "start": 6034.32, "duration": 5.6}, {"text": "forgot. So the first thing is that when", "start": 6037.52, "duration": 4.719}, {"text": "we send output to the next stage, if", "start": 6039.92, "duration": 3.44}, {"text": "it's not the last stage, we should", "start": 6042.239, "duration": 4.241}, {"text": "detach it. So if I just do detach here,", "start": 6043.36, "duration": 4.799}, {"text": "this will detach this output from the", "start": 6046.48, "duration": 4.239}, {"text": "computational graph. And what why", "start": 6048.159, "duration": 4.401}, {"text": "exactly are we doing this? I have this", "start": 6050.719, "duration": 3.281}, {"text": "explanation right here. So I'm just", "start": 6052.56, "duration": 2.96}, {"text": "going to go through it together with you", "start": 6054.0, "duration": 4.08}, {"text": "guys under detach. So let's see what it", "start": 6055.52, "duration": 4.88}, {"text": "says. When you send a tensor via disend,", "start": 6058.08, "duration": 4.079}, {"text": "the data travels over a net network", "start": 6060.4, "duration": 4.64}, {"text": "cable or between CPU processes. and the", "start": 6062.159, "duration": 4.401}, {"text": "graph which represents the memory", "start": 6065.04, "duration": 3.119}, {"text": "pointers that link one layer to the next", "start": 6066.56, "duration": 3.92}, {"text": "cannot travel through the cable because", "start": 6068.159, "duration": 4.321}, {"text": "they only exist in the local RAM of the", "start": 6070.48, "duration": 4.32}, {"text": "sending GPU because essentially all of", "start": 6072.48, "duration": 5.44}, {"text": "this PyTorch computational graph and", "start": 6074.8, "duration": 6.24}, {"text": "autograd memory is just stored in RAM.", "start": 6077.92, "duration": 6.319}, {"text": "And so to prevent memory leaks, if you", "start": 6081.04, "duration": 5.84}, {"text": "do not call detach on the output tensor,", "start": 6084.239, "duration": 3.521}, {"text": "it will remain hooked to the", "start": 6086.88, "duration": 2.4}, {"text": "computational graph of the current GPU.", "start": 6087.76, "duration": 2.879}, {"text": "And PyTorch will try to keep all the", "start": 6089.28, "duration": 2.8}, {"text": "activations of all previous layers in", "start": 6090.639, "duration": 2.881}, {"text": "memory because it thinks you might call", "start": 6092.08, "duration": 2.8}, {"text": "backward on that specific output", "start": 6093.52, "duration": 3.599}, {"text": "variable later, which leads to a massive", "start": 6094.88, "duration": 4.08}, {"text": "memory leak.", "start": 6097.119, "duration": 3.681}, {"text": "And by detaching, you are explicitly", "start": 6098.96, "duration": 3.44}, {"text": "saying, I am done with this forward pass", "start": 6100.8, "duration": 3.28}, {"text": "locally. I'm handing off a static copy", "start": 6102.4, "duration": 5.279}, {"text": "of the data to the next device. And if", "start": 6104.08, "duration": 6.079}, {"text": "we look at it right here, it says the", "start": 6107.679, "duration": 4.321}, {"text": "next device now gets a clean tensor and", "start": 6110.159, "duration": 3.361}, {"text": "restarts the graph using required grat", "start": 6112.0, "duration": 4.159}, {"text": "equals true. And then for the backward", "start": 6113.52, "duration": 5.28}, {"text": "pass, which we'll also do, we manually", "start": 6116.159, "duration": 4.241}, {"text": "reconnect the chain later when we", "start": 6118.8, "duration": 3.68}, {"text": "receive the gradient and call output", "start": 6120.4, "duration": 3.2}, {"text": "backward receive grat. So we haven't", "start": 6122.48, "duration": 2.8}, {"text": "done this yet, but stay tuned. So in", "start": 6123.6, "duration": 3.2}, {"text": "summary, you don't want the next GPU to", "start": 6125.28, "duration": 2.879}, {"text": "have ghost references to memory that it", "start": 6126.8, "duration": 3.28}, {"text": "cannot access. Instead, you give it the", "start": 6128.159, "duration": 3.841}, {"text": "activations and keep the history locally", "start": 6130.08, "duration": 3.44}, {"text": "for when the backward pass eventually", "start": 6132.0, "duration": 4.08}, {"text": "returns. Okay. And one other thing that", "start": 6133.52, "duration": 5.84}, {"text": "I didn't um get to just yet is the fact", "start": 6136.08, "duration": 8.32}, {"text": "that in our schedule we must do the", "start": 6139.36, "duration": 8.4}, {"text": "following. You say input data.requires", "start": 6144.4, "duration": 7.839}, {"text": "grad equals true as well when we um", "start": 6147.76, "duration": 6.479}, {"text": "receive this from the previous model.", "start": 6152.239, "duration": 3.92}, {"text": "And why is this the case? Well, I also", "start": 6154.239, "duration": 4.96}, {"text": "have the explanation just above. Okay,", "start": 6156.159, "duration": 5.281}, {"text": "so we're just going to explain it by", "start": 6159.199, "duration": 6.721}, {"text": "considering the two um situations", "start": 6161.44, "duration": 5.92}, {"text": "requires got equals true and requires", "start": 6165.92, "duration": 3.12}, {"text": "got equals false. So just to remind you", "start": 6167.36, "duration": 2.96}, {"text": "guys what's actually happening here in", "start": 6169.04, "duration": 4.88}, {"text": "the code, it's saying that once we get", "start": 6170.32, "duration": 7.44}, {"text": "our shape and the input data, we need to", "start": 6173.92, "duration": 5.52}, {"text": "set it to requires grad equals true", "start": 6177.76, "duration": 4.479}, {"text": "because we detach it actually in this", "start": 6179.44, "duration": 4.48}, {"text": "case. So when you detach a value, the", "start": 6182.239, "duration": 3.681}, {"text": "requires grad is equal to false by", "start": 6183.92, "duration": 3.44}, {"text": "default. So we need to set it back to", "start": 6185.92, "duration": 4.88}, {"text": "true. And let's see through this setup", "start": 6187.36, "duration": 6.96}, {"text": "why that's the case. When we set recog,", "start": 6190.8, "duration": 4.8}, {"text": "we allow the chain of math to stay", "start": 6194.32, "duration": 2.96}, {"text": "connected across the two devices. So if", "start": 6195.6, "duration": 3.28}, {"text": "we have only two devices, let's say that", "start": 6197.28, "duration": 3.76}, {"text": "rank zero performs a calculation A * B", "start": 6198.88, "duration": 4.96}, {"text": "equals C and its weights are B and the", "start": 6201.04, "duration": 6.639}, {"text": "input is A. Now rank one receives C. We", "start": 6203.84, "duration": 7.04}, {"text": "manually set C.IUS equals to true. Now", "start": 6207.679, "duration": 5.44}, {"text": "rank one performs its own calculation", "start": 6210.88, "duration": 4.72}, {"text": "which is C * D equals output. And then", "start": 6213.119, "duration": 4.0}, {"text": "when we call backward on rank one,", "start": 6215.6, "duration": 3.119}, {"text": "pietorch calculates the gradients for", "start": 6217.119, "duration": 4.0}, {"text": "its weight D and for the and with", "start": 6218.719, "duration": 4.081}, {"text": "respect to input C because we set", "start": 6221.119, "duration": 4.321}, {"text": "C.Rires get equals to true. And the", "start": 6222.8, "duration": 5.439}, {"text": "result is rank one now has a value for", "start": 6225.44, "duration": 5.199}, {"text": "the gradient of C. And when it calls", "start": 6228.239, "duration": 4.801}, {"text": "send backward of those gradients to rank", "start": 6230.639, "duration": 4.961}, {"text": "zero, rank zero receives those gradients", "start": 6233.04, "duration": 3.84}, {"text": "and can now calculate the gradients for", "start": 6235.6, "duration": 3.28}, {"text": "its own weights B and can now update it", "start": 6236.88, "duration": 4.88}, {"text": "in the optimiz optimizer step. But in", "start": 6238.88, "duration": 4.4}, {"text": "the counterfactual where you have", "start": 6241.76, "duration": 4.24}, {"text": "requires got equals to false. Then rank", "start": 6243.28, "duration": 4.64}, {"text": "one treats incoming data as a constant", "start": 6246.0, "duration": 3.679}, {"text": "rather than a variable because we", "start": 6247.92, "duration": 3.6}, {"text": "detached it. So it does not track", "start": 6249.679, "duration": 4.56}, {"text": "gradients anymore. So rank zero send C", "start": 6251.52, "duration": 4.88}, {"text": "to rank one. Rank one receives C. By", "start": 6254.239, "duration": 4.081}, {"text": "default requires grad equals to false.", "start": 6256.4, "duration": 6.719}, {"text": "So rank one calculates its output C * D.", "start": 6258.32, "duration": 6.48}, {"text": "And then when we call backward on rank", "start": 6263.119, "duration": 3.6}, {"text": "one, PyTorch calculates the grains for", "start": 6264.8, "duration": 3.68}, {"text": "its weights D. And you can actually", "start": 6266.719, "duration": 3.52}, {"text": "optim up update this in the optimizer", "start": 6268.48, "duration": 3.92}, {"text": "step. But then because C was marked as a", "start": 6270.239, "duration": 3.601}, {"text": "constant, no grad required. The engine", "start": 6272.4, "duration": 3.44}, {"text": "stops there and the result is C.GRAD is", "start": 6273.84, "duration": 3.839}, {"text": "none. So rank one has nothing to send", "start": 6275.84, "duration": 3.6}, {"text": "back to rank zero in the backward pass.", "start": 6277.679, "duration": 3.681}, {"text": "And therefore rank zero never receives a", "start": 6279.44, "duration": 3.6}, {"text": "gradient. So its weights B are never", "start": 6281.36, "duration": 4.4}, {"text": "updated. Only half the model learns.", "start": 6283.04, "duration": 4.24}, {"text": "Essentially requires grad equals 2", "start": 6285.76, "duration": 2.64}, {"text": "creates a hook at the very edge of the", "start": 6287.28, "duration": 2.56}, {"text": "devices memory. And without that hook,", "start": 6288.4, "duration": 2.88}, {"text": "the backward pass is nothing to grab", "start": 6289.84, "duration": 3.359}, {"text": "onto to pull the information back across", "start": 6291.28, "duration": 3.52}, {"text": "the network to other device. And this is", "start": 6293.199, "duration": 3.601}, {"text": "incredibly important to note when we're", "start": 6294.8, "duration": 3.6}, {"text": "doing pipeline parallelism. This is no", "start": 6296.8, "duration": 2.96}, {"text": "longer done automatically for you by", "start": 6298.4, "duration": 2.799}, {"text": "autograph. So you need to do it on your", "start": 6299.76, "duration": 4.8}, {"text": "own. Okay. And now that we have this", "start": 6301.199, "duration": 5.281}, {"text": "properly set up, the entire forward pass", "start": 6304.56, "duration": 3.92}, {"text": "is done. And now we just have to do the", "start": 6306.48, "duration": 4.48}, {"text": "backward pass. So 50% of the naive", "start": 6308.48, "duration": 4.56}, {"text": "pipeline is already done for us. Let's", "start": 6310.96, "duration": 3.52}, {"text": "say let's see what's happening here.", "start": 6313.04, "duration": 3.36}, {"text": "Backward pass different for last and", "start": 6314.48, "duration": 4.639}, {"text": "non-last. So this is true, right? The", "start": 6316.4, "duration": 4.239}, {"text": "backward pass for the last stage we need", "start": 6319.119, "duration": 3.361}, {"text": "to calculate loss. But if it isn't then", "start": 6320.639, "duration": 3.761}, {"text": "we um just need to propagate it back to", "start": 6322.48, "duration": 4.48}, {"text": "the last node. So let's first just say", "start": 6324.4, "duration": 7.92}, {"text": "okay if model rank is equal to uh model", "start": 6326.96, "duration": 7.6}, {"text": "world size then we'll do something for", "start": 6332.32, "duration": 6.56}, {"text": "the loss and then otherwise we won't.", "start": 6334.56, "duration": 7.92}, {"text": "So just put in the skeleton here. Okay.", "start": 6338.88, "duration": 6.56}, {"text": "So let's see what it says. If last stage", "start": 6342.48, "duration": 4.639}, {"text": "compute loss and call backward on it.", "start": 6345.44, "duration": 5.36}, {"text": "Okay. So if we are the last model, let's", "start": 6347.119, "duration": 7.201}, {"text": "come and check the forward pass already", "start": 6350.8, "duration": 6.08}, {"text": "calculates the mo the loss for us. If", "start": 6354.32, "duration": 4.72}, {"text": "self.rank equals equals self.orld size", "start": 6356.88, "duration": 5.2}, {"text": "minus one. And I just realized as well", "start": 6359.04, "duration": 4.96}, {"text": "that in our forward pass we forgot to", "start": 6362.08, "duration": 4.8}, {"text": "pass targets. So we should do that", "start": 6364.0, "duration": 5.119}, {"text": "before anything else. Um so send", "start": 6366.88, "duration": 4.16}, {"text": "forward. Where is the forward pass? It's", "start": 6369.119, "duration": 3.921}, {"text": "right here. It's because the type hints", "start": 6371.04, "duration": 4.72}, {"text": "are not coming up unfortunately. So we", "start": 6373.04, "duration": 4.88}, {"text": "can't really see the inputs that we need", "start": 6375.76, "duration": 4.479}, {"text": "for our forward pass in this case. But", "start": 6377.92, "duration": 3.84}, {"text": "what's worth noting is that the targets", "start": 6380.239, "duration": 7.281}, {"text": "will be none when we are using a device", "start": 6381.76, "duration": 8.24}, {"text": "which is not the last one. Right? Right", "start": 6387.52, "duration": 4.08}, {"text": "here fixed targets none otherwise it", "start": 6390.0, "duration": 4.08}, {"text": "will be set up to those random ins. So", "start": 6391.6, "duration": 6.4}, {"text": "this is all fine. And then right here we", "start": 6394.08, "duration": 5.68}, {"text": "just all we have to do is say loss", "start": 6398.0, "duration": 3.44}, {"text": "equals output so that we change the name", "start": 6399.76, "duration": 3.84}, {"text": "of it because the output will be the", "start": 6401.44, "duration": 3.92}, {"text": "activations for every single device", "start": 6403.6, "duration": 2.88}, {"text": "except for the last one where it's", "start": 6405.36, "duration": 2.64}, {"text": "actually the loss and then we just", "start": 6406.48, "duration": 3.84}, {"text": "trigger the backward pass by saying loss", "start": 6408.0, "duration": 6.08}, {"text": "backward. Okay. So now that this is done", "start": 6410.32, "duration": 6.319}, {"text": "this will give us the gradients for the", "start": 6414.08, "duration": 3.92}, {"text": "loss with respect to the weights and the", "start": 6416.639, "duration": 3.52}, {"text": "gradients for the loss with respect to", "start": 6418.0, "duration": 4.88}, {"text": "the input data because we set input", "start": 6420.159, "duration": 5.121}, {"text": "data.requires grad equal to true. And", "start": 6422.88, "duration": 4.0}, {"text": "then now in the else it says receives", "start": 6425.28, "duration": 3.919}, {"text": "grad from next stage and call backward.", "start": 6426.88, "duration": 8.64}, {"text": "Okay. So let's do coms dot", "start": 6429.199, "duration": 10.641}, {"text": "receive backward and it takes shape", "start": 6435.52, "duration": 7.76}, {"text": "device and type again. And in this case", "start": 6439.84, "duration": 6.56}, {"text": "we know output has its own shape and the", "start": 6443.28, "duration": 4.48}, {"text": "gradients are always the same shape as", "start": 6446.4, "duration": 5.12}, {"text": "output. So we'll just do output.shape", "start": 6447.76, "duration": 6.72}, {"text": "and then device which hasn't changed.", "start": 6451.52, "duration": 6.08}, {"text": "And we need to receive this. So we can", "start": 6454.48, "duration": 5.199}, {"text": "call this", "start": 6457.6, "duration": 6.72}, {"text": "gradients equals this. And then we need", "start": 6459.679, "duration": 7.841}, {"text": "to call backward on those gradients. So", "start": 6464.32, "duration": 5.919}, {"text": "we have backward here. And how backward", "start": 6467.52, "duration": 5.679}, {"text": "works when you don't have a scalar loss", "start": 6470.239, "duration": 8.321}, {"text": "is that if we just check our schedule.py", "start": 6473.199, "duration": 9.44}, {"text": "where we have the explanation here,", "start": 6478.56, "duration": 6.24}, {"text": "right? when you call that backward on a", "start": 6482.639, "duration": 3.761}, {"text": "non-scaler tensor like a hidden", "start": 6484.8, "duration": 3.839}, {"text": "activation with shape 32128 which is", "start": 6486.4, "duration": 4.239}, {"text": "exactly the case here pietorch requires", "start": 6488.639, "duration": 3.921}, {"text": "a matching gradient tensor of the same", "start": 6490.639, "duration": 4.881}, {"text": "shape and the reason why this is is", "start": 6492.56, "duration": 6.32}, {"text": "because the provided gradient acts as a", "start": 6495.52, "duration": 5.04}, {"text": "starting point for the vector jacobian", "start": 6498.88, "duration": 3.04}, {"text": "product allowing the chain rule to flow", "start": 6500.56, "duration": 2.8}, {"text": "backward to the weights and inputs I", "start": 6501.92, "duration": 2.4}, {"text": "honestly don't know what a vector", "start": 6503.36, "duration": 3.6}, {"text": "jacobian product is to be precise but", "start": 6504.32, "duration": 5.04}, {"text": "this is just the rule that we need to", "start": 6506.96, "duration": 6.08}, {"text": "follow so just to go give you guys The", "start": 6509.36, "duration": 5.359}, {"text": "spoiler here it's output.backward of", "start": 6513.04, "duration": 4.0}, {"text": "grade gradients from the next. So we", "start": 6514.719, "duration": 7.0}, {"text": "just do output dotbackward", "start": 6517.04, "duration": 4.679}, {"text": "of the gradient.", "start": 6522.239, "duration": 3.96}, {"text": "And that's all for this stage. But then", "start": 6526.4, "duration": 5.44}, {"text": "one thing worth noting is we need to", "start": 6529.679, "duration": 3.601}, {"text": "send the gradient to the previous stage", "start": 6531.84, "duration": 2.64}, {"text": "if we're not the first stage. This is", "start": 6533.28, "duration": 7.08}, {"text": "very important. So if model.rank", "start": 6534.48, "duration": 5.88}, {"text": "rank is not equal to zero", "start": 6540.4, "duration": 4.759}, {"text": "then we'll do the following. So we", "start": 6547.04, "duration": 3.76}, {"text": "already did the backward pass which is", "start": 6549.6, "duration": 3.599}, {"text": "different for the last and the non-last", "start": 6550.8, "duration": 5.2}, {"text": "which are these two. So the send the", "start": 6553.199, "duration": 4.561}, {"text": "gradients to the previous stage if not", "start": 6556.0, "duration": 5.679}, {"text": "first is just a", "start": 6557.76, "duration": 9.28}, {"text": "coms dot send backward and in this case", "start": 6561.679, "duration": 7.681}, {"text": "we just send a tensor which is going to", "start": 6567.04, "duration": 4.72}, {"text": "be the input", "start": 6569.36, "duration": 6.799}, {"text": "data dot grad. And why is this the case?", "start": 6571.76, "duration": 7.2}, {"text": "It's because as soon as we calculate the", "start": 6576.159, "duration": 5.441}, {"text": "output backward of the gradients, it", "start": 6578.96, "duration": 5.6}, {"text": "will populate the grad value of both the", "start": 6581.6, "duration": 5.039}, {"text": "weights of the model as well as the grad", "start": 6584.56, "duration": 4.0}, {"text": "value of the input data since we", "start": 6586.639, "duration": 4.321}, {"text": "calculate those two gradients for every", "start": 6588.56, "duration": 4.4}, {"text": "single layer in neural network. So I", "start": 6590.96, "duration": 3.84}, {"text": "want to send backward those specific", "start": 6592.96, "duration": 5.04}, {"text": "gradients to the model. And this will", "start": 6594.8, "duration": 4.879}, {"text": "make sure that the backward pass flows", "start": 6598.0, "duration": 4.32}, {"text": "through all the way. Okay. And now that", "start": 6599.679, "duration": 4.241}, {"text": "this is done, send grad to previous", "start": 6602.32, "duration": 3.04}, {"text": "stage if not the first stage that we", "start": 6603.92, "duration": 3.12}, {"text": "want to return loss if we're the last", "start": 6605.36, "duration": 5.04}, {"text": "stage else return nothing. So if the", "start": 6607.04, "duration": 5.04}, {"text": "modelrank", "start": 6610.4, "duration": 3.839}, {"text": "is the same as the model world size", "start": 6612.08, "duration": 6.48}, {"text": "minus one if we're the last GPU then we", "start": 6614.239, "duration": 7.561}, {"text": "will return", "start": 6618.56, "duration": 3.24}, {"text": "the loss. Okay, we're going to return", "start": 6621.84, "duration": 4.799}, {"text": "the loss which is a PyTorch tensor. And", "start": 6623.679, "duration": 4.241}, {"text": "I just realized to make things", "start": 6626.639, "duration": 3.361}, {"text": "consistent with the other code that", "start": 6627.92, "duration": 5.36}, {"text": "we've written so far, we'll just instead", "start": 6630.0, "duration": 5.52}, {"text": "of returning loss item here and then", "start": 6633.28, "duration": 5.12}, {"text": "having a scalar loss back in our main", "start": 6635.52, "duration": 4.639}, {"text": "loop, we'll return the actual loss", "start": 6638.4, "duration": 5.52}, {"text": "tensor and then access loss by saying", "start": 6640.159, "duration": 6.56}, {"text": "loss item because if we come back just", "start": 6643.92, "duration": 5.12}, {"text": "to make sure in manual right we're we're", "start": 6646.719, "duration": 4.241}, {"text": "calling loss item because this is how", "start": 6649.04, "duration": 4.48}, {"text": "you actually get the value of a tensor", "start": 6650.96, "duration": 4.64}, {"text": "by calling the item field. So we'll just", "start": 6653.52, "duration": 5.28}, {"text": "keep it consistent like that. And that", "start": 6655.6, "duration": 6.16}, {"text": "if I'm not mistaken is all of the", "start": 6658.8, "duration": 7.2}, {"text": "necessary logic for implementing the", "start": 6661.76, "duration": 7.6}, {"text": "class. And now we can actually try to", "start": 6666.0, "duration": 6.32}, {"text": "run this. But before we run it, we need", "start": 6669.36, "duration": 6.96}, {"text": "to just change the from imports, right?", "start": 6672.32, "duration": 6.319}, {"text": "because these are actually importing", "start": 6676.32, "duration": 7.12}, {"text": "from the finished um src but instead we", "start": 6678.639, "duration": 8.321}, {"text": "want it to import from these two. So", "start": 6683.44, "duration": 6.719}, {"text": "let's just change that in this line", "start": 6686.96, "duration": 4.96}, {"text": "here.", "start": 6690.159, "duration": 3.601}, {"text": "And yeah, we don't even use init", "start": 6691.92, "duration": 4.719}, {"text": "distributed in this case. So no need for", "start": 6693.76, "duration": 6.72}, {"text": "that. Okay. And yeah, we're probably", "start": 6696.639, "duration": 6.321}, {"text": "going to get a bug here. So let's be", "start": 6700.48, "duration": 4.56}, {"text": "prepared to debug. But we're just going", "start": 6702.96, "duration": 4.88}, {"text": "to do UV run torch run with four GPUs", "start": 6705.04, "duration": 4.639}, {"text": "since that's a standard that we've taken", "start": 6707.84, "duration": 6.24}, {"text": "for um this setup here. And that's all", "start": 6709.679, "duration": 6.641}, {"text": "of the examples as well using four GPUs.", "start": 6714.08, "duration": 4.96}, {"text": "Processes per node is four. And then the", "start": 6716.32, "duration": 5.6}, {"text": "actual script is in my work and it's the", "start": 6719.04, "duration": 7.679}, {"text": "main function. So let's see. Oh, we", "start": 6721.92, "duration": 6.799}, {"text": "spelled it wrong. Boom. Let's see what", "start": 6726.719, "duration": 5.761}, {"text": "error we get to start off. Okay, it's a", "start": 6728.719, "duration": 7.841}, {"text": "uninitialized tensor. It seems int is", "start": 6732.48, "duration": 8.159}, {"text": "not a module subclass. So, okay, shared", "start": 6736.56, "duration": 5.599}, {"text": "MLP2", "start": 6740.639, "duration": 6.241}, {"text": "device. So, this is in uh line 30.", "start": 6742.159, "duration": 7.201}, {"text": "Are we using the proper", "start": 6746.88, "duration": 5.44}, {"text": "um dim total layers rank and road size?", "start": 6749.36, "duration": 6.239}, {"text": "So, that's fine. But ah I see there's a", "start": 6752.32, "duration": 5.52}, {"text": "very big error here which is the fact", "start": 6755.599, "duration": 8.161}, {"text": "that we're not adding a n sequential of", "start": 6757.84, "duration": 10.96}, {"text": "dim dim input dim and output dim 2 but", "start": 6763.76, "duration": 6.399}, {"text": "we're adding n linear. I don't even know", "start": 6768.8, "duration": 3.2}, {"text": "what that would do. Uh clearly gives you", "start": 6770.159, "duration": 4.321}, {"text": "an error. So that's now resolved. Let's", "start": 6772.0, "duration": 5.44}, {"text": "see what we get now. Okay. I saw", "start": 6774.48, "duration": 6.48}, {"text": "something else. Let's see what it says.", "start": 6777.44, "duration": 5.36}, {"text": "Okay. So we got an error on the fixed", "start": 6780.96, "duration": 4.639}, {"text": "target in line 42 of the main. I think", "start": 6782.8, "duration": 4.64}, {"text": "this means that batch size actually is a", "start": 6785.599, "duration": 4.881}, {"text": "tpple. So let's just check here using", "start": 6787.44, "duration": 6.4}, {"text": "their examples. Um yeah okay so batch", "start": 6790.48, "duration": 5.28}, {"text": "size needs to be a tuple. So 3a 5 and", "start": 6793.84, "duration": 4.0}, {"text": "then batch size is a tpple. So there we", "start": 6795.76, "duration": 4.8}, {"text": "can fix this very easily. Luckily these", "start": 6797.84, "duration": 4.64}, {"text": "these bugs so far are not difficult to", "start": 6800.56, "duration": 4.8}, {"text": "resolve. Okay. Did we have the exact", "start": 6802.48, "duration": 5.92}, {"text": "same error somewhere else? Let's see in", "start": 6805.36, "duration": 5.279}, {"text": "rand n.", "start": 6808.4, "duration": 4.239}, {"text": "No, that one you can just implement it", "start": 6810.639, "duration": 5.441}, {"text": "as a set of parameters without any", "start": 6812.639, "duration": 5.6}, {"text": "changes. Okay, I think it's because we", "start": 6816.08, "duration": 3.519}, {"text": "need to add the comma here. So it", "start": 6818.239, "duration": 4.321}, {"text": "actually becomes a tpple.", "start": 6819.599, "duration": 5.761}, {"text": "Okay, that's a new error. So the name", "start": 6822.56, "duration": 6.8}, {"text": "loss is not defined. That is fine.", "start": 6825.36, "duration": 6.879}, {"text": "And it looks like this is because we're", "start": 6829.36, "duration": 8.08}, {"text": "not doing the proper check. So if rank", "start": 6832.239, "duration": 8.4}, {"text": "equals equals world size minus one and", "start": 6837.44, "duration": 4.64}, {"text": "okay this this means that we're the last", "start": 6840.639, "duration": 5.281}, {"text": "model. Are these two checks the same?", "start": 6842.08, "duration": 8.32}, {"text": "Indeed they are. Okay. So why is loss", "start": 6845.92, "duration": 7.04}, {"text": "not defined? Okay. And this is yet", "start": 6850.4, "duration": 5.6}, {"text": "another pretty embarrassing bug. So this", "start": 6852.96, "duration": 4.8}, {"text": "is the correct way to say if rank equals", "start": 6856.0, "duration": 3.599}, {"text": "equals world size one. But here if I", "start": 6857.76, "duration": 3.359}, {"text": "have rank minus one equals equals world", "start": 6859.599, "duration": 4.0}, {"text": "size which is saying if 3 minus one", "start": 6861.119, "duration": 5.681}, {"text": "equals 4 which is not the case. So let's", "start": 6863.599, "duration": 6.64}, {"text": "see now will it work or will it still Oh", "start": 6866.8, "duration": 5.839}, {"text": "wow. Okay. I thought we would get more", "start": 6870.239, "duration": 4.96}, {"text": "bugs but look at this guys. We actually", "start": 6872.639, "duration": 6.241}, {"text": "have a parallel pipeline naive setup", "start": 6875.199, "duration": 7.44}, {"text": "going on which is getting a loss of", "start": 6878.88, "duration": 5.839}, {"text": "0.214853.", "start": 6882.639, "duration": 5.281}, {"text": "This should hopefully be deterministic.", "start": 6884.719, "duration": 6.241}, {"text": "Okay, it is deterministic. And now if we", "start": 6887.92, "duration": 4.96}, {"text": "look at the loss that we get in our", "start": 6890.96, "duration": 4.8}, {"text": "other script which we can just use", "start": 6892.88, "duration": 7.68}, {"text": "manual to to check. Let's do UV run", "start": 6895.76, "duration": 7.52}, {"text": "and go into my work and step", "start": 6900.56, "duration": 5.039}, {"text": "one.mmanual.", "start": 6903.28, "duration": 3.76}, {"text": "In this case, we're splitting the model", "start": 6905.599, "duration": 3.201}, {"text": "up manually. And you can see that we got", "start": 6907.04, "duration": 4.24}, {"text": "a loss of 0.231414.", "start": 6908.8, "duration": 5.28}, {"text": "And here you get a loss of 0.214853.", "start": 6911.28, "duration": 4.16}, {"text": "So the losses are once again not", "start": 6914.08, "duration": 3.76}, {"text": "identical because", "start": 6915.44, "duration": 5.52}, {"text": "we have this random number generator", "start": 6917.84, "duration": 4.96}, {"text": "which is trying to make the losses the", "start": 6920.96, "duration": 5.6}, {"text": "same but there's definitely some um two", "start": 6922.8, "duration": 4.879}, {"text": "some things that we're missing", "start": 6926.56, "duration": 2.96}, {"text": "essentially since we initialized these", "start": 6927.679, "duration": 4.081}, {"text": "random numbers here then that's not", "start": 6929.52, "duration": 4.079}, {"text": "happening in the other end and it's not", "start": 6931.76, "duration": 4.16}, {"text": "in the same order. So we can hope to get", "start": 6933.599, "duration": 3.761}, {"text": "similar losses which is the case here", "start": 6935.92, "duration": 2.88}, {"text": "but it's not going to be the the same", "start": 6937.36, "duration": 4.319}, {"text": "loss ever. And yeah, this is really", "start": 6938.8, "duration": 4.08}, {"text": "awesome, guys, because what we've just", "start": 6941.679, "duration": 3.761}, {"text": "done now is a single training step using", "start": 6942.88, "duration": 4.64}, {"text": "the naive stop and wait schedule. And", "start": 6945.44, "duration": 4.64}, {"text": "it's pretty simple because we've already", "start": 6947.52, "duration": 3.76}, {"text": "implemented all of the primitives", "start": 6950.08, "duration": 2.72}, {"text": "through send backward, receive backward,", "start": 6951.28, "duration": 3.28}, {"text": "send forward, receive forward. We just", "start": 6952.8, "duration": 3.28}, {"text": "need to implement them and make sure", "start": 6954.56, "duration": 3.039}, {"text": "that we're using these on the same and", "start": 6956.08, "duration": 4.4}, {"text": "correct device. And one thing that I", "start": 6957.599, "duration": 4.881}, {"text": "want to show you guys before we move on", "start": 6960.48, "duration": 5.679}, {"text": "because if we look at our syllabus which", "start": 6962.48, "duration": 7.119}, {"text": "is just in the outline then the next", "start": 6966.159, "duration": 6.56}, {"text": "thing right is to implement gpipe. So", "start": 6969.599, "duration": 5.201}, {"text": "just as a teaser we'll be changing from", "start": 6972.719, "duration": 5.041}, {"text": "batches to chunks in this case but right", "start": 6974.8, "duration": 4.399}, {"text": "now what I want to show you is a", "start": 6977.76, "duration": 5.2}, {"text": "profiler of this function. So what I", "start": 6979.199, "duration": 6.4}, {"text": "have in the src is profiler.pay and", "start": 6982.96, "duration": 4.56}, {"text": "profile schedule.py.", "start": 6985.599, "duration": 3.441}, {"text": "So, we're never going to implement the", "start": 6987.52, "duration": 3.84}, {"text": "profiler on our own because if you look", "start": 6989.04, "duration": 5.44}, {"text": "at the profiler, it's pretty boring and", "start": 6991.36, "duration": 4.799}, {"text": "just a bunch of context managers. And", "start": 6994.48, "duration": 3.199}, {"text": "then the profile version of the knife", "start": 6996.159, "duration": 3.681}, {"text": "pipeline stage is the exact same thing", "start": 6997.679, "duration": 4.241}, {"text": "except it has all these with context", "start": 6999.84, "duration": 4.72}, {"text": "managers to profile each part of the", "start": 7001.92, "duration": 4.56}, {"text": "function. And the entire intent here is", "start": 7004.56, "duration": 5.76}, {"text": "to see okay because of the fact that the", "start": 7006.48, "duration": 6.719}, {"text": "pipeline is very inefficient with all of", "start": 7010.32, "duration": 6.16}, {"text": "these bubbles, how much time are we", "start": 7013.199, "duration": 4.641}, {"text": "actually spending in each device?", "start": 7016.48, "duration": 3.759}, {"text": "Because right by default, if everything", "start": 7017.84, "duration": 4.96}, {"text": "was working perfectly, you'd spend 25%", "start": 7020.239, "duration": 5.121}, {"text": "of your time on each device because this", "start": 7022.8, "duration": 4.319}, {"text": "one um there's four devices and you just", "start": 7025.36, "duration": 5.279}, {"text": "divide um the number of devices by one.", "start": 7027.119, "duration": 5.761}, {"text": "So 1 divided 4, sorry, divide one by the", "start": 7030.639, "duration": 5.361}, {"text": "number of devices. And if we profile", "start": 7032.88, "duration": 4.64}, {"text": "this, we should get a number close to", "start": 7036.0, "duration": 3.92}, {"text": "that. And what I have right here is", "start": 7037.52, "duration": 6.8}, {"text": "profile main.py, which is the same", "start": 7039.92, "duration": 6.239}, {"text": "structure as all of our main runners,", "start": 7044.32, "duration": 5.2}, {"text": "except it imports from profile schedule", "start": 7046.159, "duration": 7.681}, {"text": "as well as takes the profiler from sorry", "start": 7049.52, "duration": 5.52}, {"text": "takes the pipeline profiler from", "start": 7053.84, "duration": 2.96}, {"text": "profiler. So all of this is kind of", "start": 7055.04, "duration": 3.92}, {"text": "confusing with these imports, but the", "start": 7056.8, "duration": 3.919}, {"text": "only thing that matters is to show you", "start": 7058.96, "duration": 5.6}, {"text": "guys the output of this run. And it's", "start": 7060.719, "duration": 6.321}, {"text": "very interesting because if we come to", "start": 7064.56, "duration": 3.92}, {"text": "profiled main, what are the actual", "start": 7067.04, "duration": 3.199}, {"text": "differences? Well, we print the summary", "start": 7068.48, "duration": 3.84}, {"text": "profile print summary which if you come", "start": 7070.239, "duration": 5.121}, {"text": "and look at the profiler just has all", "start": 7072.32, "duration": 4.72}, {"text": "right where is the profiler just has", "start": 7075.36, "duration": 2.96}, {"text": "many different print statements for the", "start": 7077.04, "duration": 3.599}, {"text": "summary statistics summary statistics", "start": 7078.32, "duration": 5.6}, {"text": "sorry. And then what else do we do? We", "start": 7080.639, "duration": 6.801}, {"text": "simply pass the profiler to the pipeline", "start": 7083.92, "duration": 6.64}, {"text": "setup in the profileuler to allow for it", "start": 7087.44, "duration": 6.159}, {"text": "to profile and we also initialize the", "start": 7090.56, "duration": 5.36}, {"text": "profiler up here with the rank of the", "start": 7093.599, "duration": 3.201}, {"text": "model because we need to know the", "start": 7095.92, "duration": 2.64}, {"text": "model's rank in order to profile each", "start": 7096.8, "duration": 4.48}, {"text": "device properly or each process in our", "start": 7098.56, "duration": 5.52}, {"text": "CPU core. So that's everything for the", "start": 7101.28, "duration": 4.399}, {"text": "logic of how this is actually going", "start": 7104.08, "duration": 3.519}, {"text": "down. And then if we look at the", "start": 7105.679, "duration": 4.0}, {"text": "profiling, so the first thing you'll", "start": 7107.599, "duration": 3.52}, {"text": "notice is that we get the same final", "start": 7109.679, "duration": 3.761}, {"text": "loss, which is great, meaning that this", "start": 7111.119, "duration": 3.761}, {"text": "pipeline setup is the same as the one", "start": 7113.44, "duration": 3.52}, {"text": "that we implemented. And then after you", "start": 7114.88, "duration": 3.92}, {"text": "can see that the pipeline profilers are", "start": 7116.96, "duration": 4.4}, {"text": "ranked from rank three all the way to 2", "start": 7118.8, "duration": 5.919}, {"text": "1 0. And coming up here, we can check", "start": 7121.36, "duration": 5.12}, {"text": "why that is. It's because the last", "start": 7124.719, "duration": 3.281}, {"text": "device always finishes first and then", "start": 7126.48, "duration": 4.0}, {"text": "it's the second, third, or rather device", "start": 7128.0, "duration": 4.8}, {"text": "two, device one, device zero. Okay. And", "start": 7130.48, "duration": 4.88}, {"text": "then what else is there to worth noting?", "start": 7132.8, "duration": 4.56}, {"text": "We can check and see that for some", "start": 7135.36, "duration": 4.16}, {"text": "reason the last device is doing the", "start": 7137.36, "duration": 4.48}, {"text": "majority of the compute even though oh", "start": 7139.52, "duration": 3.84}, {"text": "this actually makes sense actually", "start": 7141.84, "duration": 3.839}, {"text": "because the last device has the LLM not", "start": 7143.36, "duration": 4.0}, {"text": "the LLM but rather the classification", "start": 7145.679, "duration": 4.721}, {"text": "head right if we come into model.py", "start": 7147.36, "duration": 5.04}, {"text": "high. Remember that if the rank is the", "start": 7150.4, "duration": 3.279}, {"text": "last one, not only does it have to calc", "start": 7152.4, "duration": 3.44}, {"text": "the loss, but also has to calculate the", "start": 7153.679, "duration": 3.841}, {"text": "head. So this makes sense why it's", "start": 7155.84, "duration": 3.44}, {"text": "taking more time on average. And then", "start": 7157.52, "duration": 3.92}, {"text": "the communication time represents how", "start": 7159.28, "duration": 3.6}, {"text": "much time they're waiting for the other", "start": 7161.44, "duration": 4.159}, {"text": "GPUs to send them their data. So this is", "start": 7162.88, "duration": 4.56}, {"text": "also the majority of the time for each", "start": 7165.599, "duration": 4.961}, {"text": "GPU because once again, on average, each", "start": 7167.44, "duration": 6.64}, {"text": "of the four devices should be spending", "start": 7170.56, "duration": 5.679}, {"text": "three4s of their time waiting. So I'd", "start": 7174.08, "duration": 4.079}, {"text": "encourage you to run this command right", "start": 7176.239, "duration": 4.48}, {"text": "here torch run of profile main and check", "start": 7178.159, "duration": 4.321}, {"text": "it out. You can also look at some other", "start": 7180.719, "duration": 3.361}, {"text": "stats in terms of backward and forward.", "start": 7182.48, "duration": 3.679}, {"text": "So it's just a rice really nice utility", "start": 7184.08, "duration": 3.119}, {"text": "and we're going to be using that for", "start": 7186.159, "duration": 5.761}, {"text": "every single further pipeline parallel", "start": 7187.199, "duration": 7.52}, {"text": "in order to see how much more efficient", "start": 7191.92, "duration": 5.52}, {"text": "they are versus the naive approach. So", "start": 7194.719, "duration": 5.601}, {"text": "on that note, we're now completely done", "start": 7197.44, "duration": 5.759}, {"text": "with the naiveuler. So let's look at", "start": 7200.32, "duration": 5.839}, {"text": "what's next. And what's next is Gpipe.", "start": 7203.199, "duration": 5.92}, {"text": "Okay, so diving right into Gpipe. What", "start": 7206.159, "duration": 6.48}, {"text": "is it? It splits the large mini batches", "start": 7209.119, "duration": 6.48}, {"text": "into smaller micro batches and processes", "start": 7212.639, "duration": 6.08}, {"text": "them through model partitions, which are", "start": 7215.599, "duration": 6.56}, {"text": "the different GPUs. And how does it", "start": 7218.719, "duration": 5.44}, {"text": "work? Well, we simply split the mini", "start": 7222.159, "duration": 4.721}, {"text": "batch. So each mini batch of size B is", "start": 7224.159, "duration": 6.801}, {"text": "split into M micro batches. So the size", "start": 7226.88, "duration": 6.16}, {"text": "of each micro batch being B divided by", "start": 7230.96, "duration": 5.279}, {"text": "M. If we have 32 as our batch size and", "start": 7233.04, "duration": 6.8}, {"text": "we have four batch micro batches then", "start": 7236.239, "duration": 6.4}, {"text": "the microbatch size will be eight and", "start": 7239.84, "duration": 5.759}, {"text": "then we just pipeline execute them uh", "start": 7242.639, "duration": 4.401}, {"text": "just as we've done with our naive", "start": 7245.599, "duration": 2.881}, {"text": "implementation. So you can really see", "start": 7247.04, "duration": 3.84}, {"text": "that as opposed to our naive", "start": 7248.48, "duration": 5.759}, {"text": "implementation it's not that different.", "start": 7250.88, "duration": 6.56}, {"text": "We are simply making more batches. And", "start": 7254.239, "duration": 4.96}, {"text": "what you'll notice is that this bubble", "start": 7257.44, "duration": 4.799}, {"text": "which will make more formal through an", "start": 7259.199, "duration": 5.04}, {"text": "equation that will give us the amount of", "start": 7262.239, "duration": 5.281}, {"text": "space that is actually white or idle. Um", "start": 7264.239, "duration": 6.4}, {"text": "this bubble is actually made smaller the", "start": 7267.52, "duration": 5.44}, {"text": "more that we increase the micro batch", "start": 7270.639, "duration": 4.721}, {"text": "size. So there you have it for the", "start": 7272.96, "duration": 3.84}, {"text": "little introduction. And one thing that", "start": 7275.36, "duration": 3.279}, {"text": "we would write that we should right away", "start": 7276.8, "duration": 4.399}, {"text": "note is that we have to do gradient", "start": 7278.639, "duration": 5.441}, {"text": "accumulation here across the set of in", "start": 7281.199, "duration": 6.161}, {"text": "this case four microbatches. So um just", "start": 7284.08, "duration": 5.28}, {"text": "to give you the explanation here each", "start": 7287.36, "duration": 3.68}, {"text": "microbatch is trained in its gradients", "start": 7289.36, "duration": 3.359}, {"text": "are computed independently and then", "start": 7291.04, "duration": 2.96}, {"text": "gradients from all microbatches are", "start": 7292.719, "duration": 2.721}, {"text": "accumulated locally before the optimizer", "start": 7294.0, "duration": 3.119}, {"text": "update giving the same effect as if", "start": 7295.44, "duration": 2.88}, {"text": "you're training on full batch one. So", "start": 7297.119, "duration": 2.56}, {"text": "this is just classic gradient", "start": 7298.32, "duration": 2.96}, {"text": "accumulation", "start": 7299.679, "duration": 3.04}, {"text": "and let's talk about some of the", "start": 7301.28, "duration": 3.439}, {"text": "details. So as I mentioned once again we", "start": 7302.719, "duration": 3.841}, {"text": "have the bubbles and there's this", "start": 7304.719, "duration": 4.721}, {"text": "formula which I will motivate. So first", "start": 7306.56, "duration": 5.599}, {"text": "of all there is a fill and a drain phase", "start": 7309.44, "duration": 4.96}, {"text": "of the pipeline when not all devices are", "start": 7312.159, "duration": 5.281}, {"text": "occupied and these create the bubble. So", "start": 7314.4, "duration": 5.68}, {"text": "um at this point right there's actually", "start": 7317.44, "duration": 4.799}, {"text": "a single point here for example where", "start": 7320.08, "duration": 4.24}, {"text": "all devices are being used and a single", "start": 7322.239, "duration": 3.601}, {"text": "point here where all devices are being", "start": 7324.32, "duration": 3.359}, {"text": "used. If you just take the vertical line", "start": 7325.84, "duration": 5.279}, {"text": "test of this batch um and this is if we", "start": 7327.679, "duration": 6.641}, {"text": "have batch size four sorry we have yeah", "start": 7331.119, "duration": 5.201}, {"text": "batch size four and four GPUs in this", "start": 7334.32, "duration": 3.839}, {"text": "case we have batch size one and four", "start": 7336.32, "duration": 3.359}, {"text": "GPUs that's what we've done in our naive", "start": 7338.159, "duration": 4.48}, {"text": "solution so you can say that the naive", "start": 7339.679, "duration": 6.48}, {"text": "case is a special case of micro um", "start": 7342.639, "duration": 5.761}, {"text": "batching which is called gpipe where the", "start": 7346.159, "duration": 4.08}, {"text": "micro batch size is just one it's the", "start": 7348.4, "duration": 4.96}, {"text": "same as the batch size and now let's", "start": 7350.239, "duration": 4.801}, {"text": "finally look at how we arrive at this", "start": 7353.36, "duration": 4.319}, {"text": "formula which the fraction of time lost", "start": 7355.04, "duration": 6.88}, {"text": "to bubbles aka 1 - m over m + n minus", "start": 7357.679, "duration": 6.081}, {"text": "one. Okay, first of all, looking at the", "start": 7361.92, "duration": 6.799}, {"text": "numerator, we have 2 nm m with I haven't", "start": 7363.76, "duration": 7.6}, {"text": "mentioned this, but n being the number", "start": 7368.719, "duration": 7.201}, {"text": "of GPUs, so the number of devices and m", "start": 7371.36, "duration": 7.2}, {"text": "being the number of micro batches. And", "start": 7375.92, "duration": 4.56}, {"text": "how do we arrive at this number? Let's", "start": 7378.56, "duration": 4.559}, {"text": "use this example to illustrate that.", "start": 7380.48, "duration": 4.56}, {"text": "First of all, the two I'll just mention", "start": 7383.119, "duration": 3.281}, {"text": "right now comes from the fact that we", "start": 7385.04, "duration": 3.36}, {"text": "have a forward and a backward pass which", "start": 7386.4, "duration": 4.319}, {"text": "are mirrors of themsel, right? If you", "start": 7388.4, "duration": 4.88}, {"text": "put your hand right down this line,", "start": 7390.719, "duration": 4.081}, {"text": "you'll see that you're just doing double", "start": 7393.28, "duration": 3.68}, {"text": "the work. Of course, the backward pass", "start": 7394.8, "duration": 5.28}, {"text": "is more computationally heavy, but we're", "start": 7396.96, "duration": 4.4}, {"text": "going to abstract that for now and just", "start": 7400.08, "duration": 2.48}, {"text": "assume that we're just doing the same", "start": 7401.36, "duration": 2.879}, {"text": "thing twice. So that's where the two", "start": 7402.56, "duration": 5.36}, {"text": "comes from. And then we have n devices.", "start": 7404.239, "duration": 6.48}, {"text": "Okay, this is the vertical column here.", "start": 7407.92, "duration": 5.12}, {"text": "And the m number of microbatches is just", "start": 7410.719, "duration": 3.841}, {"text": "one here and the number of microbatches", "start": 7413.04, "duration": 4.159}, {"text": "is four here. So in this case, the 2nm", "start": 7414.56, "duration": 4.8}, {"text": "is like the theoretical best case", "start": 7417.199, "duration": 5.121}, {"text": "scenario of the work that all of these", "start": 7419.36, "duration": 6.4}, {"text": "GPUs would accomplish. So if somehow for", "start": 7422.32, "duration": 5.359}, {"text": "example you could synchronize all four", "start": 7425.76, "duration": 3.439}, {"text": "of these happen at the same time, then", "start": 7427.679, "duration": 3.52}, {"text": "you'd have F1 here, F1 here, F1 here,", "start": 7429.199, "duration": 5.201}, {"text": "and F1 here, which is equal to four. And", "start": 7431.199, "duration": 6.321}, {"text": "if we calculate this as well, we get", "start": 7434.4, "duration": 5.68}, {"text": "eight because we multiply by two. So in", "start": 7437.52, "duration": 4.48}, {"text": "essence, we do four units of work for", "start": 7440.08, "duration": 3.92}, {"text": "the forward pass and four units of work", "start": 7442.0, "duration": 4.0}, {"text": "for the backward pass. Once again, two", "start": 7444.0, "duration": 3.92}, {"text": "times four GPUs and each GPU does one", "start": 7446.0, "duration": 5.119}, {"text": "unit of work. And this is the ideal case", "start": 7447.92, "duration": 6.239}, {"text": "where if there was no idle time, then", "start": 7451.119, "duration": 5.361}, {"text": "the GPUs would do eight units of work", "start": 7454.159, "duration": 5.281}, {"text": "with zero idle time. And in the bottom,", "start": 7456.48, "duration": 4.639}, {"text": "this represents the idle time. So this", "start": 7459.44, "duration": 4.4}, {"text": "is how we get the the fraction. So", "start": 7461.119, "duration": 4.48}, {"text": "looking once again, I'll already tell", "start": 7463.84, "duration": 4.0}, {"text": "you that the two in this denominator", "start": 7465.599, "duration": 4.321}, {"text": "represents the amount sorry the the", "start": 7467.84, "duration": 3.759}, {"text": "forward and the backward. So we can just", "start": 7469.92, "duration": 4.48}, {"text": "ignore that for now. And then if we look", "start": 7471.599, "duration": 6.08}, {"text": "inside what's going on here,", "start": 7474.4, "duration": 5.68}, {"text": "it's easiest to think of this um when we", "start": 7477.679, "duration": 5.761}, {"text": "look at the last GPU. So GPU 4 n minus", "start": 7480.08, "duration": 5.119}, {"text": "one here means how much time does it", "start": 7483.44, "duration": 3.279}, {"text": "have to wait before it can actually", "start": 7485.199, "duration": 4.641}, {"text": "start. So in this case we have n being", "start": 7486.719, "duration": 5.92}, {"text": "four minus one. So three. So GPU 4 has", "start": 7489.84, "duration": 5.759}, {"text": "to wait one two three time steps before", "start": 7492.639, "duration": 5.281}, {"text": "it has to start. And then it has to do m", "start": 7495.599, "duration": 4.481}, {"text": "units of work which is just one. So in", "start": 7497.92, "duration": 4.4}, {"text": "total it has to wait three time steps", "start": 7500.08, "duration": 4.079}, {"text": "and then do one unit of work which is", "start": 7502.32, "duration": 7.919}, {"text": "the same as 1 + 4 minus 1 and the answer", "start": 7504.159, "duration": 10.48}, {"text": "to this is 4. So we have three units of", "start": 7510.239, "duration": 8.081}, {"text": "idle time and then one unit of work.", "start": 7514.639, "duration": 6.321}, {"text": "Okay. So this explains what's going on", "start": 7518.32, "duration": 4.0}, {"text": "here. And then how can we interpret this", "start": 7520.96, "duration": 3.759}, {"text": "for GP1 for example? Well in GP1 it's", "start": 7522.32, "duration": 6.24}, {"text": "the opposite. In GP1 we do our work at", "start": 7524.719, "duration": 5.601}, {"text": "the very beginning but then we have to", "start": 7528.56, "duration": 4.639}, {"text": "wait n minus one time steps which is the", "start": 7530.32, "duration": 6.16}, {"text": "same as three. So we wait 1 2 three time", "start": 7533.199, "duration": 5.841}, {"text": "steps and then we actually multiply this", "start": 7536.48, "duration": 5.52}, {"text": "by two. So we wait another 1 2 3 time", "start": 7539.04, "duration": 5.679}, {"text": "steps and then we add back the one which", "start": 7542.0, "duration": 5.52}, {"text": "is m here to do our backward pass.", "start": 7544.719, "duration": 6.081}, {"text": "So that's 2 * m + n minus one. And then", "start": 7547.52, "duration": 4.8}, {"text": "where does n come from? It comes to the", "start": 7550.8, "duration": 2.879}, {"text": "fact that we have to do this over all", "start": 7552.32, "duration": 5.359}, {"text": "four GPUs. So for the GPU 1, there are m", "start": 7553.679, "duration": 7.52}, {"text": "+ n minus1 opportunities to do work in", "start": 7557.679, "duration": 5.92}, {"text": "this slot, even though we only do it m", "start": 7561.199, "duration": 4.321}, {"text": "times because the other n minus one", "start": 7563.599, "duration": 4.881}, {"text": "times were waiting. Whereas in the", "start": 7565.52, "duration": 4.96}, {"text": "numerator, this represents what would", "start": 7568.48, "duration": 4.4}, {"text": "happen if we didn't have to wait. So to", "start": 7570.48, "duration": 5.28}, {"text": "recap here, there's a forward and a", "start": 7572.88, "duration": 4.16}, {"text": "backward pass, which gives us two. In", "start": 7575.76, "duration": 4.24}, {"text": "the numerator, there are n devices and", "start": 7577.04, "duration": 4.639}, {"text": "there are m units of work. That's the", "start": 7580.0, "duration": 4.32}, {"text": "ideal. And then in the real situation", "start": 7581.679, "duration": 5.52}, {"text": "here we have once again the forward the", "start": 7584.32, "duration": 5.919}, {"text": "backward two here across n devices", "start": 7587.199, "duration": 5.201}, {"text": "and then we have to do one unit of work", "start": 7590.239, "duration": 4.081}, {"text": "here but then we have to wait n minus", "start": 7592.4, "duration": 3.839}, {"text": "one time steps n minus one which is", "start": 7594.32, "duration": 3.839}, {"text": "three. Okay. And then you can just", "start": 7596.239, "duration": 4.0}, {"text": "divide the 2n out of the numerator and", "start": 7598.159, "duration": 4.08}, {"text": "the denominator and then this gives you", "start": 7600.239, "duration": 4.081}, {"text": "this expression. And then why are we", "start": 7602.239, "duration": 4.321}, {"text": "doing one minus this amount? It's", "start": 7604.32, "duration": 5.12}, {"text": "because we are actually we are actually", "start": 7606.56, "duration": 6.079}, {"text": "computing the time of uh the the amount", "start": 7609.44, "duration": 5.12}, {"text": "of time spent idling not the amount of", "start": 7612.639, "duration": 4.0}, {"text": "time spent working right this this", "start": 7614.56, "duration": 3.84}, {"text": "fraction here because the the numerator", "start": 7616.639, "duration": 4.08}, {"text": "gives us the ideal amount of time that", "start": 7618.4, "duration": 4.319}, {"text": "we spend right which is eight in this", "start": 7620.719, "duration": 3.841}, {"text": "case it's actually out eight if you", "start": 7622.719, "duration": 5.041}, {"text": "count 1 2 3 4 5 6 7 8 this is the this", "start": 7624.56, "duration": 4.559}, {"text": "is the amount of work that we have to do", "start": 7627.76, "duration": 3.68}, {"text": "which you could ideally do just in two", "start": 7629.119, "duration": 4.0}, {"text": "vertical time steps but we can't do that", "start": 7631.44, "duration": 4.08}, {"text": "because these are sequentially dependent", "start": 7633.119, "duration": 5.281}, {"text": "in any case that's what the numerator is", "start": 7635.52, "duration": 6.159}, {"text": "and we want the opposite of that and", "start": 7638.4, "duration": 5.04}, {"text": "then this is why we scrapped one minus", "start": 7641.679, "duration": 3.281}, {"text": "one. If we wanted to know the fraction", "start": 7643.44, "duration": 3.36}, {"text": "of time spent computing then we would", "start": 7644.96, "duration": 4.4}, {"text": "just um keep this in the calculated in", "start": 7646.8, "duration": 6.879}, {"text": "as a as a calculation. So just as a uh", "start": 7649.36, "duration": 6.4}, {"text": "quick exercise let's calculate these", "start": 7653.679, "duration": 3.52}, {"text": "numbers and the the first thing that", "start": 7655.76, "duration": 2.64}, {"text": "you'll note is that this is actually", "start": 7657.199, "duration": 2.721}, {"text": "incorrect the bubble fraction if you", "start": 7658.4, "duration": 3.199}, {"text": "have tried this for yourself up to now", "start": 7659.92, "duration": 2.799}, {"text": "then you will notice that it is", "start": 7661.599, "duration": 6.08}, {"text": "incorrect. So let's do one minus m /", "start": 7662.719, "duration": 7.201}, {"text": "oh not m but rather one I mean we could", "start": 7667.679, "duration": 4.4}, {"text": "really do this in our head but uh let's", "start": 7669.92, "duration": 3.92}, {"text": "just use the Python interpreter because", "start": 7672.079, "duration": 3.681}, {"text": "we're lazy. So this once again gives us", "start": 7673.84, "duration": 5.359}, {"text": "0.75 and this is incorrect. So this", "start": 7675.76, "duration": 6.08}, {"text": "would be the case if we had five GPUs", "start": 7679.199, "duration": 6.48}, {"text": "and each GPU was only doing computations", "start": 7681.84, "duration": 5.68}, {"text": "20% of the time. So this is just an", "start": 7685.679, "duration": 3.361}, {"text": "error in the diagram. And once again", "start": 7687.52, "duration": 4.079}, {"text": "this makes sense 0.75, right? Because if", "start": 7689.04, "duration": 5.28}, {"text": "we consider each row of here as four", "start": 7691.599, "duration": 4.801}, {"text": "possibilities to do something, it's only", "start": 7694.32, "duration": 4.0}, {"text": "doing something in one of the four", "start": 7696.4, "duration": 5.52}, {"text": "possibilities. And therefore um 1/4 of", "start": 7698.32, "duration": 5.6}, {"text": "the time is spent computing and 3/4 of", "start": 7701.92, "duration": 4.0}, {"text": "the time is spent not computing. But", "start": 7703.92, "duration": 5.679}, {"text": "then now in this case we can recalculate", "start": 7705.92, "duration": 6.0}, {"text": "it and see what we get. So the m in this", "start": 7709.599, "duration": 4.881}, {"text": "time is four. Um so let's just change", "start": 7711.92, "duration": 6.0}, {"text": "everything to four. That was m and um we", "start": 7714.48, "duration": 6.96}, {"text": "still have four GPUs. So, okay, there's", "start": 7717.92, "duration": 6.08}, {"text": "an error in my calculation. We got 3.4.", "start": 7721.44, "duration": 3.84}, {"text": "I think it's because we didn't do one", "start": 7724.0, "duration": 4.32}, {"text": "minus. There you go. So, as you can see", "start": 7725.28, "duration": 7.04}, {"text": "here, 42% of the time is spent idling.", "start": 7728.32, "duration": 7.12}, {"text": "So, overall, the average time is spent", "start": 7732.32, "duration": 6.319}, {"text": "doing something, but still it's not the", "start": 7735.44, "duration": 6.08}, {"text": "most efficient. So, in conclusion, if we", "start": 7738.639, "duration": 5.361}, {"text": "look here, since we have m in the", "start": 7741.52, "duration": 4.88}, {"text": "numerator and in the denominator, but um", "start": 7744.0, "duration": 4.56}, {"text": "the numerator also has other terms. The", "start": 7746.4, "duration": 5.12}, {"text": "greater that we make m, the closer that", "start": 7748.56, "duration": 6.88}, {"text": "this uh this term will become to one and", "start": 7751.52, "duration": 8.159}, {"text": "then therefore we'll have a smaller", "start": 7755.44, "duration": 7.92}, {"text": "portion of our graph using bubbles. But", "start": 7759.679, "duration": 6.081}, {"text": "conversely, the more devices you add,", "start": 7763.36, "duration": 4.4}, {"text": "the more time each device spends doing", "start": 7765.76, "duration": 3.52}, {"text": "nothing. And this makes sense. If you", "start": 7767.76, "duration": 4.64}, {"text": "have 10 GPUs, then in the case where you", "start": 7769.28, "duration": 5.76}, {"text": "have one micro batch, then the GPUs are", "start": 7772.4, "duration": 4.64}, {"text": "only on 10% of the time, each each", "start": 7775.04, "duration": 4.8}, {"text": "individual one. So to conclude, each", "start": 7777.04, "duration": 4.72}, {"text": "microbatch requires storing its", "start": 7779.84, "duration": 3.359}, {"text": "activations until the backward patch is", "start": 7781.76, "duration": 3.12}, {"text": "finished.", "start": 7783.199, "duration": 3.841}, {"text": "And now with respect to memory demand,", "start": 7784.88, "duration": 3.759}, {"text": "each microbatch needs to store its", "start": 7787.04, "duration": 3.039}, {"text": "activations until the backward pass is", "start": 7788.639, "duration": 3.921}, {"text": "finished. So even increasing", "start": 7790.079, "duration": 4.481}, {"text": "microbatches is good for parallelism,", "start": 7792.56, "duration": 4.559}, {"text": "but it does make the memory more", "start": 7794.56, "duration": 4.24}, {"text": "consumptive. And you can think of this", "start": 7797.119, "duration": 4.241}, {"text": "through the following example, right? If", "start": 7798.8, "duration": 4.64}, {"text": "we have F1 here and we're doing the", "start": 7801.36, "duration": 4.56}, {"text": "backward of F1 at this time step,", "start": 7803.44, "duration": 4.88}, {"text": "whereas before we would just free up the", "start": 7805.92, "duration": 5.44}, {"text": "activations in one go for the first", "start": 7808.32, "duration": 5.2}, {"text": "batch or the only batch in this case of", "start": 7811.36, "duration": 4.56}, {"text": "the first GPU, in this case we have to", "start": 7813.52, "duration": 5.52}, {"text": "do this four times. And this simply just", "start": 7815.92, "duration": 6.159}, {"text": "takes more memory and also introduces", "start": 7819.04, "duration": 5.119}, {"text": "more communication overhead because in", "start": 7822.079, "duration": 4.08}, {"text": "Gpipe we need to cache the activations", "start": 7824.159, "duration": 3.52}, {"text": "for each microbatch from the time it was", "start": 7826.159, "duration": 2.641}, {"text": "forwarded until the corresponding", "start": 7827.679, "duration": 2.96}, {"text": "backward pass. One thing that is worth", "start": 7828.8, "duration": 3.12}, {"text": "noting is that gradient checkpointing", "start": 7830.639, "duration": 3.281}, {"text": "can be used to trade computation for", "start": 7831.92, "duration": 4.0}, {"text": "reduced activation memory. So instead of", "start": 7833.92, "duration": 3.679}, {"text": "storing all intermediate activations,", "start": 7835.92, "duration": 4.319}, {"text": "some are recomputed on the fly. And here", "start": 7837.599, "duration": 4.881}, {"text": "you can see that we only would store the", "start": 7840.239, "duration": 4.161}, {"text": "activations for the whole batch at the", "start": 7842.48, "duration": 3.28}, {"text": "boundary. This is the gradient", "start": 7844.4, "duration": 2.799}, {"text": "checkpoint. and then you recomp compute", "start": 7845.76, "duration": 3.76}, {"text": "the non-cash activations for the current", "start": 7847.199, "duration": 4.721}, {"text": "microbatch during that backward pass. So", "start": 7849.52, "duration": 4.079}, {"text": "that's also an option. Another thing", "start": 7851.92, "duration": 3.199}, {"text": "worth noting is that if you use batch", "start": 7853.599, "duration": 3.921}, {"text": "norm since now you're splitting up your", "start": 7855.119, "duration": 5.841}, {"text": "batch, it would break the microbatch", "start": 7857.52, "duration": 7.28}, {"text": "independence assumption. So, um, yeah,", "start": 7860.96, "duration": 5.04}, {"text": "we're not actually going to be computing", "start": 7864.8, "duration": 2.879}, {"text": "micro uh, sorry, we're not actually", "start": 7866.0, "duration": 3.199}, {"text": "doing any batch norm computations in", "start": 7867.679, "duration": 3.201}, {"text": "this series, but this is something worth", "start": 7869.199, "duration": 3.201}, {"text": "noting is that as soon as you split the", "start": 7870.88, "duration": 3.68}, {"text": "batches, then you can no longer compute", "start": 7872.4, "duration": 4.64}, {"text": "the statistics of them in one shot. So,", "start": 7874.56, "duration": 4.079}, {"text": "you would have to do it on the", "start": 7877.04, "duration": 3.119}, {"text": "microbatch level instead of the batch", "start": 7878.639, "duration": 5.44}, {"text": "level. And one other thing is that even", "start": 7880.159, "duration": 7.761}, {"text": "though classic GPIPE does not add", "start": 7884.079, "duration": 5.681}, {"text": "overlapping communication and", "start": 7887.92, "duration": 3.84}, {"text": "computation, there is still a", "start": 7889.76, "duration": 3.68}, {"text": "possibility here and this is once again", "start": 7891.76, "duration": 4.0}, {"text": "shown in the Simon Bow article where in", "start": 7893.44, "duration": 5.12}, {"text": "essence you compute half of the", "start": 7895.76, "duration": 4.399}, {"text": "microbatch", "start": 7898.56, "duration": 5.36}, {"text": "and then you send that half while you're", "start": 7900.159, "duration": 5.601}, {"text": "doing the computation of the second", "start": 7903.92, "duration": 4.719}, {"text": "half. And in this case, you overlap the", "start": 7905.76, "duration": 4.16}, {"text": "computation of the second half of the", "start": 7908.639, "duration": 3.681}, {"text": "microbatch with sending the first half", "start": 7909.92, "duration": 5.04}, {"text": "of the microbatch. So once the second", "start": 7912.32, "duration": 6.08}, {"text": "half is done computing, the first half", "start": 7914.96, "duration": 5.92}, {"text": "of the microbatch has already been sent.", "start": 7918.4, "duration": 7.44}, {"text": "So now GPU 2 can already start computing", "start": 7920.88, "duration": 8.08}, {"text": "the activations of the first half of the", "start": 7925.84, "duration": 5.68}, {"text": "microbatch and so on. So you can see the", "start": 7928.96, "duration": 5.04}, {"text": "dependency graph is as such. This is one", "start": 7931.52, "duration": 4.88}, {"text": "way that you could introduce a", "start": 7934.0, "duration": 5.28}, {"text": "computation computation computation", "start": 7936.4, "duration": 5.36}, {"text": "communication overlap, but it is worth", "start": 7939.28, "duration": 3.6}, {"text": "noting that this would increase", "start": 7941.76, "duration": 3.6}, {"text": "complexity quite a lot and with the way", "start": 7942.88, "duration": 6.0}, {"text": "that the kernels are set up, it may not", "start": 7945.36, "duration": 6.64}, {"text": "even work. So there you have the", "start": 7948.88, "duration": 5.92}, {"text": "introduction to Gpipe and now we're", "start": 7952.0, "duration": 5.84}, {"text": "going to implement it. So coming into", "start": 7954.8, "duration": 6.08}, {"text": "the schedule, we have the to-do list", "start": 7957.84, "duration": 5.12}, {"text": "here which we're going to go through one", "start": 7960.88, "duration": 5.52}, {"text": "by one. But first of all, since the", "start": 7962.96, "duration": 7.36}, {"text": "G-pipe algorithm is essentially just uh", "start": 7966.4, "duration": 6.719}, {"text": "the naive algorithm but across batches,", "start": 7970.32, "duration": 5.44}, {"text": "we can just reuse this as our skeleton", "start": 7973.119, "duration": 5.281}, {"text": "here so that we save time. So I'm going", "start": 7975.76, "duration": 5.76}, {"text": "to just copy paste this here and then", "start": 7978.4, "duration": 4.4}, {"text": "we're essentially going to make this a", "start": 7981.52, "duration": 3.44}, {"text": "for loop. So the first thing that it", "start": 7982.8, "duration": 3.919}, {"text": "says is to chunk the batches into", "start": 7984.96, "duration": 3.52}, {"text": "microbatches and the targets into", "start": 7986.719, "duration": 4.321}, {"text": "microtargets. This is a very important", "start": 7988.48, "duration": 4.56}, {"text": "first step. Oh, I think we just ran", "start": 7991.04, "duration": 4.559}, {"text": "something internal by accident. Anyways,", "start": 7993.04, "duration": 6.079}, {"text": "so what we're going to do is say that if", "start": 7995.599, "duration": 6.0}, {"text": "we have model", "start": 7999.119, "duration": 4.321}, {"text": "rank", "start": 8001.599, "duration": 4.56}, {"text": "equal equal to zero, then we'll set up", "start": 8003.44, "duration": 6.48}, {"text": "our micro batches. So we'll say micro", "start": 8006.159, "duration": 10.0}, {"text": "uh batches is equal to the batch and the", "start": 8009.92, "duration": 8.88}, {"text": "chunks. Okay. So in essence we going to", "start": 8016.159, "duration": 5.52}, {"text": "we're going to call torch. There's a", "start": 8018.8, "duration": 6.0}, {"text": "there's a method called torch.chunk.", "start": 8021.679, "duration": 5.121}, {"text": "And I think we sorry guys I think we", "start": 8024.8, "duration": 4.56}, {"text": "have torch as a dependency here in the", "start": 8026.8, "duration": 4.72}, {"text": "top. Yes. Imported. And what does", "start": 8029.36, "duration": 3.92}, {"text": "torch.chunk do? Let's read it or let's", "start": 8031.52, "duration": 3.44}, {"text": "just look at the example rather. The", "start": 8033.28, "duration": 3.2}, {"text": "examples are pretty bad, but it just", "start": 8034.96, "duration": 3.04}, {"text": "takes a input and the number of chunks", "start": 8036.48, "duration": 3.119}, {"text": "that you want to make it into and then", "start": 8038.0, "duration": 4.32}, {"text": "it returns that as the um the chunked", "start": 8039.599, "duration": 4.801}, {"text": "input. So the batch is the input which", "start": 8042.32, "duration": 4.96}, {"text": "is of size 32x 128 if you remember. And", "start": 8044.4, "duration": 4.799}, {"text": "then the chunks is here which we're", "start": 8047.28, "duration": 3.359}, {"text": "going to set up to four. So then it", "start": 8049.199, "duration": 6.0}, {"text": "would become 4x 8x 128 instead of 32x", "start": 8050.639, "duration": 8.321}, {"text": "128. And then we also do the same thing", "start": 8055.199, "duration": 6.96}, {"text": "if we're the last one. So if model.rank", "start": 8058.96, "duration": 6.8}, {"text": "equals equals model.world world", "start": 8062.159, "duration": 6.401}, {"text": "size minus one.", "start": 8065.76, "duration": 5.04}, {"text": "Then we'll say the microcore", "start": 8068.56, "duration": 4.24}, {"text": "oh why am I not just copy pasting this", "start": 8070.8, "duration": 4.319}, {"text": "and then doing that micro targets is", "start": 8072.8, "duration": 5.68}, {"text": "equal to the torch.chunk of instead of", "start": 8075.119, "duration": 6.401}, {"text": "the batch the targets but the chunks is", "start": 8078.48, "duration": 5.44}, {"text": "the same it's four. Okay so we've done", "start": 8081.52, "duration": 4.8}, {"text": "the first thing chunk the batches into", "start": 8083.92, "duration": 4.239}, {"text": "micro batches and the targets into", "start": 8086.32, "duration": 3.6}, {"text": "microtargets", "start": 8088.159, "duration": 3.601}, {"text": "and then we need to initialize buffers", "start": 8089.92, "duration": 4.239}, {"text": "for the inputs and activations. So,", "start": 8091.76, "duration": 6.08}, {"text": "we'll just do that. Input", "start": 8094.159, "duration": 5.92}, {"text": "buffer", "start": 8097.84, "duration": 4.879}, {"text": "equals this. And we'll we'll see why we", "start": 8100.079, "duration": 5.761}, {"text": "want to do this very soon. Output uh", "start": 8102.719, "duration": 4.88}, {"text": "buffer", "start": 8105.84, "duration": 3.52}, {"text": "equals this. I'll just show you very", "start": 8107.599, "duration": 5.201}, {"text": "quickly. This is essentially the manual", "start": 8109.36, "duration": 5.2}, {"text": "uh activation storing that we're going", "start": 8112.8, "duration": 3.919}, {"text": "to be doing. Why are we doing this? It's", "start": 8114.56, "duration": 5.84}, {"text": "because since we have four micro batches", "start": 8116.719, "duration": 8.0}, {"text": "for a single run through the network um", "start": 8120.4, "duration": 6.56}, {"text": "if we're not storing okay this was the", "start": 8124.719, "duration": 4.161}, {"text": "input activations for this microbatch", "start": 8126.96, "duration": 3.279}, {"text": "this was input activations for the", "start": 8128.88, "duration": 3.68}, {"text": "second one then yeah you're kind of", "start": 8130.239, "duration": 4.081}, {"text": "screwed because you need to either store", "start": 8132.56, "duration": 3.44}, {"text": "these in a list if you have them in a in", "start": 8134.32, "duration": 3.2}, {"text": "a local variable then you'll just", "start": 8136.0, "duration": 4.48}, {"text": "override them. So whereas before we", "start": 8137.52, "duration": 5.599}, {"text": "would just say oh where are we? Whereas", "start": 8140.48, "duration": 4.639}, {"text": "before we would say input data equals", "start": 8143.119, "duration": 3.281}, {"text": "this. Now we're going to have to add", "start": 8145.119, "duration": 4.161}, {"text": "this to a list so that we have four um", "start": 8146.4, "duration": 6.799}, {"text": "values one for each micro batch. All", "start": 8149.28, "duration": 5.919}, {"text": "right. So now that that's done, we've", "start": 8153.199, "duration": 3.361}, {"text": "initialized the buffers for the inputs", "start": 8155.199, "duration": 2.48}, {"text": "and activations. We're just going to", "start": 8156.56, "duration": 2.559}, {"text": "call the output the activations in this", "start": 8157.679, "duration": 4.48}, {"text": "case. Now we're going to do a for loop.", "start": 8159.119, "duration": 4.48}, {"text": "And this is where we're going to wrap a", "start": 8162.159, "duration": 3.44}, {"text": "lot of this stuff. So let's just see", "start": 8163.599, "duration": 3.681}, {"text": "where was the where's the backward path", "start": 8165.599, "duration": 3.281}, {"text": "starting. So we can split this into two", "start": 8167.28, "duration": 3.2}, {"text": "halves. All right. So this is the", "start": 8168.88, "duration": 3.279}, {"text": "forward path. Sorry, this is the forward", "start": 8170.48, "duration": 3.36}, {"text": "pass here. This is the backward pass", "start": 8172.159, "duration": 3.681}, {"text": "here. And each one is going to be", "start": 8173.84, "duration": 3.759}, {"text": "wrapped in a for loop. So, we're going", "start": 8175.84, "duration": 4.879}, {"text": "to say for i in range chunks because we", "start": 8177.599, "duration": 5.6}, {"text": "need to do this over the four chunks.", "start": 8180.719, "duration": 4.641}, {"text": "And let's just tab this right here.", "start": 8183.199, "duration": 5.361}, {"text": "What's it saying? It's saying for the", "start": 8185.36, "duration": 6.48}, {"text": "microbatch, if the coms.rank is equal", "start": 8188.56, "duration": 4.959}, {"text": "equal to zero, use the microbatch", "start": 8191.84, "duration": 3.92}, {"text": "directly else receive input. So we're", "start": 8193.519, "duration": 4.0}, {"text": "already pretty much doing this except", "start": 8195.76, "duration": 6.559}, {"text": "now we should add the data to the input", "start": 8197.519, "duration": 6.88}, {"text": "buffer. So let's just go through this to", "start": 8202.319, "duration": 4.641}, {"text": "be sure. If the rank is equal to this", "start": 8204.399, "duration": 5.681}, {"text": "then first of all we're going to say um", "start": 8206.96, "duration": 5.92}, {"text": "the data is equal to microcore", "start": 8210.08, "duration": 5.92}, {"text": "batches of i, right? Because we want the", "start": 8212.88, "duration": 5.759}, {"text": "i microbatch to pass through our", "start": 8216.0, "duration": 5.2}, {"text": "network. For example, if we're on", "start": 8218.639, "duration": 5.281}, {"text": "iteration or i equals 3, then we'll send", "start": 8221.2, "duration": 5.199}, {"text": "the third microbatch to the forward pass", "start": 8223.92, "duration": 5.36}, {"text": "of the network. And then here we're", "start": 8226.399, "duration": 5.681}, {"text": "looking at any device which is not of", "start": 8229.28, "duration": 5.84}, {"text": "rank zero. So the first thing is just to", "start": 8232.08, "duration": 4.559}, {"text": "get the shape of that device. Shape", "start": 8235.12, "duration": 4.0}, {"text": "equals batch hidden uh comma hidden dim", "start": 8236.639, "duration": 4.401}, {"text": "just as we've done in our uh", "start": 8239.12, "duration": 3.04}, {"text": "implementation of here of course since", "start": 8241.04, "duration": 3.92}, {"text": "we copy pasted it. And then now input", "start": 8242.16, "duration": 5.76}, {"text": "data equals coms. Forward of the shape", "start": 8244.96, "duration": 5.12}, {"text": "and the device. This is still fine. And", "start": 8247.92, "duration": 4.32}, {"text": "then input data.requires get equals", "start": 8250.08, "duration": 4.559}, {"text": "true. Okay. And then it's at this point", "start": 8252.24, "duration": 4.239}, {"text": "that we want to add it to the input", "start": 8254.639, "duration": 4.641}, {"text": "buffer. So we'll say", "start": 8256.479, "duration": 5.801}, {"text": "input_buffer.append", "start": 8259.28, "duration": 3.0}, {"text": "of input data. But in fact, we want to", "start": 8263.76, "duration": 6.799}, {"text": "also add the input data for the first", "start": 8267.04, "duration": 6.319}, {"text": "microbatch. Sorry, for the first GPU. So", "start": 8270.559, "duration": 4.16}, {"text": "we'll have to take this out of the loop.", "start": 8273.359, "duration": 3.841}, {"text": "And I just realized that the to-do says", "start": 8274.719, "duration": 4.081}, {"text": "append input output to buffers at the", "start": 8277.2, "duration": 2.72}, {"text": "end. So it doesn't really matter when we", "start": 8278.8, "duration": 2.559}, {"text": "do it as long as we do it before the", "start": 8279.92, "duration": 3.04}, {"text": "backward pass starts. So we'll do that", "start": 8281.359, "duration": 3.36}, {"text": "here. And then I'm just going to jump", "start": 8282.96, "duration": 4.399}, {"text": "the gun and do that also for the output", "start": 8284.719, "duration": 4.321}, {"text": "even though we haven't set the output", "start": 8287.359, "duration": 5.441}, {"text": "yet. Okay. So the output data or it's", "start": 8289.04, "duration": 5.84}, {"text": "actually just called output guys. So", "start": 8292.8, "duration": 5.839}, {"text": "output is set up here. Let's see going", "start": 8294.88, "duration": 6.639}, {"text": "through. Right. We've done this. We've", "start": 8298.639, "duration": 7.201}, {"text": "done yeah this as well as this.", "start": 8301.519, "duration": 5.681}, {"text": "Now it's saying if not last stage send", "start": 8305.84, "duration": 2.96}, {"text": "the output to the next stage. We've also", "start": 8307.2, "duration": 4.399}, {"text": "done that. Output equals model input", "start": 8308.8, "duration": 5.04}, {"text": "data, targets. One thing that I would", "start": 8311.599, "duration": 4.241}, {"text": "like to quickly change here though is", "start": 8313.84, "duration": 4.719}, {"text": "the targets because now the targets are", "start": 8315.84, "duration": 7.679}, {"text": "only set as microtargets if we are on", "start": 8318.559, "duration": 7.441}, {"text": "the last device whereas before it was", "start": 8323.519, "duration": 4.88}, {"text": "set to none otherwise. So the targets", "start": 8326.0, "duration": 4.0}, {"text": "were set to none. here. Since we're", "start": 8328.399, "duration": 2.721}, {"text": "batching the targets, I don't want to", "start": 8330.0, "duration": 4.719}, {"text": "make a batch of none um targets for", "start": 8331.12, "duration": 5.12}, {"text": "every single device. Instead, we're just", "start": 8334.719, "duration": 5.201}, {"text": "going to make the input based on whether", "start": 8336.24, "duration": 5.199}, {"text": "we're on the last device or not. So,", "start": 8339.92, "duration": 5.12}, {"text": "what I mean by that is if model.rank", "start": 8341.439, "duration": 5.521}, {"text": "equals equals I should I should have", "start": 8345.04, "duration": 3.2}, {"text": "this in my clipboard so I don't have to", "start": 8346.96, "duration": 2.559}, {"text": "write it every time. But anyways,", "start": 8348.24, "duration": 3.84}, {"text": "model.world size", "start": 8349.519, "duration": 4.641}, {"text": "minus one,", "start": 8352.08, "duration": 4.8}, {"text": "right? We're going to do this and we're", "start": 8354.16, "duration": 5.92}, {"text": "going to take the microtargets", "start": 8356.88, "duration": 5.439}, {"text": "instead of the actual targets themselves", "start": 8360.08, "duration": 6.16}, {"text": "of I and then otherwise right we're just", "start": 8362.319, "duration": 6.961}, {"text": "going to do the model", "start": 8366.24, "duration": 5.84}, {"text": "on the input data only because if we", "start": 8369.28, "duration": 5.68}, {"text": "come to model.py it expects the targets", "start": 8372.08, "duration": 5.2}, {"text": "to be none by default anyways. It's", "start": 8374.96, "duration": 7.12}, {"text": "coming back here. This is valid for both", "start": 8377.28, "duration": 7.84}, {"text": "instances. And", "start": 8382.08, "duration": 6.0}, {"text": "right, let's see if not last stage and", "start": 8385.12, "duration": 4.319}, {"text": "output to the next stage. Are we doing", "start": 8388.08, "duration": 3.84}, {"text": "that? Yes. com comms. Forward output.det", "start": 8389.439, "duration": 5.521}, {"text": "detach. Exactly. And then append the", "start": 8391.92, "duration": 5.439}, {"text": "input and the output to the buffers. So", "start": 8394.96, "duration": 4.08}, {"text": "it looks like everything's done for the", "start": 8397.359, "duration": 4.801}, {"text": "forward pass. Very nice. Now if we come", "start": 8399.04, "duration": 5.68}, {"text": "to the backward pass, it's going to be", "start": 8402.16, "duration": 5.199}, {"text": "somewhat similar. So we'll first iterate", "start": 8404.72, "duration": 5.36}, {"text": "over all the chunks here and indent", "start": 8407.359, "duration": 4.561}, {"text": "first. But now we're going to have to", "start": 8410.08, "duration": 4.399}, {"text": "start to call items from the input and", "start": 8411.92, "duration": 4.72}, {"text": "the output buffer.", "start": 8414.479, "duration": 4.401}, {"text": "So what does it say? It says get inputs", "start": 8416.64, "duration": 5.04}, {"text": "and outputs for this chunk from buffers.", "start": 8418.88, "duration": 7.439}, {"text": "Very well. So we'll say in this case the", "start": 8421.68, "duration": 7.639}, {"text": "input", "start": 8426.319, "duration": 3.0}, {"text": "equals input.buffer buffer input_buffer", "start": 8429.52, "duration": 6.839}, {"text": "of I", "start": 8433.04, "duration": 3.319}, {"text": "and we'll do the same for output and", "start": 8436.399, "duration": 3.441}, {"text": "let's see actually what we're going to", "start": 8438.64, "duration": 5.04}, {"text": "call this first. So yeah, we get the", "start": 8439.84, "duration": 6.4}, {"text": "output here. So we'll just call it that", "start": 8443.68, "duration": 4.4}, {"text": "and then how do we call input in this", "start": 8446.24, "duration": 3.44}, {"text": "case? We call it input data. That's", "start": 8448.08, "duration": 5.52}, {"text": "right. So if we come here say input data", "start": 8449.68, "duration": 7.52}, {"text": "equals this and then output equals that.", "start": 8453.6, "duration": 5.44}, {"text": "All right. So this should this should be", "start": 8457.2, "duration": 4.72}, {"text": "good now. And otherwise, what else are", "start": 8459.04, "duration": 4.48}, {"text": "we doing? So, we get the inputs from the", "start": 8461.92, "duration": 3.36}, {"text": "chunk from the buffers. If for the last", "start": 8463.52, "duration": 3.12}, {"text": "stage, we compute the loss and call", "start": 8465.28, "duration": 3.44}, {"text": "backward. What are we doing here? I", "start": 8466.64, "duration": 3.44}, {"text": "think we still need to index a few", "start": 8468.72, "duration": 3.04}, {"text": "items. So, let's just make sure we're", "start": 8470.08, "duration": 2.88}, {"text": "doing that. If model that rank is the", "start": 8471.76, "duration": 3.12}, {"text": "last one, loss equals output and then", "start": 8472.96, "duration": 6.24}, {"text": "loss. Ah, I just forgot. Now, the thing", "start": 8474.88, "duration": 7.68}, {"text": "is we're doing gradient accumulation. So", "start": 8479.2, "duration": 8.159}, {"text": "our loss is actually going to be a um", "start": 8482.56, "duration": 8.96}, {"text": "sum of the four losses for each micro", "start": 8487.359, "duration": 5.681}, {"text": "batch and then we divide by the chunk", "start": 8491.52, "duration": 3.52}, {"text": "size at the end. So we're just going to", "start": 8493.04, "duration": 5.92}, {"text": "initialize our loss as a torch.tensor", "start": 8495.04, "duration": 6.88}, {"text": "of size output", "start": 8498.96, "duration": 4.72}, {"text": "dot", "start": 8501.92, "duration": 4.16}, {"text": "um shape. Do we have an output tensor", "start": 8503.68, "duration": 6.32}, {"text": "here to refer to? this because the um", "start": 8506.08, "duration": 6.8}, {"text": "the loss or the activations are the same", "start": 8510.0, "duration": 4.72}, {"text": "size as the output except for the scalar", "start": 8512.88, "duration": 4.559}, {"text": "loss. In any case, we're going to add", "start": 8514.72, "duration": 4.96}, {"text": "the loss to this every single time. So", "start": 8517.439, "duration": 5.201}, {"text": "loss equals output loss.backward and", "start": 8519.68, "duration": 7.44}, {"text": "then we'll do loss plus equals loss. And", "start": 8522.64, "duration": 6.0}, {"text": "I realize now that we should call this", "start": 8527.12, "duration": 4.239}, {"text": "total loss or else we will get mixed up", "start": 8528.64, "duration": 5.36}, {"text": "with the two different the two values.", "start": 8531.359, "duration": 4.481}, {"text": "one is the entire count and the other is", "start": 8534.0, "duration": 6.479}, {"text": "just the loss. Okay. And then afterwards", "start": 8535.84, "duration": 7.92}, {"text": "we're going to check if the model rank", "start": 8540.479, "duration": 6.96}, {"text": "is the last one then we'll return the", "start": 8543.76, "duration": 5.92}, {"text": "loss and return the total loss to be", "start": 8547.439, "duration": 4.161}, {"text": "precise. And we want to take that out of", "start": 8549.68, "duration": 6.08}, {"text": "the for loop because if we don't then", "start": 8551.6, "duration": 7.04}, {"text": "what will happen is that as soon as we", "start": 8555.76, "duration": 7.04}, {"text": "calculate the loss of one of these", "start": 8558.64, "duration": 7.6}, {"text": "values here it will um right as soon as", "start": 8562.8, "duration": 5.519}, {"text": "we calculate this loss here which is", "start": 8566.24, "duration": 3.52}, {"text": "equivalent to this one here then it will", "start": 8568.319, "duration": 3.201}, {"text": "terminate but we want to calculate all", "start": 8569.76, "duration": 4.719}, {"text": "four losses for all four batches. So", "start": 8571.52, "duration": 4.56}, {"text": "that's why it's going to become out of", "start": 8574.479, "duration": 3.281}, {"text": "the for loop. We could also say if", "start": 8576.08, "duration": 3.84}, {"text": "modelrank equals world size minus one", "start": 8577.76, "duration": 6.0}, {"text": "and the i is equal to three. But um", "start": 8579.92, "duration": 6.399}, {"text": "let's just do this to be more rigorous", "start": 8583.76, "duration": 4.639}, {"text": "to take it out of the for loop entirely.", "start": 8586.319, "duration": 6.16}, {"text": "So right now we have everything working", "start": 8588.399, "duration": 5.521}, {"text": "except the fact that the loss should be", "start": 8592.479, "duration": 4.321}, {"text": "initialized to zero. So torch.z is our", "start": 8593.92, "duration": 7.519}, {"text": "initial loss and in theory this should", "start": 8596.8, "duration": 7.2}, {"text": "work as our g-pipe implementation. So in", "start": 8601.439, "duration": 5.601}, {"text": "resume\u00e9 we just took the naive", "start": 8604.0, "duration": 4.16}, {"text": "implementation and then wrapped it", "start": 8607.04, "duration": 4.319}, {"text": "around with microbatches and let's see", "start": 8608.16, "duration": 5.04}, {"text": "did we do everything right return loss", "start": 8611.359, "duration": 3.441}, {"text": "of last stage else none that's that's", "start": 8613.2, "duration": 4.08}, {"text": "true because if we don't return the", "start": 8614.8, "duration": 4.24}, {"text": "total loss here then for every other", "start": 8617.28, "duration": 3.68}, {"text": "device from 0 to two it'll just return", "start": 8619.04, "duration": 5.2}, {"text": "none okay but the unfortunate thing is", "start": 8620.96, "duration": 5.84}, {"text": "that we can't just come in this code and", "start": 8624.24, "duration": 8.56}, {"text": "do UV run torch run d- n process it", "start": 8626.8, "duration": 8.32}, {"text": "takes a to write these things. Hey, per", "start": 8632.8, "duration": 8.8}, {"text": "node of four and run the step five.", "start": 8635.12, "duration": 9.12}, {"text": "Well, one thing I just realized is okay,", "start": 8641.6, "duration": 4.879}, {"text": "let's first of all get the correct", "start": 8644.24, "duration": 4.159}, {"text": "directory and then run step five. So,", "start": 8646.479, "duration": 3.441}, {"text": "this should work because it's still", "start": 8648.399, "duration": 5.201}, {"text": "wired up to use the naive solution, but", "start": 8649.92, "duration": 5.92}, {"text": "I realize yeah, we haven't even imported", "start": 8653.6, "duration": 5.04}, {"text": "the other one. So, what we want is", "start": 8655.84, "duration": 4.479}, {"text": "gpipe.", "start": 8658.64, "duration": 4.16}, {"text": "Okay. And then we're just going to also", "start": 8660.319, "duration": 5.201}, {"text": "add a parameter here called um not batch", "start": 8662.8, "duration": 5.519}, {"text": "but rather chunks or just let's call it", "start": 8665.52, "duration": 4.4}, {"text": "chunk. Who cares? Uh no it should be", "start": 8668.319, "duration": 2.801}, {"text": "plural because it's going to be four", "start": 8669.92, "duration": 2.8}, {"text": "chunks.", "start": 8671.12, "duration": 4.16}, {"text": "And then coming down here we want to", "start": 8672.72, "duration": 5.84}, {"text": "change this to begpipe", "start": 8675.28, "duration": 5.84}, {"text": "right and then we see that the chunks is", "start": 8678.56, "duration": 4.0}, {"text": "the second to last argument for whatever", "start": 8681.12, "duration": 4.319}, {"text": "reason. So we're going to add that here", "start": 8682.56, "duration": 5.28}, {"text": "as chunks.", "start": 8685.439, "duration": 4.96}, {"text": "Okay. And another thing worth noting is", "start": 8687.84, "duration": 3.76}, {"text": "that we should always be dividing the", "start": 8690.399, "duration": 4.241}, {"text": "loss by the number of chunks or else", "start": 8691.6, "duration": 4.56}, {"text": "we'll get four times the loss since", "start": 8694.64, "duration": 2.96}, {"text": "we're using grading doing gradient", "start": 8696.16, "duration": 4.4}, {"text": "accumulation. And", "start": 8697.6, "duration": 5.2}, {"text": "there's probably another few things that", "start": 8700.56, "duration": 5.44}, {"text": "we have to do here. However,", "start": 8702.8, "duration": 4.559}, {"text": "I think it's better if we just run this", "start": 8706.0, "duration": 3.04}, {"text": "to get an error and then see what we", "start": 8707.359, "duration": 4.0}, {"text": "need to change. If this works, then", "start": 8709.04, "duration": 5.2}, {"text": "that'll be even better. But I yeah, I'm", "start": 8711.359, "duration": 4.481}, {"text": "somewhat suspicious that it won't work", "start": 8714.24, "duration": 4.64}, {"text": "the first time. So", "start": 8715.84, "duration": 5.12}, {"text": "currently there seems to be an infinite", "start": 8718.88, "duration": 5.12}, {"text": "loop occurring in our code. So the first", "start": 8720.96, "duration": 4.32}, {"text": "thing I'm just going to do is make sure", "start": 8724.0, "duration": 2.56}, {"text": "that we have everything that we need", "start": 8725.28, "duration": 5.36}, {"text": "here to run the uh the chunks properly.", "start": 8726.56, "duration": 6.08}, {"text": "I think that yeah there's nothing", "start": 8730.64, "duration": 4.32}, {"text": "happening on the side of main.py. There", "start": 8732.64, "duration": 4.48}, {"text": "must be something wrong in ouruler. So", "start": 8734.96, "duration": 4.479}, {"text": "let's just terminate this and see if we", "start": 8737.12, "duration": 6.239}, {"text": "get any errors that we can analyze.", "start": 8739.439, "duration": 6.801}, {"text": "Okay. So I just compared the code to the", "start": 8743.359, "duration": 4.321}, {"text": "ground truth because I couldn't find the", "start": 8746.24, "duration": 3.68}, {"text": "bug. And the problem that we have", "start": 8747.68, "duration": 6.4}, {"text": "currently is with the size that we're", "start": 8749.92, "duration": 6.72}, {"text": "setting here which is batch size of 32", "start": 8754.08, "duration": 5.279}, {"text": "even though the shape of the tense that", "start": 8756.64, "duration": 5.04}, {"text": "we're receiving is actually only of", "start": 8759.359, "duration": 6.161}, {"text": "batch divided by chunks.", "start": 8761.68, "duration": 6.0}, {"text": "So this is what we receive for every", "start": 8765.52, "duration": 3.839}, {"text": "single micro batch because it's 32", "start": 8767.68, "duration": 3.44}, {"text": "divided by 4 which would be eight in", "start": 8769.359, "duration": 4.321}, {"text": "this case. So, this is strange though", "start": 8771.12, "duration": 4.16}, {"text": "because I thought that such an error", "start": 8773.68, "duration": 4.719}, {"text": "would give you a um a runtime error, but", "start": 8775.28, "duration": 5.119}, {"text": "it was just running forever. So, I'm", "start": 8778.399, "duration": 3.361}, {"text": "going to try one more time and maybe", "start": 8780.399, "duration": 3.361}, {"text": "this will Okay, nice. This gives us", "start": 8781.76, "duration": 4.16}, {"text": "another error that wasn't the same. So,", "start": 8783.76, "duration": 4.16}, {"text": "what it's saying here Oh, okay. Received", "start": 8785.92, "duration": 3.84}, {"text": "an invalid combination. So, I think for", "start": 8787.92, "duration": 5.439}, {"text": "the torch.zer's function, we actually", "start": 8789.76, "duration": 5.599}, {"text": "what is the error with this? Right. So,", "start": 8793.359, "duration": 3.441}, {"text": "it actually is a problem in the", "start": 8795.359, "duration": 2.96}, {"text": "receive/forward.", "start": 8796.8, "duration": 5.36}, {"text": "If we look in the error, it's where is", "start": 8798.319, "duration": 6.801}, {"text": "it exactly happening? It's saying the", "start": 8802.16, "duration": 4.48}, {"text": "coms that receive forward has got an", "start": 8805.12, "duration": 2.96}, {"text": "error and it's because we're giving it a", "start": 8806.64, "duration": 4.96}, {"text": "float, I believe, of DT type of 32. So,", "start": 8808.08, "duration": 4.88}, {"text": "this is just because I did not do", "start": 8811.6, "duration": 2.719}, {"text": "integers division here. So, that's", "start": 8812.96, "duration": 4.24}, {"text": "another big mistake. And I just made", "start": 8814.319, "duration": 4.961}, {"text": "another mistake, but let's hope it works", "start": 8817.2, "duration": 4.32}, {"text": "now.", "start": 8819.28, "duration": 4.079}, {"text": "All right. So, that's really awesome.", "start": 8821.52, "duration": 3.76}, {"text": "Looks like we have the micro batch", "start": 8823.359, "duration": 5.281}, {"text": "working properly. And if we just want to", "start": 8825.28, "duration": 6.32}, {"text": "make sure that we get the same value as", "start": 8828.64, "duration": 6.0}, {"text": "the ground truth, we can check 0.1779", "start": 8831.6, "duration": 7.839}, {"text": "and 8. And if we come into SRC and run", "start": 8834.64, "duration": 7.12}, {"text": "this instead, which I've now changed to", "start": 8839.439, "duration": 6.88}, {"text": "run with the same uh algorithm gpipe", "start": 8841.76, "duration": 6.88}, {"text": "instead of the knife solution, we get", "start": 8846.319, "duration": 4.721}, {"text": "the same thing. So this is really nice.", "start": 8848.64, "duration": 4.96}, {"text": "And instead of profiling this now to see", "start": 8851.04, "duration": 4.24}, {"text": "how the bubbles have changed because", "start": 8853.6, "duration": 3.92}, {"text": "this in theory should give us that same", "start": 8855.28, "duration": 5.119}, {"text": "42% since we did four micro batches.", "start": 8857.52, "duration": 4.32}, {"text": "Then we're not going to do this now.", "start": 8860.399, "duration": 2.801}, {"text": "We're going to do this after we do the", "start": 8861.84, "duration": 2.96}, {"text": "last algorithm so we can compare them at", "start": 8863.2, "duration": 3.279}, {"text": "the very end. But let's just try one", "start": 8864.8, "duration": 3.44}, {"text": "thing really quickly and change the", "start": 8866.479, "duration": 3.521}, {"text": "number of chunks to eight which would", "start": 8868.24, "duration": 5.119}, {"text": "make the um device utilization much", "start": 8870.0, "duration": 6.319}, {"text": "higher but also increase the memory use.", "start": 8873.359, "duration": 4.881}, {"text": "And yeah, we can't really measure the", "start": 8876.319, "duration": 3.361}, {"text": "memory use here. So we need to use the", "start": 8878.24, "duration": 2.88}, {"text": "profiler to see what's actually", "start": 8879.68, "duration": 3.04}, {"text": "happening. But at least it's not", "start": 8881.12, "duration": 4.0}, {"text": "changing the final loss when we improve", "start": 8882.72, "duration": 4.24}, {"text": "when we increase the number of chunks.", "start": 8885.12, "duration": 3.76}, {"text": "That's a good sign. Something you also", "start": 8886.96, "duration": 3.76}, {"text": "might have thought as we're implementing", "start": 8888.88, "duration": 5.84}, {"text": "G-pipe is shouldn't we go back through", "start": 8890.72, "duration": 6.08}, {"text": "the chunks in reverse order in the", "start": 8894.72, "duration": 4.32}, {"text": "backward pass since the backward pass", "start": 8896.8, "duration": 5.44}, {"text": "goes from the last to the first GPU. And", "start": 8899.04, "duration": 5.76}, {"text": "when I talked with Gemini about this, it", "start": 8902.24, "duration": 4.159}, {"text": "told me that no, you should do the first", "start": 8904.8, "duration": 3.36}, {"text": "in first out order. But then when I", "start": 8906.399, "duration": 3.761}, {"text": "tested it, it was actually giving the", "start": 8908.16, "duration": 3.92}, {"text": "same results anyway. So if I do for", "start": 8910.16, "duration": 3.76}, {"text": "reversed range of chunks, let's see what", "start": 8912.08, "duration": 5.44}, {"text": "we get. We end up getting 0.177998.", "start": 8913.92, "duration": 5.6}, {"text": "And just as a reminder, this is the", "start": 8917.52, "duration": 4.799}, {"text": "exact same value that we got when we", "start": 8919.52, "duration": 6.959}, {"text": "iterated from 0 to 3 instead of from 3", "start": 8922.319, "duration": 7.201}, {"text": "to 0. So if anybody knows how we're", "start": 8926.479, "duration": 6.081}, {"text": "still getting the same output whether or", "start": 8929.52, "duration": 4.4}, {"text": "not we iterate through our chunks in", "start": 8932.56, "duration": 3.52}, {"text": "reverse or forward order, then please", "start": 8933.92, "duration": 3.6}, {"text": "enlighten us. But it's just an", "start": 8936.08, "duration": 2.88}, {"text": "interesting observation that I want to", "start": 8937.52, "duration": 2.959}, {"text": "make. So it doesn't really matter in", "start": 8938.96, "duration": 3.2}, {"text": "which way you iterate through your", "start": 8940.479, "duration": 4.721}, {"text": "batches when you're using GPU. And now", "start": 8942.16, "duration": 6.159}, {"text": "if we come back to our course outline,", "start": 8945.2, "duration": 4.88}, {"text": "we only have one thing left. This last", "start": 8948.319, "duration": 4.801}, {"text": "uh point is left as an exercise. So 1", "start": 8950.08, "duration": 6.08}, {"text": "F1B is the last topic that we're going", "start": 8953.12, "duration": 5.68}, {"text": "to cover. And it's called pipe dream.", "start": 8956.16, "duration": 5.76}, {"text": "One forward, one backward. And what does", "start": 8958.8, "duration": 4.24}, {"text": "it do? It accelerates pipeline", "start": 8961.92, "duration": 2.32}, {"text": "parallelism by starting the backward", "start": 8963.04, "duration": 3.12}, {"text": "pass for microbatch as soon as its", "start": 8964.24, "duration": 3.84}, {"text": "forward pass completes the final stage", "start": 8966.16, "duration": 4.08}, {"text": "enabling earlier disposal of cached", "start": 8968.08, "duration": 3.76}, {"text": "earlier disposal sorry of cacheed", "start": 8970.24, "duration": 3.52}, {"text": "activations. So what it means is that as", "start": 8971.84, "duration": 4.639}, {"text": "soon as you finish the entire forward", "start": 8973.76, "duration": 5.12}, {"text": "pass of the first micro batch then you", "start": 8976.479, "duration": 3.92}, {"text": "start the backward pass as soon as", "start": 8978.88, "duration": 4.16}, {"text": "possible and this allows you to free up", "start": 8980.399, "duration": 7.04}, {"text": "the activations sooner than with G-pipe.", "start": 8983.04, "duration": 7.04}, {"text": "And this 1F1B algorithm can be composed", "start": 8987.439, "duration": 6.0}, {"text": "of three parts. the warm up where you", "start": 8990.08, "duration": 6.08}, {"text": "load in all of the microbatches. This is", "start": 8993.439, "duration": 4.321}, {"text": "what you call the steady state in the", "start": 8996.16, "duration": 4.159}, {"text": "middle here. And then the flush where", "start": 8997.76, "duration": 5.36}, {"text": "you get rid of all of the or you finish", "start": 9000.319, "duration": 5.201}, {"text": "all the backward passes rather. So in", "start": 9003.12, "duration": 4.72}, {"text": "the steady state, each device alternates", "start": 9005.52, "duration": 3.839}, {"text": "between forward and backward passes. You", "start": 9007.84, "duration": 3.36}, {"text": "can see this here would go backward,", "start": 9009.359, "duration": 4.161}, {"text": "forward, backward, forward. And with", "start": 9011.2, "duration": 4.0}, {"text": "four GPUs and eight microbatches, which", "start": 9013.52, "duration": 3.6}, {"text": "is the case here, at most four", "start": 9015.2, "duration": 3.92}, {"text": "microbatches are in flight at any time,", "start": 9017.12, "duration": 3.68}, {"text": "having the same peak activation memory", "start": 9019.12, "duration": 3.52}, {"text": "as G-pipe. And what we mean when we say", "start": 9020.8, "duration": 3.679}, {"text": "a microbatch is in flight is if we've", "start": 9022.64, "duration": 3.759}, {"text": "performed more than one forward pass for", "start": 9024.479, "duration": 3.281}, {"text": "it, but haven't completed all the", "start": 9026.399, "duration": 3.121}, {"text": "backward passes yet. And one way to", "start": 9027.76, "duration": 3.76}, {"text": "calculate the peak activation memory is", "start": 9029.52, "duration": 3.44}, {"text": "just the number of macro batches in", "start": 9031.52, "duration": 4.56}, {"text": "flight at one time multiplied by the", "start": 9032.96, "duration": 5.439}, {"text": "size of those microbatches and the", "start": 9036.08, "duration": 5.52}, {"text": "number of layers per GPU. So in this", "start": 9038.399, "duration": 5.361}, {"text": "case it would be 16 divided 4 here and", "start": 9041.6, "duration": 4.32}, {"text": "then the microbatch size would be 8 and", "start": 9043.76, "duration": 3.599}, {"text": "the number of microbatches in flight", "start": 9045.92, "duration": 7.04}, {"text": "would be 4. So it's 4 * 8 * 4. So that", "start": 9047.359, "duration": 10.401}, {"text": "would give us 64 * 8 or 512 as the", "start": 9052.96, "duration": 6.399}, {"text": "memory unit. So it would depend on our", "start": 9057.76, "duration": 2.96}, {"text": "hidden layers how much memory this", "start": 9059.359, "duration": 4.08}, {"text": "actually takes in reality. But all we", "start": 9060.72, "duration": 4.88}, {"text": "need to know is that at a given time,", "start": 9063.439, "duration": 3.601}, {"text": "right, when all of the devices are being", "start": 9065.6, "duration": 2.879}, {"text": "used at once, the max number of", "start": 9067.04, "duration": 3.12}, {"text": "microbatches that are in flight is four", "start": 9068.479, "duration": 4.0}, {"text": "because we only have four GPUs. So", "start": 9070.16, "duration": 4.4}, {"text": "that's the same peak activation memory", "start": 9072.479, "duration": 4.161}, {"text": "as GPIP, which is really desirable. One", "start": 9074.56, "duration": 3.36}, {"text": "thing worth noting is that despite the", "start": 9076.64, "duration": 2.96}, {"text": "memory advantage achieved by being able", "start": 9077.92, "duration": 3.76}, {"text": "to free up the activations much sooner", "start": 9079.6, "duration": 4.08}, {"text": "with this algorithm that has the steady", "start": 9081.68, "duration": 4.16}, {"text": "state, if you look, for example, by the", "start": 9083.68, "duration": 3.6}, {"text": "time we've done the entire forward", "start": 9085.84, "duration": 3.36}, {"text": "backward pass for the first microbatch,", "start": 9087.28, "duration": 3.36}, {"text": "the fifth microbatch hasn't even started", "start": 9089.2, "duration": 4.08}, {"text": "yet. So we're much faster in terms of", "start": 9090.64, "duration": 4.32}, {"text": "freeing up these activations. The amount", "start": 9093.28, "duration": 3.84}, {"text": "of idle time due to dependencies is the", "start": 9094.96, "duration": 4.0}, {"text": "same for G-pipe and pipe dream because", "start": 9097.12, "duration": 3.52}, {"text": "of the sequential dependency structure", "start": 9098.96, "duration": 4.32}, {"text": "of the fact that you must do a forward", "start": 9100.64, "duration": 5.2}, {"text": "on a previous layer before you can do it", "start": 9103.28, "duration": 5.28}, {"text": "on a succeeding layer. And one thing is", "start": 9105.84, "duration": 4.479}, {"text": "that if you look at the above diagram", "start": 9108.56, "duration": 3.68}, {"text": "and you shift the blue forward passes", "start": 9110.319, "duration": 3.921}, {"text": "left and the green backward passes", "start": 9112.24, "duration": 4.32}, {"text": "right, you get G-pipe. And this explains", "start": 9114.24, "duration": 3.68}, {"text": "why the bubble fraction is the same. So", "start": 9116.56, "duration": 2.56}, {"text": "the amount of idle time is the same", "start": 9117.92, "duration": 3.68}, {"text": "because of this uh sequential dependency", "start": 9119.12, "duration": 5.12}, {"text": "but instead the activation memories are", "start": 9121.6, "duration": 5.44}, {"text": "much lighter as opposed to G-pipe. So", "start": 9124.24, "duration": 5.6}, {"text": "just to emphasize this shift all you do", "start": 9127.04, "duration": 5.359}, {"text": "is you take 5 6 7 and 8 and you move", "start": 9129.84, "duration": 4.72}, {"text": "them to the left and you move them down", "start": 9132.399, "duration": 5.441}, {"text": "and then you would get a set of eight", "start": 9134.56, "duration": 6.56}, {"text": "micro batches that go so they go down in", "start": 9137.84, "duration": 7.28}, {"text": "this direction. And then if you take the", "start": 9141.12, "duration": 6.8}, {"text": "green backward calls, you can already", "start": 9145.12, "duration": 4.72}, {"text": "see that they're going in that", "start": 9147.92, "duration": 4.08}, {"text": "direction. So if you just move the blue", "start": 9149.84, "duration": 4.16}, {"text": "to the left and the uh the green to the", "start": 9152.0, "duration": 5.76}, {"text": "right, then you have the G-pipe", "start": 9154.0, "duration": 6.88}, {"text": "algorithm for eight micro batches and", "start": 9157.76, "duration": 4.639}, {"text": "there's no difference in terms of idle", "start": 9160.88, "duration": 3.36}, {"text": "time in this case. And another last", "start": 9162.399, "duration": 3.681}, {"text": "thing worth noting is just for", "start": 9164.24, "duration": 3.76}, {"text": "communication.", "start": 9166.08, "duration": 5.359}, {"text": "Both the G-pipe and pipe dream 1 F1B", "start": 9168.0, "duration": 6.399}, {"text": "algorithm require each GPU to send and", "start": 9171.439, "duration": 4.561}, {"text": "receive", "start": 9174.399, "duration": 4.721}, {"text": "N times batch size data per forward and", "start": 9176.0, "duration": 5.2}, {"text": "per backward. So that means two times", "start": 9179.12, "duration": 4.96}, {"text": "batch size* N where n is the hidden size", "start": 9181.2, "duration": 6.159}, {"text": "of the model. So in the forward pass you", "start": 9184.08, "duration": 6.96}, {"text": "have to send a tensor of size 32x 128", "start": 9187.359, "duration": 6.241}, {"text": "for all the GPUs and in the backward", "start": 9191.04, "duration": 3.92}, {"text": "pass you do the same thing. You send", "start": 9193.6, "duration": 4.4}, {"text": "this tensor of 32x 128 backwards through", "start": 9194.96, "duration": 4.8}, {"text": "all the GPUs and then we of course", "start": 9198.0, "duration": 3.12}, {"text": "multiply this by two because we do it", "start": 9199.76, "duration": 3.12}, {"text": "once for forward once for backward and", "start": 9201.12, "duration": 3.04}, {"text": "multiply it by the number of GPUs", "start": 9202.88, "duration": 2.4}, {"text": "because we need to send it for every", "start": 9204.16, "duration": 2.96}, {"text": "single GPU. But then we're doing minus", "start": 9205.28, "duration": 3.28}, {"text": "one here. Why are we doing minus one", "start": 9207.12, "duration": 3.359}, {"text": "GPU? The minus one terms comes from the", "start": 9208.56, "duration": 3.839}, {"text": "fact that the initial GPU and the last", "start": 9210.479, "duration": 4.321}, {"text": "GPU both miss one operation. So the", "start": 9212.399, "duration": 4.321}, {"text": "initial GPU does not receive anything", "start": 9214.8, "duration": 3.92}, {"text": "and the last GPU does not send anything.", "start": 9216.72, "duration": 3.92}, {"text": "So we can kind of think that we remove", "start": 9218.72, "duration": 3.84}, {"text": "one GPU from the picture or we remove", "start": 9220.64, "duration": 4.08}, {"text": "half a GPU operation, right? Because the", "start": 9222.56, "duration": 5.52}, {"text": "GPU one here is not receiving any from a", "start": 9224.72, "duration": 6.08}, {"text": "forward pass and the last GPU here uh", "start": 9228.08, "duration": 7.76}, {"text": "GPU 4 is not going to send anything. So", "start": 9230.8, "duration": 8.4}, {"text": "there's the overview of 1 F1B pipe", "start": 9235.84, "duration": 5.44}, {"text": "dream. And now we're going to be doing", "start": 9239.2, "duration": 4.64}, {"text": "something some might find funny, but in", "start": 9241.28, "duration": 4.96}, {"text": "order to gain further intuition on 1", "start": 9243.84, "duration": 5.12}, {"text": "F1B, we're going to derive it for", "start": 9246.24, "duration": 6.56}, {"text": "ourselves in this Google sheet. So as", "start": 9248.96, "duration": 6.399}, {"text": "you can see, we have this split up in", "start": 9252.8, "duration": 5.36}, {"text": "the grids which perfectly resemble the", "start": 9255.359, "duration": 6.401}, {"text": "GPUs and each tile is one operation.", "start": 9258.16, "duration": 4.96}, {"text": "What I'd like to say first of all is if", "start": 9261.76, "duration": 2.4}, {"text": "you've already understood from the", "start": 9263.12, "duration": 2.64}, {"text": "initial explanation on how one forward,", "start": 9264.16, "duration": 3.12}, {"text": "one backward works, then feel free to", "start": 9265.76, "duration": 3.2}, {"text": "skip ahead. But if not, then I think", "start": 9267.28, "duration": 3.84}, {"text": "this this will be very this will be very", "start": 9268.96, "duration": 4.0}, {"text": "helpful to gain intuition on how this", "start": 9271.12, "duration": 3.92}, {"text": "algorithm works fundamentally. So we're", "start": 9272.96, "duration": 4.32}, {"text": "just going to be literally numbering the", "start": 9275.04, "duration": 6.56}, {"text": "micro batches like this and the grid one", "start": 9277.28, "duration": 7.199}, {"text": "here will be GPU 0, two will be one,", "start": 9281.6, "duration": 5.2}, {"text": "three will be two, and four will be", "start": 9284.479, "duration": 5.92}, {"text": "three. So let's first talk about the", "start": 9286.8, "duration": 6.08}, {"text": "warm-up stage and how this is", "start": 9290.399, "duration": 4.241}, {"text": "calculated. So, I haven't previously", "start": 9292.88, "duration": 3.36}, {"text": "presented the formula for calculating", "start": 9294.64, "duration": 3.759}, {"text": "the warm-up stage, where you're not yet", "start": 9296.24, "duration": 4.239}, {"text": "doing the one forward, one backward run.", "start": 9298.399, "duration": 4.641}, {"text": "So, just as a reminder here, the three", "start": 9300.479, "duration": 4.96}, {"text": "stages are warm up, where you're just", "start": 9303.04, "duration": 4.48}, {"text": "only doing forward passes, and then the", "start": 9305.439, "duration": 3.681}, {"text": "steady state where it's one forward, one", "start": 9307.52, "duration": 3.52}, {"text": "backward. That's why it's called 1 F1B.", "start": 9309.12, "duration": 3.279}, {"text": "And then the cool down stage where", "start": 9311.04, "duration": 3.04}, {"text": "you're only doing backward passes. So", "start": 9312.399, "duration": 3.92}, {"text": "the warm-up stage has a formula and you", "start": 9314.08, "duration": 5.76}, {"text": "can calculate it as the warm-up equal to", "start": 9316.319, "duration": 5.441}, {"text": "world", "start": 9319.84, "duration": 4.559}, {"text": "size minus rank minus one. And let's", "start": 9321.76, "duration": 5.04}, {"text": "understand the intuition here with an", "start": 9324.399, "duration": 8.08}, {"text": "example. So for the last GPU,", "start": 9326.8, "duration": 9.12}, {"text": "what is the logical answer for warm-up?", "start": 9332.479, "duration": 5.281}, {"text": "The logical answer is that there is zero", "start": 9335.92, "duration": 3.92}, {"text": "warm-up because as you can see this last", "start": 9337.76, "duration": 4.8}, {"text": "GPU is starting the forward backward", "start": 9339.84, "duration": 4.24}, {"text": "forward backward right away. In fact,", "start": 9342.56, "duration": 3.36}, {"text": "you can see that during the entire run", "start": 9344.08, "duration": 3.52}, {"text": "it is in a steady state. So there is no", "start": 9345.92, "duration": 3.92}, {"text": "warm-up or cool down for the last device", "start": 9347.6, "duration": 6.4}, {"text": "because it doesn't have to wait for some", "start": 9349.84, "duration": 6.32}, {"text": "device in front of it to do some forward", "start": 9354.0, "duration": 3.92}, {"text": "passes and then backward. It's the last", "start": 9356.16, "duration": 3.04}, {"text": "one. So as as soon as it does the", "start": 9357.92, "duration": 3.76}, {"text": "forward on that last um on the as as", "start": 9359.2, "duration": 3.6}, {"text": "soon as it does the last forward pass", "start": 9361.68, "duration": 2.799}, {"text": "for this micro batch then it can do the", "start": 9362.8, "duration": 3.04}, {"text": "mic then it can do the backward pass", "start": 9364.479, "duration": 3.201}, {"text": "immediately. Whereas for example worker", "start": 9365.84, "duration": 4.96}, {"text": "two needs to wait for its microbatch to", "start": 9367.68, "duration": 5.2}, {"text": "get sent to worker three it does its", "start": 9370.8, "duration": 3.76}, {"text": "backward pass and then it can do the", "start": 9372.88, "duration": 4.72}, {"text": "backward pass on the same micro batch.", "start": 9374.56, "duration": 6.4}, {"text": "So coming back here this means that for", "start": 9377.6, "duration": 9.28}, {"text": "the last GPU right warm-up equals 4 - 3", "start": 9380.96, "duration": 10.32}, {"text": "- 1 and this is zero just as we expected", "start": 9386.88, "duration": 6.24}, {"text": "and the way you can think about this in", "start": 9391.28, "duration": 4.64}, {"text": "an even more intuitive manner is how", "start": 9393.12, "duration": 5.68}, {"text": "many GPUs are there in front of me this", "start": 9395.92, "duration": 5.12}, {"text": "is what this represents right because if", "start": 9398.8, "duration": 5.519}, {"text": "we're on the last GPU world rank sorry", "start": 9401.04, "duration": 6.399}, {"text": "of rank three then there are zero GPUs", "start": 9404.319, "duration": 5.441}, {"text": "in front of it. But then let's just do", "start": 9407.439, "duration": 7.04}, {"text": "the first GPU which would be 4 -", "start": 9409.76, "duration": 9.28}, {"text": "0 - 1 which is just three. And in this", "start": 9414.479, "duration": 7.361}, {"text": "case if we come back to our diagram we", "start": 9419.04, "duration": 4.64}, {"text": "know that there are three GPUs in front", "start": 9421.84, "duration": 6.559}, {"text": "of the GPU zero. And for that reason, it", "start": 9423.68, "duration": 6.96}, {"text": "needs to do three warm-up steps such", "start": 9428.399, "duration": 6.08}, {"text": "that for every micro batch we send out", "start": 9430.64, "duration": 6.719}, {"text": "afterwards as a forward, there is a", "start": 9434.479, "duration": 5.201}, {"text": "corresponding micro batch coming into", "start": 9437.359, "duration": 5.681}, {"text": "our inbox as a backward because we sent", "start": 9439.68, "duration": 6.08}, {"text": "these three preemptive ones since we had", "start": 9443.04, "duration": 5.2}, {"text": "these three GPUs. Meaning that in this", "start": 9445.76, "duration": 4.4}, {"text": "steady state, we always have a backward", "start": 9448.24, "duration": 5.52}, {"text": "to match with our forward. Okay, so", "start": 9450.16, "duration": 5.76}, {"text": "we've now done these two calculations", "start": 9453.76, "duration": 5.28}, {"text": "which gives us an intuition on why we", "start": 9455.92, "duration": 6.24}, {"text": "need to warm up for however many GPUs", "start": 9459.04, "duration": 5.04}, {"text": "are in front of us. Let's do one more", "start": 9462.16, "duration": 4.48}, {"text": "example and this one will be the la the", "start": 9464.08, "duration": 7.04}, {"text": "second to last GPU. So, it's 4 - 2 - 1 =", "start": 9466.64, "duration": 7.28}, {"text": "1 because we're on rank two. And this", "start": 9471.12, "duration": 5.52}, {"text": "gives us one warm-up step. And in this", "start": 9473.92, "duration": 5.519}, {"text": "case, we only need to send one forward", "start": 9476.64, "duration": 5.2}, {"text": "pass in advance. So that it gets", "start": 9479.439, "duration": 6.88}, {"text": "received by the last GPU, then backward,", "start": 9481.84, "duration": 7.2}, {"text": "and then we can backward it once again.", "start": 9486.319, "duration": 5.681}, {"text": "So that now in this steady state, we", "start": 9489.04, "duration": 5.76}, {"text": "send the second one and then the last", "start": 9492.0, "duration": 5.04}, {"text": "GPU will start the background that. And", "start": 9494.8, "duration": 4.0}, {"text": "then we just do this process over and", "start": 9497.04, "duration": 4.96}, {"text": "over. It just repeats and it continues", "start": 9498.8, "duration": 5.519}, {"text": "until we have the cool down stage. So", "start": 9502.0, "duration": 5.28}, {"text": "this is the intuition on why the worker", "start": 9504.319, "duration": 4.961}, {"text": "number two needs to send one extra", "start": 9507.28, "duration": 4.32}, {"text": "warm-up because it needs to wait one", "start": 9509.28, "duration": 5.84}, {"text": "step for that microbatch to become", "start": 9511.6, "duration": 5.759}, {"text": "forwarded and backwarded by the last", "start": 9515.12, "duration": 4.319}, {"text": "one. Okay, so there you have it. Let's", "start": 9517.359, "duration": 4.08}, {"text": "delete this. So what we're going to do", "start": 9519.439, "duration": 4.081}, {"text": "now is just fill in the warm-up table", "start": 9521.439, "duration": 5.04}, {"text": "which you've already established. So the", "start": 9523.52, "duration": 5.6}, {"text": "GPU 0 does three warm-ups, GPU 1 does", "start": 9526.479, "duration": 4.721}, {"text": "two warm-ups, and GPU 3 does one", "start": 9529.12, "duration": 4.08}, {"text": "warm-up. And as soon as this micro batch", "start": 9531.2, "duration": 4.159}, {"text": "is received and we do the forward pass", "start": 9533.2, "duration": 4.159}, {"text": "on the last GPU, then we'll do the", "start": 9535.359, "duration": 3.841}, {"text": "backward on that same GPU. So in order", "start": 9537.359, "duration": 3.521}, {"text": "to avoid confusion with forward and", "start": 9539.2, "duration": 3.84}, {"text": "backward, I'm just going to make these", "start": 9540.88, "duration": 4.88}, {"text": "cells pink, meaning that's a backward.", "start": 9543.04, "duration": 5.76}, {"text": "And now what you'll notice is that as", "start": 9545.76, "duration": 6.8}, {"text": "soon as this batch has been backwarded,", "start": 9548.8, "duration": 8.24}, {"text": "this can now be done also by GPU of rank", "start": 9552.56, "duration": 7.2}, {"text": "two. And then furthermore, we need to", "start": 9557.04, "duration": 5.04}, {"text": "forward the first microbatch because", "start": 9559.76, "duration": 4.719}, {"text": "it's called one forward, one backward,", "start": 9562.08, "duration": 4.88}, {"text": "not one backward, one forward, which is", "start": 9564.479, "duration": 4.481}, {"text": "to say that the steady state always", "start": 9566.96, "duration": 4.24}, {"text": "begins with a forward and then the", "start": 9568.96, "duration": 3.92}, {"text": "corresponding backward. And then now you", "start": 9571.2, "duration": 3.92}, {"text": "can see this interle beautiful", "start": 9572.88, "duration": 4.32}, {"text": "crosshatch which happened which means", "start": 9575.12, "duration": 5.76}, {"text": "that this one can now be forwarded right", "start": 9577.2, "duration": 4.96}, {"text": "this microbatch can be forwarded by the", "start": 9580.88, "duration": 4.0}, {"text": "last GPU and then it will also be", "start": 9582.16, "duration": 5.92}, {"text": "backwarded by that GPU and then when", "start": 9584.88, "duration": 6.16}, {"text": "that happens we can do the same exact", "start": 9588.08, "duration": 7.68}, {"text": "thing with GPU rank 2. So what's", "start": 9591.04, "duration": 7.359}, {"text": "missing? There's something missing in", "start": 9595.76, "duration": 4.88}, {"text": "this stage and that is notably", "start": 9598.399, "duration": 5.441}, {"text": "microbatch 2 which we now must forward", "start": 9600.64, "duration": 5.679}, {"text": "and then we'll also forward it on the", "start": 9603.84, "duration": 5.04}, {"text": "last GPU. But there's one problem here", "start": 9606.319, "duration": 4.16}, {"text": "and it's the fact that although", "start": 9608.88, "duration": 5.12}, {"text": "microbatch 2 has been forwarded by the", "start": 9610.479, "duration": 6.081}, {"text": "GPU of rank zero, it has not yet been", "start": 9614.0, "duration": 5.28}, {"text": "forwarded by the GPU of rank one. And", "start": 9616.56, "duration": 3.919}, {"text": "this is because we haven't started the", "start": 9619.28, "duration": 5.039}, {"text": "steady state yet for this GPU. But if we", "start": 9620.479, "duration": 5.601}, {"text": "come here, this is exactly where it's", "start": 9624.319, "duration": 3.841}, {"text": "going to start. So we can say that we", "start": 9626.08, "duration": 6.239}, {"text": "are going to now begin with the backward", "start": 9628.16, "duration": 8.8}, {"text": "pass of microbatch zero on GPU of rank", "start": 9632.319, "duration": 8.721}, {"text": "one. And then therefore we start the", "start": 9636.96, "duration": 7.2}, {"text": "micro batch forward pass of microbatch 2", "start": 9641.04, "duration": 5.439}, {"text": "so that it can be propagated forward", "start": 9644.16, "duration": 4.72}, {"text": "across the last two devices. And then", "start": 9646.479, "duration": 6.0}, {"text": "continuing we can now see that in this", "start": 9648.88, "duration": 7.599}, {"text": "square we need to send microbatch three", "start": 9652.479, "duration": 5.681}, {"text": "so that the chain can continue. And", "start": 9656.479, "duration": 4.801}, {"text": "therefore for this microbatch to be sent", "start": 9658.16, "duration": 6.56}, {"text": "by GPU rank one it first needs to happen", "start": 9661.28, "duration": 7.119}, {"text": "on the forward pass of GPU rank zero.", "start": 9664.72, "duration": 7.679}, {"text": "And now finally we can send the micro", "start": 9668.399, "duration": 7.761}, {"text": "batch of zero all the way through the", "start": 9672.399, "duration": 5.601}, {"text": "backward. And this means it has been", "start": 9676.16, "duration": 5.76}, {"text": "completely cleared from our pipeline.", "start": 9678.0, "duration": 6.319}, {"text": "And one thing that this now shows you is", "start": 9681.92, "duration": 4.8}, {"text": "why we only have a peak activation", "start": 9684.319, "duration": 4.721}, {"text": "memory of at most four. So if I just", "start": 9686.72, "duration": 5.44}, {"text": "paste the following here, we'll see that", "start": 9689.04, "duration": 5.76}, {"text": "for more up one, we sent microbatch", "start": 9692.16, "duration": 5.04}, {"text": "zero. For two, we sent microbatch one.", "start": 9694.8, "duration": 4.8}, {"text": "For three, we sent microbatch 2. And we", "start": 9697.2, "duration": 4.56}, {"text": "have nothing happening in the backwards.", "start": 9699.6, "duration": 4.16}, {"text": "And then once you reach the steady state", "start": 9701.76, "duration": 5.92}, {"text": "in GPO rank zero then we send the micro", "start": 9703.76, "duration": 5.92}, {"text": "batch three which means that this time", "start": 9707.68, "duration": 3.36}, {"text": "we have the activations for three", "start": 9709.68, "duration": 2.96}, {"text": "microbatches", "start": 9711.04, "duration": 4.0}, {"text": "uh sorry four microbatches 0 1 2 and", "start": 9712.64, "duration": 4.88}, {"text": "three and before we get the fifth", "start": 9715.04, "duration": 4.56}, {"text": "activation which would be for microbatch", "start": 9717.52, "duration": 3.919}, {"text": "five you can see that we've already", "start": 9719.6, "duration": 4.32}, {"text": "cleared the microbatch zero through the", "start": 9721.439, "duration": 4.721}, {"text": "entire queue meaning that the maximum", "start": 9723.92, "duration": 4.559}, {"text": "activation size is for four microbatches", "start": 9726.16, "duration": 4.239}, {"text": "at this point in time and then in the", "start": 9728.479, "duration": 3.361}, {"text": "next time step when we send the", "start": 9730.399, "duration": 4.801}, {"text": "microbatch forward pass for the number", "start": 9731.84, "duration": 7.2}, {"text": "four microbatch then we only have 1 2 3", "start": 9735.2, "duration": 5.6}, {"text": "and four stored in the activations. So", "start": 9739.04, "duration": 3.12}, {"text": "once again this shows you that the peak", "start": 9740.8, "duration": 4.48}, {"text": "activation memory is only equal to the", "start": 9742.16, "duration": 5.84}, {"text": "number of devices in this case four and", "start": 9745.28, "duration": 4.8}, {"text": "that is quite nice because if we were", "start": 9748.0, "duration": 5.12}, {"text": "using microbatching with G-pipe then", "start": 9750.08, "duration": 4.48}, {"text": "this would be eight if we had eight", "start": 9753.12, "duration": 3.6}, {"text": "micro batches. So just continue on now.", "start": 9754.56, "duration": 3.839}, {"text": "And one thing that you'll notice is that", "start": 9756.72, "duration": 3.52}, {"text": "we can just put this checkered pattern", "start": 9758.399, "duration": 4.08}, {"text": "here for the backward passes already in", "start": 9760.24, "duration": 3.92}, {"text": "advance so that we know where the", "start": 9762.479, "duration": 4.161}, {"text": "backward passes need to be because the", "start": 9764.16, "duration": 4.96}, {"text": "backward passes are happening in an", "start": 9766.64, "duration": 4.4}, {"text": "alternating fashion compared to the", "start": 9769.12, "duration": 4.08}, {"text": "forward passes. So here we can see that", "start": 9771.04, "duration": 7.84}, {"text": "we have to do the backward pass now for", "start": 9773.2, "duration": 9.199}, {"text": "microbatch 2 and then the next one will", "start": 9778.88, "duration": 6.16}, {"text": "be for microbatch three. So we're", "start": 9782.399, "duration": 4.881}, {"text": "sending microbatch 3 here. You can see", "start": 9785.04, "duration": 4.319}, {"text": "it's coming like this. Essentially every", "start": 9787.28, "duration": 4.64}, {"text": "single number, right, it comes down all", "start": 9789.359, "duration": 3.681}, {"text": "the way through the pipeline and then", "start": 9791.92, "duration": 4.64}, {"text": "gets sent back up. So it's quite nice to", "start": 9793.04, "duration": 5.68}, {"text": "draw out for us. And at this point,", "start": 9796.56, "duration": 5.2}, {"text": "we've already done the first three micro", "start": 9798.72, "duration": 5.12}, {"text": "batches completely for microbatch 0,", "start": 9801.76, "duration": 4.639}, {"text": "one, and two. Let's continue on here. We", "start": 9803.84, "duration": 4.16}, {"text": "can see that now we need to send the", "start": 9806.399, "duration": 5.521}, {"text": "backward pass for microbatch three. And", "start": 9808.0, "duration": 7.92}, {"text": "it's just going to continue on and on.", "start": 9811.92, "duration": 6.479}, {"text": "So at this point, we've sent microbatch", "start": 9815.92, "duration": 4.8}, {"text": "4 in the forward pass for the f the", "start": 9818.399, "duration": 5.121}, {"text": "first GPU. Let's continue it on through", "start": 9820.72, "duration": 4.96}, {"text": "the pipeline and then in the", "start": 9823.52, "duration": 5.12}, {"text": "corresponding backward pass like so. But", "start": 9825.68, "duration": 5.679}, {"text": "we should probably put the monikers", "start": 9828.64, "duration": 6.16}, {"text": "first like this. Okay. So this is", "start": 9831.359, "duration": 4.96}, {"text": "already progressing quite well. Let's", "start": 9834.8, "duration": 2.96}, {"text": "look at what we're missing here. So", "start": 9836.319, "duration": 3.201}, {"text": "what's missing at this stage? This is", "start": 9837.76, "duration": 3.679}, {"text": "going to be the start of the forward", "start": 9839.52, "duration": 4.16}, {"text": "pass for microbatch number five. So this", "start": 9841.439, "duration": 4.0}, {"text": "is just going to come like this. And", "start": 9843.68, "duration": 4.799}, {"text": "then now we're going to do the backward", "start": 9845.439, "duration": 6.081}, {"text": "for microbatch number five. So that's", "start": 9848.479, "duration": 4.561}, {"text": "just going to be like so. And we're", "start": 9851.52, "duration": 3.44}, {"text": "going to go up to seven because in our", "start": 9853.04, "duration": 3.439}, {"text": "real life implementation we're going to", "start": 9854.96, "duration": 4.0}, {"text": "do four devices and eight microbatches.", "start": 9856.479, "duration": 4.641}, {"text": "Okay. So where's the latest thing that", "start": 9858.96, "duration": 3.84}, {"text": "we have yet to fill? Okay. What's going", "start": 9861.12, "duration": 4.88}, {"text": "on here? We need to end off our backward", "start": 9862.8, "duration": 5.76}, {"text": "pass for microbatch 3 on GPU rank one", "start": 9866.0, "duration": 5.12}, {"text": "and GPU rank zero. And before we do", "start": 9868.56, "duration": 4.96}, {"text": "that, I should oh call this three and", "start": 9871.12, "duration": 5.359}, {"text": "call this three. Okay. And now the next", "start": 9873.52, "duration": 5.36}, {"text": "thing is to start microbatch six. So", "start": 9876.479, "duration": 4.721}, {"text": "let's start microbatch 6 all the way", "start": 9878.88, "duration": 4.64}, {"text": "through the four GPUs. And now we need", "start": 9881.2, "duration": 5.52}, {"text": "to stop doing the forward pass and start", "start": 9883.52, "duration": 5.2}, {"text": "sending the backward pass for the same", "start": 9886.72, "duration": 4.639}, {"text": "microbatch. So there you go. You can see", "start": 9888.72, "duration": 4.719}, {"text": "in this notation microbatch 6 has been", "start": 9891.359, "duration": 3.681}, {"text": "passed through the entire forward and", "start": 9893.439, "duration": 4.241}, {"text": "backward. Okay. Now going on what is", "start": 9895.04, "duration": 4.08}, {"text": "missing here? This is going to be the", "start": 9897.68, "duration": 4.4}, {"text": "last microbatch. Microbatch 7. So this", "start": 9899.12, "duration": 5.12}, {"text": "is the end of the batches that we're", "start": 9902.08, "duration": 3.76}, {"text": "going to send to the pipeline. This", "start": 9904.24, "duration": 5.52}, {"text": "means that at this stage once we send", "start": 9905.84, "duration": 7.92}, {"text": "microbatch 7 this GPU is now done", "start": 9909.76, "duration": 6.0}, {"text": "because it has done all of the forward", "start": 9913.76, "duration": 4.559}, {"text": "and all of the backward passes necessary", "start": 9915.76, "duration": 5.44}, {"text": "to complete one training step. And if we", "start": 9918.319, "duration": 4.96}, {"text": "just compare this quickly to the diagram", "start": 9921.2, "duration": 4.64}, {"text": "that we saw at the beginning, it's the", "start": 9923.279, "duration": 3.841}, {"text": "same thing except the indexing here", "start": 9925.84, "duration": 2.8}, {"text": "starts from one. But you can see it", "start": 9927.12, "duration": 2.88}, {"text": "starts with a forward and it ends with a", "start": 9928.64, "duration": 3.679}, {"text": "backward just as is the case here. It", "start": 9930.0, "duration": 3.2}, {"text": "starts with the forward and then", "start": 9932.319, "duration": 2.561}, {"text": "alternates ending with the backward. But", "start": 9933.2, "duration": 3.04}, {"text": "let's finish it off for the rest of the", "start": 9934.88, "duration": 2.96}, {"text": "GPUs and then we'll also be able to", "start": 9936.24, "duration": 4.96}, {"text": "motivate why some GPUs have a cool down", "start": 9937.84, "duration": 5.84}, {"text": "and the relationship of this cool down", "start": 9941.2, "duration": 4.56}, {"text": "to the warm-up. So before we do that,", "start": 9943.68, "duration": 3.36}, {"text": "let's just make sure that we finish this", "start": 9945.76, "duration": 4.8}, {"text": "training properly. So let's see what the", "start": 9947.04, "duration": 6.64}, {"text": "status is on all of the backward passes.", "start": 9950.56, "duration": 5.68}, {"text": "I think at this point we're finished. So", "start": 9953.68, "duration": 5.84}, {"text": "we've passed all of the um corresponding", "start": 9956.24, "duration": 5.84}, {"text": "microbatches in the forward up to up to", "start": 9959.52, "duration": 5.2}, {"text": "microbatch 7. And then we've also done", "start": 9962.08, "duration": 3.92}, {"text": "all of the backward passes up to", "start": 9964.72, "duration": 3.92}, {"text": "Microbat 7. But what you'll notice is", "start": 9966.0, "duration": 5.92}, {"text": "that for GPU 0, which has three warm-up", "start": 9968.64, "duration": 5.28}, {"text": "stages, because it's world size minus", "start": 9971.92, "duration": 5.519}, {"text": "the rank minus 1, which is 4 - 0 - 1,", "start": 9973.92, "duration": 6.399}, {"text": "three, it has the same number of cool", "start": 9977.439, "duration": 4.88}, {"text": "down backward passes. And the way that", "start": 9980.319, "duration": 4.0}, {"text": "you can think of this is the fact that", "start": 9982.319, "duration": 4.801}, {"text": "because we sent three microbatches in", "start": 9984.319, "duration": 5.281}, {"text": "advance, then in our steady state, we're", "start": 9987.12, "duration": 6.319}, {"text": "only going to be able to do eight minus", "start": 9989.6, "duration": 6.32}, {"text": "three steady state, one forward, one", "start": 9993.439, "duration": 4.88}, {"text": "backwards because we have this debt here", "start": 9995.92, "duration": 4.72}, {"text": "of three forward passes that we need to", "start": 9998.319, "duration": 4.881}, {"text": "pay back in the cool down stage with the", "start": 10000.64, "duration": 4.799}, {"text": "three backward passes of the last three", "start": 10003.2, "duration": 4.079}, {"text": "microbatches. So we have three", "start": 10005.439, "duration": 4.0}, {"text": "microbatches that are sent forward, the", "start": 10007.279, "duration": 4.401}, {"text": "first three in advance and three", "start": 10009.439, "duration": 4.96}, {"text": "microbatches that are backwarded without", "start": 10011.68, "duration": 4.32}, {"text": "being in the steady state. So of course", "start": 10014.399, "duration": 3.841}, {"text": "this is not ideal but because we have to", "start": 10016.0, "duration": 3.92}, {"text": "send these first three microbatches at", "start": 10018.24, "duration": 3.52}, {"text": "the beginning in order to ensure that", "start": 10019.92, "duration": 5.12}, {"text": "the steady state occurs without any", "start": 10021.76, "duration": 5.12}, {"text": "stalling then we also need to perform", "start": 10025.04, "duration": 3.76}, {"text": "these last three backward passes outside", "start": 10026.88, "duration": 4.399}, {"text": "of the steady state. And then", "start": 10028.8, "duration": 6.639}, {"text": "analogously for GPU rank one we have two", "start": 10031.279, "duration": 6.481}, {"text": "cool down periods. And then for GPU rank", "start": 10035.439, "duration": 4.721}, {"text": "two we just have one. And this is the", "start": 10037.76, "duration": 4.8}, {"text": "exact same number of warm-up stages that", "start": 10040.16, "duration": 4.4}, {"text": "we have. And the last thing that I want", "start": 10042.56, "duration": 4.0}, {"text": "to mention here is because in our", "start": 10044.56, "duration": 5.04}, {"text": "formula for the implementation for 141b", "start": 10046.56, "duration": 4.08}, {"text": "we're going to have three distinct", "start": 10049.6, "duration": 3.2}, {"text": "stages. So we're going to first have one", "start": 10050.64, "duration": 3.679}, {"text": "for loop which is only the warm-up", "start": 10052.8, "duration": 3.599}, {"text": "phase. And then we'll have one for loop", "start": 10054.319, "duration": 3.601}, {"text": "which is the steady state phase and one", "start": 10056.399, "duration": 4.0}, {"text": "form up one warm-up which is the coolown", "start": 10057.92, "duration": 5.04}, {"text": "phase. What you'll notice is that for", "start": 10060.399, "duration": 5.361}, {"text": "the last GPU there will be no warm-up", "start": 10062.96, "duration": 5.68}, {"text": "and cool down. So it'll just be a null", "start": 10065.76, "duration": 4.639}, {"text": "for loop for those two and then it will", "start": 10068.64, "duration": 3.92}, {"text": "just happen all in the steady state. But", "start": 10070.399, "duration": 4.241}, {"text": "for the other GPUs they will have both a", "start": 10072.56, "duration": 4.719}, {"text": "warmup and a cool down. And I want to", "start": 10074.64, "duration": 4.96}, {"text": "also talk about quickly the relationship", "start": 10077.279, "duration": 4.241}, {"text": "between the index of the microbatch in", "start": 10079.6, "duration": 3.759}, {"text": "the forward pass and the index of the", "start": 10081.52, "duration": 4.4}, {"text": "microbatch in the backward pass during", "start": 10083.359, "duration": 4.401}, {"text": "the steady state. So here you can see", "start": 10085.92, "duration": 4.32}, {"text": "that we're forwarding microbatch number", "start": 10087.76, "duration": 4.4}, {"text": "three and we're backwarding microbatch", "start": 10090.24, "duration": 4.079}, {"text": "zero. And in this case there's a", "start": 10092.16, "duration": 4.08}, {"text": "difference of three. Here we're", "start": 10094.319, "duration": 3.921}, {"text": "forwarding microbatch two and then we're", "start": 10096.24, "duration": 4.159}, {"text": "backwarding microbatch zero. Here we're", "start": 10098.24, "duration": 3.92}, {"text": "forwarding microbatch one microbatch", "start": 10100.399, "duration": 3.361}, {"text": "zero. And here we're forwarding micro", "start": 10102.16, "duration": 3.44}, {"text": "back zero and also backwarding micro", "start": 10103.76, "duration": 3.84}, {"text": "back zero because once again the last", "start": 10105.6, "duration": 3.92}, {"text": "GPU does not need to wait for any", "start": 10107.6, "duration": 3.679}, {"text": "downstream GPUs to do their forward", "start": 10109.52, "duration": 4.32}, {"text": "passes because it is the last GPU. So", "start": 10111.279, "duration": 4.881}, {"text": "the pattern here is that starting at an", "start": 10113.84, "duration": 5.36}, {"text": "index of zero. The number of the", "start": 10116.16, "duration": 4.96}, {"text": "microbatch that you must forward pass is", "start": 10119.2, "duration": 4.64}, {"text": "equal to the index plus the number of", "start": 10121.12, "duration": 4.88}, {"text": "warm-up stages that that GP has to do.", "start": 10123.84, "duration": 4.96}, {"text": "So in this case, we have index zero for", "start": 10126.0, "duration": 4.8}, {"text": "the backward pass and that's the same", "start": 10128.8, "duration": 4.88}, {"text": "for all four GPUs because it's doing it", "start": 10130.8, "duration": 4.96}, {"text": "in this staircase sequential manner. But", "start": 10133.68, "duration": 3.759}, {"text": "because we have this warm-up stage, we", "start": 10135.76, "duration": 6.08}, {"text": "need to do the index 0 + 3 and forward", "start": 10137.439, "duration": 7.92}, {"text": "that microbatch for the GPU zero. And", "start": 10141.84, "duration": 5.04}, {"text": "then in this case, because we had two", "start": 10145.359, "duration": 3.361}, {"text": "warm-up stages here, we are still doing", "start": 10146.88, "duration": 3.599}, {"text": "the microbatch on index zero, but now", "start": 10148.72, "duration": 4.08}, {"text": "we're doing the microbatch forward pass", "start": 10150.479, "duration": 5.521}, {"text": "on index 0 plus two because it's number", "start": 10152.8, "duration": 4.8}, {"text": "of warm-up stages we have here. And then", "start": 10156.0, "duration": 5.2}, {"text": "here it's 0 + one giving us the", "start": 10157.6, "duration": 5.6}, {"text": "microbatch one where we need to do the", "start": 10161.2, "duration": 3.52}, {"text": "forward pass in this one forward one", "start": 10163.2, "duration": 3.36}, {"text": "backward. And because there's no warm-up", "start": 10164.72, "duration": 4.8}, {"text": "here, then we just also forward the same", "start": 10166.56, "duration": 4.719}, {"text": "micro batch that we backward in the next", "start": 10169.52, "duration": 5.759}, {"text": "stage. So if I zoom out here", "start": 10171.279, "duration": 7.04}, {"text": "then we can see that we have essentially", "start": 10175.279, "duration": 8.241}, {"text": "derived by ourselves the 1 F1B algorithm", "start": 10178.319, "duration": 8.0}, {"text": "just as it's done in this case. And this", "start": 10183.52, "duration": 4.959}, {"text": "just continues on. And one thing worth", "start": 10186.319, "duration": 4.241}, {"text": "noting of course is that at the end of", "start": 10188.479, "duration": 4.561}, {"text": "every single pipeline we need to do the", "start": 10190.56, "duration": 5.52}, {"text": "optimizer step to update all of our", "start": 10193.04, "duration": 4.64}, {"text": "model weights. So this will give you", "start": 10196.08, "duration": 3.84}, {"text": "some good intuition to how we're going", "start": 10197.68, "duration": 5.2}, {"text": "to implement the 1 F1B in code and let's", "start": 10199.92, "duration": 5.359}, {"text": "get that to that right now. Okay. So", "start": 10202.88, "duration": 4.72}, {"text": "let's jump right into the code here. 1", "start": 10205.279, "duration": 5.601}, {"text": "F1B pipeline step. Reviewing this pseudo", "start": 10207.6, "duration": 6.0}, {"text": "code implementation guide. We first just", "start": 10210.88, "duration": 5.12}, {"text": "as before with Gpipe need to chunk the", "start": 10213.6, "duration": 3.839}, {"text": "batches into micro batches and the", "start": 10216.0, "duration": 2.96}, {"text": "targets into microtargets since we're", "start": 10217.439, "duration": 3.521}, {"text": "still dealing with these micro batches.", "start": 10218.96, "duration": 3.359}, {"text": "initialize the buffers for the", "start": 10220.96, "duration": 4.479}, {"text": "activations which are the outputs of the", "start": 10222.319, "duration": 5.441}, {"text": "forward pass and the gradients which are", "start": 10225.439, "duration": 4.801}, {"text": "the thing which we pass along in our", "start": 10227.76, "duration": 5.28}, {"text": "backward pass. And then we have our", "start": 10230.24, "duration": 6.88}, {"text": "forward warm-up during which we will do", "start": 10233.04, "duration": 6.88}, {"text": "for the number of normup steps just the", "start": 10237.12, "duration": 6.72}, {"text": "forward pass on those specific GPUs. And", "start": 10239.92, "duration": 6.72}, {"text": "here we use the same formula as before.", "start": 10243.84, "duration": 6.559}, {"text": "If we are using GPU zero, then we use", "start": 10246.64, "duration": 5.44}, {"text": "the microbatch directly. Else we receive", "start": 10250.399, "duration": 3.92}, {"text": "the input from the previous GPU. Then we", "start": 10252.08, "duration": 4.0}, {"text": "forward that microbatch to the model. If", "start": 10254.319, "duration": 3.201}, {"text": "we're not the last stage, then we send", "start": 10256.08, "duration": 3.04}, {"text": "output to the next stage. And then we", "start": 10257.52, "duration": 4.24}, {"text": "append the input or output to the", "start": 10259.12, "duration": 4.159}, {"text": "buffers.", "start": 10261.76, "duration": 3.92}, {"text": "And then during the steady state, we do", "start": 10263.279, "duration": 5.04}, {"text": "a forward pass as per above and a", "start": 10265.68, "duration": 4.4}, {"text": "backward pass where if we are the last", "start": 10268.319, "duration": 3.681}, {"text": "stage, we commit the loss and call", "start": 10270.08, "duration": 3.68}, {"text": "backward to start the backward", "start": 10272.0, "duration": 3.76}, {"text": "propagation chain. Otherwise, we receive", "start": 10273.76, "duration": 3.519}, {"text": "the gradient from the next stage, which", "start": 10275.76, "duration": 3.599}, {"text": "is the GPU that's upstream, and call", "start": 10277.279, "duration": 4.241}, {"text": "backward, and then send the gradient to", "start": 10279.359, "duration": 3.361}, {"text": "the previous stage if we're not the", "start": 10281.52, "duration": 2.799}, {"text": "first stage. And then during the", "start": 10282.72, "duration": 4.08}, {"text": "backward drain, we do for the number of", "start": 10284.319, "duration": 3.761}, {"text": "drain steps, which is the same thing as", "start": 10286.8, "duration": 2.8}, {"text": "the number of warm-up steps, the", "start": 10288.08, "duration": 4.56}, {"text": "remaining backward passes. And finally,", "start": 10289.6, "duration": 5.44}, {"text": "we return loss if we're the last GPU,", "start": 10292.64, "duration": 5.2}, {"text": "else we return nothing. So I would like", "start": 10295.04, "duration": 6.88}, {"text": "to first say instead of implementing the", "start": 10297.84, "duration": 6.559}, {"text": "forward pass here and then here because", "start": 10301.92, "duration": 4.16}, {"text": "as you can see we have to do the same", "start": 10304.399, "duration": 3.201}, {"text": "forward pass here and the same forward", "start": 10306.08, "duration": 3.84}, {"text": "pass here and then also instead of doing", "start": 10307.6, "duration": 4.96}, {"text": "the backward pass here and here to be", "start": 10309.92, "duration": 6.72}, {"text": "redundant we can instead make a function", "start": 10312.56, "duration": 6.32}, {"text": "which performs the forward pass and then", "start": 10316.64, "duration": 5.12}, {"text": "simply call it in these higher level", "start": 10318.88, "duration": 4.399}, {"text": "functions. So that's what I'm going to", "start": 10321.76, "duration": 4.719}, {"text": "start off with. In fact, what I'm going", "start": 10323.279, "duration": 6.241}, {"text": "to do is establish that we first have", "start": 10326.479, "duration": 5.281}, {"text": "the number of warm-up steps. Um, let's", "start": 10329.52, "duration": 4.64}, {"text": "just call it warm-up to be simple. And", "start": 10331.76, "duration": 6.519}, {"text": "this is equal to coms.orld", "start": 10334.16, "duration": 4.119}, {"text": "size minus coms.rank", "start": 10339.04, "duration": 5.84}, {"text": "minus one. And I apologize, I realize", "start": 10342.399, "duration": 6.241}, {"text": "now that I've been using model.trank", "start": 10344.88, "duration": 5.84}, {"text": "and model.orld size interchangeably.", "start": 10348.64, "duration": 4.48}, {"text": "That's not really good code practice to", "start": 10350.72, "duration": 3.92}, {"text": "have two different classes which have", "start": 10353.12, "duration": 3.92}, {"text": "the same elements that we refer to. So", "start": 10354.64, "duration": 4.24}, {"text": "for the rest of the series and for the", "start": 10357.04, "duration": 3.279}, {"text": "rest of this implementation, I'll just", "start": 10358.88, "duration": 3.28}, {"text": "be using communications. But if you've", "start": 10360.319, "duration": 4.0}, {"text": "been using the attribute in the model", "start": 10362.16, "duration": 3.36}, {"text": "class, then that's fine. It doesn't", "start": 10364.319, "duration": 4.16}, {"text": "really make a difference to be clear.", "start": 10365.52, "duration": 7.759}, {"text": "And then we'll do 1 f_1B", "start": 10368.479, "duration": 6.481}, {"text": "which is the number of steady state", "start": 10373.279, "duration": 3.441}, {"text": "passes. And this is simply the number of", "start": 10374.96, "duration": 4.0}, {"text": "chunks which in this case will be eight", "start": 10376.72, "duration": 4.48}, {"text": "minus the number of warm-up. And for", "start": 10378.96, "duration": 5.68}, {"text": "example, if we are on GPU", "start": 10381.2, "duration": 7.279}, {"text": "zero, then in this case we have three", "start": 10384.64, "duration": 8.16}, {"text": "warm-ups at least and then we have five", "start": 10388.479, "duration": 7.361}, {"text": "steady states because 8 minus 3 equals", "start": 10392.8, "duration": 7.599}, {"text": "5. Okay. Now moving on, we've calculated", "start": 10395.84, "duration": 6.8}, {"text": "these two values and now we're not going", "start": 10400.399, "duration": 6.481}, {"text": "to use them to say for I in range", "start": 10402.64, "duration": 5.92}, {"text": "warm-up.", "start": 10406.88, "duration": 3.36}, {"text": "So this will be the warm-up. Then we're", "start": 10408.56, "duration": 5.6}, {"text": "going to do forward of I and we haven't", "start": 10410.24, "duration": 5.52}, {"text": "implemented for it yet. So it's", "start": 10414.16, "duration": 3.36}, {"text": "technically incorrect, but we'll just", "start": 10415.76, "duration": 5.76}, {"text": "keep on going. For I in range", "start": 10417.52, "duration": 6.56}, {"text": "1 F1B,", "start": 10421.52, "duration": 4.32}, {"text": "what are we going to do? We're going to", "start": 10424.08, "duration": 6.08}, {"text": "do a forward of I and then a backward of", "start": 10425.84, "duration": 5.92}, {"text": "I.", "start": 10430.16, "duration": 4.48}, {"text": "And we need to capture the value for the", "start": 10431.76, "duration": 5.2}, {"text": "backward. And I'll explain why this is", "start": 10434.64, "duration": 5.28}, {"text": "the case once we get to that point. But", "start": 10436.96, "duration": 5.519}, {"text": "we'll just have that for now as result.", "start": 10439.92, "duration": 4.64}, {"text": "Okay. And just as a reminder, we have", "start": 10442.479, "duration": 4.561}, {"text": "forward and then backward. Because in", "start": 10444.56, "duration": 4.08}, {"text": "the steady state, we do a forward pass", "start": 10447.04, "duration": 4.319}, {"text": "and then a backward pass. Moving on. Now", "start": 10448.64, "duration": 6.08}, {"text": "let's tackle the last part of the one", "start": 10451.359, "duration": 5.441}, {"text": "forward one backward pipeline", "start": 10454.72, "duration": 4.559}, {"text": "parallelism algorithm which is simply", "start": 10456.8, "duration": 6.32}, {"text": "the backward cool down or the drain. And", "start": 10459.279, "duration": 6.641}, {"text": "this time we are going to be doing this", "start": 10463.12, "duration": 5.92}, {"text": "the same amount warm-up times because we", "start": 10465.92, "duration": 4.32}, {"text": "do the same amount of cool down as", "start": 10469.04, "duration": 2.16}, {"text": "warm-up. We're just going to call it", "start": 10470.24, "duration": 2.8}, {"text": "warm-up because there's no point of", "start": 10471.2, "duration": 3.279}, {"text": "defining a second variable which is the", "start": 10473.04, "duration": 2.96}, {"text": "same as warm-up since they have the same", "start": 10474.479, "duration": 3.521}, {"text": "value. And then we're going to call", "start": 10476.0, "duration": 4.16}, {"text": "backward. And we also need to store the", "start": 10478.0, "duration": 5.04}, {"text": "result. And then this will be done not", "start": 10480.16, "duration": 5.6}, {"text": "just on i because this would be too", "start": 10483.04, "duration": 4.56}, {"text": "early of an index at the very end of the", "start": 10485.76, "duration": 4.16}, {"text": "drain. We're on the later indexes. So", "start": 10487.6, "duration": 6.24}, {"text": "it'll be I + 1 F_1B", "start": 10489.92, "duration": 7.359}, {"text": "such that if we take again GPU0 as the", "start": 10493.84, "duration": 6.8}, {"text": "example for the first backward that it", "start": 10497.279, "duration": 6.961}, {"text": "has to do which in this case in its uh", "start": 10500.64, "duration": 7.679}, {"text": "drain is the microbatch 6 but as you can", "start": 10504.24, "duration": 6.079}, {"text": "see once again it's really misleading", "start": 10508.319, "duration": 3.601}, {"text": "here because we should really be calling", "start": 10510.319, "duration": 3.441}, {"text": "this microbatch 5 since we're ending", "start": 10511.92, "duration": 4.16}, {"text": "second zero. So this is microbatch 5 and", "start": 10513.76, "duration": 6.08}, {"text": "if we come here for the GPU zero the", "start": 10516.08, "duration": 6.16}, {"text": "warm-up uh sorry the 1 F1B is five right", "start": 10519.84, "duration": 4.32}, {"text": "because we established 8 - 3 is five and", "start": 10522.24, "duration": 4.159}, {"text": "then it'll be 0 + 5 meaning that it will", "start": 10524.16, "duration": 5.44}, {"text": "backward the correct GPU and then it", "start": 10526.399, "duration": 4.96}, {"text": "will do another one which would be six", "start": 10529.6, "duration": 2.879}, {"text": "and then the last one which will be", "start": 10531.359, "duration": 3.761}, {"text": "seven and since we're just doing this", "start": 10532.479, "duration": 4.321}, {"text": "three times we only do five six and", "start": 10535.12, "duration": 4.159}, {"text": "seven and then this is all we do and", "start": 10536.8, "duration": 4.72}, {"text": "let's just also add the logic here", "start": 10539.279, "duration": 5.361}, {"text": "return loss if last stage else none So", "start": 10541.52, "duration": 10.4}, {"text": "um if coms do rank equals coms do world", "start": 10544.64, "duration": 9.839}, {"text": "size minus one.", "start": 10551.92, "duration": 4.0}, {"text": "Actually I realize we haven't defined", "start": 10554.479, "duration": 3.681}, {"text": "the loss. So let's do that right here.", "start": 10555.92, "duration": 3.84}, {"text": "And we need to once again do gradient", "start": 10558.16, "duration": 2.64}, {"text": "accumulation because we're doing this", "start": 10559.76, "duration": 3.92}, {"text": "over microbatches. So we'll say total", "start": 10560.8, "duration": 8.88}, {"text": "loss equals torch.zer zeros of one and", "start": 10563.68, "duration": 9.44}, {"text": "device equals device in case we are", "start": 10569.68, "duration": 7.36}, {"text": "moving this onto the GPU and I realized", "start": 10573.12, "duration": 7.199}, {"text": "that in our G-pipe implementation. Okay,", "start": 10577.04, "duration": 6.239}, {"text": "I've erased it since but if we come just", "start": 10580.319, "duration": 6.721}, {"text": "back to the schedule.py I can quickly", "start": 10583.279, "duration": 6.0}, {"text": "show you guys the G-pipe implementation", "start": 10587.04, "duration": 4.08}, {"text": "where we instead initialize the total", "start": 10589.279, "duration": 3.2}, {"text": "losses to that series of the output", "start": 10591.12, "duration": 3.199}, {"text": "shape. This is slightly misleading", "start": 10592.479, "duration": 3.041}, {"text": "because we don't know what the output", "start": 10594.319, "duration": 3.601}, {"text": "shape is. But it's worth noting that", "start": 10595.52, "duration": 3.6}, {"text": "since we're only calling this on the", "start": 10597.92, "duration": 3.439}, {"text": "last GPU, the output shape of the last", "start": 10599.12, "duration": 5.279}, {"text": "GPU is always going to be if we come to", "start": 10601.359, "duration": 5.201}, {"text": "the model, it's always going to be the", "start": 10604.399, "duration": 3.841}, {"text": "return value of this loss function,", "start": 10606.56, "duration": 3.36}, {"text": "which is cross entropy loss, which is a", "start": 10608.24, "duration": 4.48}, {"text": "scalar value. So all this is to say that", "start": 10609.92, "duration": 5.92}, {"text": "it's slightly more readable in the code", "start": 10612.72, "duration": 4.32}, {"text": "to just put this one here because we", "start": 10615.84, "duration": 2.8}, {"text": "know it's a scale loss already. But if", "start": 10617.04, "duration": 3.68}, {"text": "you do outputshape, that's also not", "start": 10618.64, "duration": 6.08}, {"text": "incorrect because we are checking that", "start": 10620.72, "duration": 8.08}, {"text": "this is the last GPU here. If we aren't", "start": 10624.72, "duration": 5.759}, {"text": "checking that this is the last GPU, then", "start": 10628.8, "duration": 2.559}, {"text": "at that point you'll have an", "start": 10630.479, "duration": 6.321}, {"text": "output.shape of 128, sorry, of 32x 128", "start": 10631.359, "duration": 8.481}, {"text": "um or in this case of 4x8x 128. And", "start": 10636.8, "duration": 4.4}, {"text": "where are these numbers coming from?", "start": 10639.84, "duration": 3.04}, {"text": "Once again, if we come into our main", "start": 10641.2, "duration": 4.8}, {"text": "function, the batch size is 32, but we", "start": 10642.88, "duration": 4.24}, {"text": "split up into four. So it's going to be", "start": 10646.0, "duration": 4.24}, {"text": "4 by 8 and then by 128. Um so that would", "start": 10647.12, "duration": 6.64}, {"text": "be incorrect. But one more time just to", "start": 10650.24, "duration": 6.64}, {"text": "reiterate since we are so I'm switching", "start": 10653.76, "duration": 5.28}, {"text": "in between these files way too fast.", "start": 10656.88, "duration": 4.32}, {"text": "Since we are checking in the G-pipe", "start": 10659.04, "duration": 4.96}, {"text": "algorithm that we are on the last GPU", "start": 10661.2, "duration": 4.8}, {"text": "then it doesn't make a difference. And", "start": 10664.0, "duration": 5.359}, {"text": "one last inaccuracy that I have not", "start": 10666.0, "duration": 5.439}, {"text": "corrected since our GPU implementation,", "start": 10669.359, "duration": 5.361}, {"text": "but I've now changed is we previously", "start": 10671.439, "duration": 6.481}, {"text": "were not dividing the loss by the number", "start": 10674.72, "duration": 5.36}, {"text": "of chunks or the number of microbatches", "start": 10677.92, "duration": 4.32}, {"text": "that we were accumulating over. So that", "start": 10680.08, "duration": 5.52}, {"text": "meant that our loss was actually scaled", "start": 10682.24, "duration": 5.44}, {"text": "by four and all of our gradients would", "start": 10685.6, "duration": 4.0}, {"text": "be accumulated by four since we had four", "start": 10687.68, "duration": 4.16}, {"text": "microbatches. meaning that the effective", "start": 10689.6, "duration": 4.24}, {"text": "learning rate of our model was four", "start": 10691.84, "duration": 4.96}, {"text": "times that of the intended learning", "start": 10693.84, "duration": 5.439}, {"text": "rate. So as you can see here the rate is", "start": 10696.8, "duration": 4.24}, {"text": "0.001", "start": 10699.279, "duration": 2.721}, {"text": "but in this case it would have been", "start": 10701.04, "duration": 2.72}, {"text": "0.004", "start": 10702.0, "duration": 5.2}, {"text": "and why is that? It's because instead of", "start": 10703.76, "duration": 6.32}, {"text": "dividing by four which was the number of", "start": 10707.2, "duration": 5.119}, {"text": "microbatches in our training loop I was", "start": 10710.08, "duration": 4.56}, {"text": "actually just summing up the log the", "start": 10712.319, "duration": 4.881}, {"text": "loss and I was only dividing it by four", "start": 10714.64, "duration": 4.88}, {"text": "in the logging step. So if you guys", "start": 10717.2, "duration": 4.96}, {"text": "remember if I just come and open up step", "start": 10719.52, "duration": 4.959}, {"text": "five main what we were doing previously", "start": 10722.16, "duration": 6.159}, {"text": "was loss do item divided by chunks which", "start": 10724.479, "duration": 5.441}, {"text": "would be dividing the loss by four and", "start": 10728.319, "duration": 3.12}, {"text": "this would correctly show what the loss", "start": 10729.92, "duration": 5.359}, {"text": "is but this is not correctly dividing", "start": 10731.439, "duration": 5.761}, {"text": "the loss and the corresponding gradients", "start": 10735.279, "duration": 3.361}, {"text": "in the training step itself. So although", "start": 10737.2, "duration": 3.44}, {"text": "we were showing the correct loss here in", "start": 10738.64, "duration": 4.4}, {"text": "the actual back propagation mechanics we", "start": 10740.64, "duration": 5.04}, {"text": "were effectively using a gradient that", "start": 10743.04, "duration": 3.84}, {"text": "was four times larger than it should", "start": 10745.68, "duration": 3.04}, {"text": "have been. So I just want to clear that", "start": 10746.88, "duration": 5.519}, {"text": "up now. But it's not to worry because", "start": 10748.72, "duration": 6.24}, {"text": "for this implementation and for Gpipe", "start": 10752.399, "duration": 4.481}, {"text": "that's going to be in the course repo.", "start": 10754.96, "duration": 4.16}, {"text": "I've added that correction. So I wanted", "start": 10756.88, "duration": 5.04}, {"text": "to quickly comment on that and we were", "start": 10759.12, "duration": 4.72}, {"text": "just on the total loss because we wanted", "start": 10761.92, "duration": 4.399}, {"text": "to return it. So what we're going to do", "start": 10763.84, "duration": 6.88}, {"text": "is just return total loss. Nothing too", "start": 10766.319, "duration": 7.761}, {"text": "complicated. And now we have the basic", "start": 10770.72, "duration": 5.759}, {"text": "skeleton of how the one forward one", "start": 10774.08, "duration": 5.76}, {"text": "backward algorithm will go. And the only", "start": 10776.479, "duration": 4.96}, {"text": "thing remaining now is to implement both", "start": 10779.84, "duration": 5.439}, {"text": "backward and forward. And the fact is", "start": 10781.439, "duration": 5.281}, {"text": "that we've already done these already.", "start": 10785.279, "duration": 2.961}, {"text": "So it's going to be very simple. But", "start": 10786.72, "duration": 4.48}, {"text": "let's first address the res. And this is", "start": 10788.24, "duration": 5.119}, {"text": "just because in the backward pass we", "start": 10791.2, "duration": 6.96}, {"text": "either return the loss or the gradients.", "start": 10793.359, "duration": 6.641}, {"text": "In fact, in the backward pass, if we're", "start": 10798.16, "duration": 4.8}, {"text": "on the lost GPU, we return the loss,", "start": 10800.0, "duration": 5.04}, {"text": "which is the cross entry loss. If we're", "start": 10802.96, "duration": 3.92}, {"text": "not on the last GPU, we return nothing.", "start": 10805.04, "duration": 3.52}, {"text": "We just back propagate the gradients to", "start": 10806.88, "duration": 4.24}, {"text": "the previous GPU. So, all we want to do", "start": 10808.56, "duration": 6.56}, {"text": "with this re result is to say if we are", "start": 10811.12, "duration": 8.0}, {"text": "on the last GPU, then we want to add", "start": 10815.12, "duration": 7.6}, {"text": "this to our total loss to accumulate it.", "start": 10819.12, "duration": 5.84}, {"text": "And then if not, the backward function", "start": 10822.72, "duration": 5.04}, {"text": "returns none as the default value. if", "start": 10824.96, "duration": 4.399}, {"text": "we're not on the last GPU. So in that", "start": 10827.76, "duration": 4.8}, {"text": "case, we do nothing. So plus equals the", "start": 10829.359, "duration": 6.721}, {"text": "result and then the same thing goes", "start": 10832.56, "duration": 6.08}, {"text": "here. We're going to do if coms.rank", "start": 10836.08, "duration": 5.12}, {"text": "equals equals comroll size one add to", "start": 10838.64, "duration": 4.56}, {"text": "the loss. But this is actually a trick", "start": 10841.2, "duration": 4.72}, {"text": "question here because in the cool down", "start": 10843.2, "duration": 4.8}, {"text": "if we remember we never actually do", "start": 10845.92, "duration": 4.0}, {"text": "anything on the last GPU because it's", "start": 10848.0, "duration": 3.439}, {"text": "already done everything in the steady", "start": 10849.92, "duration": 3.92}, {"text": "state. So for that reason, all of the", "start": 10851.439, "duration": 4.401}, {"text": "backward passes where we do add and", "start": 10853.84, "duration": 3.84}, {"text": "accumulate the loss over the last GPU", "start": 10855.84, "duration": 3.599}, {"text": "only happen in the steady state. Once", "start": 10857.68, "duration": 3.52}, {"text": "again, there's no cool down for the last", "start": 10859.439, "duration": 3.601}, {"text": "GPU. So there's no point of checking it", "start": 10861.2, "duration": 4.32}, {"text": "in the cool down phase. So we only want", "start": 10863.04, "duration": 4.64}, {"text": "to check it here. And in fact, for that", "start": 10865.52, "duration": 5.04}, {"text": "reason, we can remove this. Okay. So", "start": 10867.68, "duration": 6.4}, {"text": "let's now define forward and backward.", "start": 10870.56, "duration": 8.96}, {"text": "And it it it uh receives a index I as", "start": 10874.08, "duration": 7.6}, {"text": "its input. And we'll just call this", "start": 10879.52, "duration": 4.08}, {"text": "microcorebatch_index", "start": 10881.68, "duration": 5.679}, {"text": "so we don't have it the same name as in", "start": 10883.6, "duration": 5.759}, {"text": "this loop where it's called I just to", "start": 10887.359, "duration": 4.481}, {"text": "avoid any confusion. And then we are", "start": 10889.359, "duration": 5.04}, {"text": "also correspondingly going to call", "start": 10891.84, "duration": 5.68}, {"text": "backward this which also receives the", "start": 10894.399, "duration": 7.281}, {"text": "microbatch index. So we'll just refer to", "start": 10897.52, "duration": 7.2}, {"text": "our G-pipe code here since we've already", "start": 10901.68, "duration": 6.719}, {"text": "written it and this way I'll have less", "start": 10904.72, "duration": 7.759}, {"text": "chance of writing any bugs mistakenly.", "start": 10908.399, "duration": 6.0}, {"text": "So just as a reminder during the forward", "start": 10912.479, "duration": 4.8}, {"text": "pass we first set up the input and this", "start": 10914.399, "duration": 6.721}, {"text": "is to say if we are on GPU0 then we will", "start": 10917.279, "duration": 7.761}, {"text": "get the micro batch from our micro", "start": 10921.12, "duration": 6.8}, {"text": "batches list and then since this list", "start": 10925.04, "duration": 5.279}, {"text": "only exists for GPU0 otherwise we're", "start": 10927.92, "duration": 4.64}, {"text": "just going to receive from the previous", "start": 10930.319, "duration": 5.521}, {"text": "GPU. So coming here we're just going to", "start": 10932.56, "duration": 7.759}, {"text": "do if coms.rank rank equals zero.", "start": 10935.84, "duration": 9.12}, {"text": "Then we will say input data", "start": 10940.319, "duration": 9.04}, {"text": "equals micro batches. Did we not define", "start": 10944.96, "duration": 6.88}, {"text": "the list? Okay, that's a good start. I", "start": 10949.359, "duration": 4.401}, {"text": "realized that we haven't defined the", "start": 10951.84, "duration": 3.439}, {"text": "chunk.", "start": 10953.76, "duration": 2.719}, {"text": "We haven't actually chunked the batches", "start": 10955.279, "duration": 3.361}, {"text": "into microbatch and targets and we also", "start": 10956.479, "duration": 3.761}, {"text": "have not initialized the buffer for", "start": 10958.64, "duration": 3.44}, {"text": "activations and gradients. So we'll", "start": 10960.24, "duration": 3.92}, {"text": "remove the pass here and do that", "start": 10962.08, "duration": 4.8}, {"text": "quickly. If coms.rank rank equals equals", "start": 10964.16, "duration": 5.92}, {"text": "zero. Then let's define the microbatches", "start": 10966.88, "duration": 6.32}, {"text": "for the last GPU and we will call", "start": 10970.08, "duration": 5.84}, {"text": "torch.chunk for this purpose over the", "start": 10973.2, "duration": 5.68}, {"text": "batches and we'll chunk them into in", "start": 10975.92, "duration": 7.92}, {"text": "this case a chunks for 1 F1B and then if", "start": 10978.88, "duration": 9.36}, {"text": "coms.rank rank equals coms.orld", "start": 10983.84, "duration": 9.68}, {"text": "size minus one. We will add our micro or", "start": 10988.24, "duration": 8.96}, {"text": "we'll instantiate our micro targets list", "start": 10993.52, "duration": 5.759}, {"text": "over the targets which are once again", "start": 10997.2, "duration": 5.04}, {"text": "randomly initialized. So I realize now", "start": 10999.279, "duration": 4.561}, {"text": "that this is kind of strange because in", "start": 11002.24, "duration": 2.88}, {"text": "one case we're using the torch.chunk", "start": 11003.84, "duration": 3.519}, {"text": "method. Another case we're using the", "start": 11005.12, "duration": 4.239}, {"text": "other uh notation where you can replace", "start": 11007.359, "duration": 4.321}, {"text": "torch with just the first argument of", "start": 11009.359, "duration": 4.721}, {"text": "the function. So let's not do that here", "start": 11011.68, "duration": 4.88}, {"text": "just to be consistent. Torch.chunk", "start": 11014.08, "duration": 5.68}, {"text": "over targets into chunks.", "start": 11016.56, "duration": 5.6}, {"text": "Okay. And then we also want to define", "start": 11019.76, "duration": 5.599}, {"text": "our input buffers", "start": 11022.16, "duration": 6.159}, {"text": "and our output buffers. Oh, sorry about", "start": 11025.359, "duration": 5.841}, {"text": "that. One thing that I will mention is", "start": 11028.319, "duration": 7.601}, {"text": "that instead of defining this as a empty", "start": 11031.2, "duration": 8.079}, {"text": "list, I'm going to define it as a list", "start": 11035.92, "duration": 7.04}, {"text": "of none by chunks. And why am I doing", "start": 11039.279, "duration": 5.841}, {"text": "this? It's because", "start": 11042.96, "duration": 4.88}, {"text": "just out of my own uh practice of", "start": 11045.12, "duration": 4.8}, {"text": "implementing this before recording this", "start": 11047.84, "duration": 5.36}, {"text": "course, since we do have interled", "start": 11049.92, "duration": 6.08}, {"text": "forward backward passes in 1 F1B versus", "start": 11053.2, "duration": 6.079}, {"text": "the G-pipe algorithm where everything is", "start": 11056.0, "duration": 5.6}, {"text": "consecutive and there's no interle of", "start": 11059.279, "duration": 4.881}, {"text": "forward and backward. This results in", "start": 11061.6, "duration": 5.44}, {"text": "list accesses which are out of index. if", "start": 11064.16, "duration": 6.0}, {"text": "you simply append like we're doing here", "start": 11067.04, "duration": 5.52}, {"text": "and just call the index consecutively.", "start": 11070.16, "duration": 3.84}, {"text": "But since we're doing it kind of out of", "start": 11072.56, "duration": 3.44}, {"text": "order in F1B, just out of my own", "start": 11074.0, "duration": 4.399}, {"text": "experience, doing a list that's", "start": 11076.0, "duration": 4.72}, {"text": "initialized as zero leads to a index", "start": 11078.399, "duration": 4.721}, {"text": "error. So for that reason, if we come", "start": 11080.72, "duration": 5.04}, {"text": "back here, we're just going to start our", "start": 11083.12, "duration": 5.359}, {"text": "index, sorry, our input and our output", "start": 11085.76, "duration": 7.76}, {"text": "buffers both as a list of non-chunks. So", "start": 11088.479, "duration": 7.441}, {"text": "in this case, chunks being eight. Okay,", "start": 11093.52, "duration": 4.64}, {"text": "so let's just change this to output. And", "start": 11095.92, "duration": 4.64}, {"text": "then coming back here, we can finally", "start": 11098.16, "duration": 5.119}, {"text": "recont continue our micro batches", "start": 11100.56, "duration": 5.12}, {"text": "implementation by just grabbing the", "start": 11103.279, "duration": 4.961}, {"text": "microbatch index for our forward pass.", "start": 11105.68, "duration": 4.4}, {"text": "So that's all we have to do in this case", "start": 11108.24, "duration": 6.48}, {"text": "and then otherwise we are on a nonGPU", "start": 11110.08, "duration": 6.96}, {"text": "zero GPU and the shape of the tensor", "start": 11114.72, "duration": 4.24}, {"text": "since we are doing gradientation across", "start": 11117.04, "duration": 4.0}, {"text": "these microbatches is batch divided by", "start": 11118.96, "duration": 4.64}, {"text": "chunks and the hidden dimension remains", "start": 11121.04, "duration": 4.8}, {"text": "the same. Then we will grab the", "start": 11123.6, "duration": 4.879}, {"text": "activations from the GPU that is in", "start": 11125.84, "duration": 5.92}, {"text": "front of us through the coms. Forward", "start": 11128.479, "duration": 7.201}, {"text": "method and that takes the following", "start": 11131.76, "duration": 7.679}, {"text": "shape and device parameters. And then we", "start": 11135.68, "duration": 8.08}, {"text": "also need to set the input data uh", "start": 11139.439, "duration": 6.401}, {"text": "requires gradient to true. So so we", "start": 11143.76, "duration": 3.44}, {"text": "properly save the gradients. And then", "start": 11145.84, "duration": 3.12}, {"text": "the last step of course is to do the", "start": 11147.2, "duration": 3.279}, {"text": "forward pass itself since at this point", "start": 11148.96, "duration": 3.68}, {"text": "we've just got the data the activations", "start": 11150.479, "duration": 3.92}, {"text": "whatever they may be to perform that", "start": 11152.64, "duration": 4.16}, {"text": "forward pass. So let's just first check", "start": 11154.399, "duration": 6.08}, {"text": "if the GPU is the last one in which case", "start": 11156.8, "duration": 7.12}, {"text": "we will compute with our targets. So", "start": 11160.479, "duration": 9.041}, {"text": "world size oops minus one. Then in this", "start": 11163.92, "duration": 8.72}, {"text": "case the output is the model", "start": 11169.52, "duration": 6.879}, {"text": "with both the input data and the micro", "start": 11172.64, "duration": 5.44}, {"text": "targets", "start": 11176.399, "duration": 4.161}, {"text": "but we take the specific micro batch", "start": 11178.08, "duration": 7.359}, {"text": "index in this case and then otherwise", "start": 11180.56, "duration": 8.96}, {"text": "we will say the output is the", "start": 11185.439, "duration": 7.121}, {"text": "model on the input", "start": 11189.52, "duration": 5.2}, {"text": "and then we'll pass it forward. So we'll", "start": 11192.56, "duration": 6.36}, {"text": "say coms send_forward", "start": 11194.72, "duration": 4.2}, {"text": "of the detached output since we don't", "start": 11199.279, "duration": 5.681}, {"text": "want to pass the pietorch graph to", "start": 11203.04, "duration": 4.399}, {"text": "another device as that will lead to", "start": 11204.96, "duration": 4.56}, {"text": "memory leakage as we as we established", "start": 11207.439, "duration": 6.0}, {"text": "and here we will set our input buffers", "start": 11209.52, "duration": 7.6}, {"text": "of the specific micro batch so we save", "start": 11213.439, "duration": 8.401}, {"text": "it and don't lose this to the input data", "start": 11217.12, "duration": 8.4}, {"text": "and then likewise is for the output. So", "start": 11221.84, "duration": 7.2}, {"text": "if I come here and do this, this should", "start": 11225.52, "duration": 5.52}, {"text": "work except this is called output. All", "start": 11229.04, "duration": 3.84}, {"text": "right. So that's the forward pass. And", "start": 11231.04, "duration": 4.88}, {"text": "now for the backward pass, it's the same", "start": 11232.88, "duration": 6.64}, {"text": "procedure as the forward except in the", "start": 11235.92, "duration": 5.68}, {"text": "opposite direction. So first we need to", "start": 11239.52, "duration": 3.839}, {"text": "grab our input data from the input", "start": 11241.6, "duration": 5.679}, {"text": "buffers using our index.", "start": 11243.359, "duration": 8.08}, {"text": "And let's also grab the output from the", "start": 11247.279, "duration": 7.601}, {"text": "output buffers list.", "start": 11251.439, "duration": 5.281}, {"text": "All right. And then the next step is to", "start": 11254.88, "duration": 3.84}, {"text": "see what GPU we're on. So if we're on", "start": 11256.72, "duration": 4.96}, {"text": "GPU, the last one, then we need to", "start": 11258.72, "duration": 6.88}, {"text": "calculate the loss and", "start": 11261.68, "duration": 7.44}, {"text": "add it to our running sum. But since", "start": 11265.6, "duration": 5.839}, {"text": "it's a function here, this is actually", "start": 11269.12, "duration": 4.319}, {"text": "not going to be the case. We're instead", "start": 11271.439, "duration": 4.321}, {"text": "going to return the loss because if you", "start": 11273.439, "duration": 5.92}, {"text": "remember here, we add the total the", "start": 11275.76, "duration": 5.44}, {"text": "result of the function to the total", "start": 11279.359, "duration": 5.361}, {"text": "loss. So let's first put this check", "start": 11281.2, "duration": 7.84}, {"text": "here. If we're on the last device,", "start": 11284.72, "duration": 8.48}, {"text": "then the loss is equal to the output", "start": 11289.04, "duration": 7.359}, {"text": "divided by the chunks. Then we will call", "start": 11293.2, "duration": 5.6}, {"text": "backward on the loss to begin the", "start": 11296.399, "duration": 6.88}, {"text": "backward propagation pass. And then", "start": 11298.8, "duration": 6.479}, {"text": "nothing will happen here because we're", "start": 11303.279, "duration": 3.521}, {"text": "just going to return the loss. And now", "start": 11305.279, "duration": 2.641}, {"text": "let's move on to the case we're not the", "start": 11306.8, "duration": 3.519}, {"text": "last GPU. So let's just call this", "start": 11307.92, "duration": 5.519}, {"text": "gradient equals coms.receive backward", "start": 11310.319, "duration": 7.841}, {"text": "from the GPU that is upstream and this", "start": 11313.439, "duration": 6.801}, {"text": "takes the parameter output shape since", "start": 11318.16, "duration": 3.279}, {"text": "we need to know what size that we're", "start": 11320.24, "duration": 2.8}, {"text": "receiving and it's the same size as the", "start": 11321.439, "duration": 2.88}, {"text": "output activation since the gradients", "start": 11323.04, "duration": 2.399}, {"text": "output activations have the same", "start": 11324.319, "duration": 4.561}, {"text": "dimensionality the same device too and", "start": 11325.439, "duration": 5.92}, {"text": "then in this case we will call output", "start": 11328.88, "duration": 3.92}, {"text": "backward. So it's applying the first", "start": 11331.359, "duration": 3.04}, {"text": "argument as the activations and the", "start": 11332.8, "duration": 3.44}, {"text": "second argument as the gradients or we", "start": 11334.399, "duration": 3.441}, {"text": "can equivalently do just torch.backward", "start": 11336.24, "duration": 6.079}, {"text": "backward of outputs and", "start": 11337.84, "duration": 6.16}, {"text": "grads and this is called output if I'm", "start": 11342.319, "duration": 3.361}, {"text": "not mistaken. So torch.backward and", "start": 11344.0, "duration": 3.359}, {"text": "output.backward of the gradients is the", "start": 11345.68, "duration": 5.2}, {"text": "same exact syntax. And now if we come", "start": 11347.359, "duration": 6.881}, {"text": "into our penultimate check which is to", "start": 11350.88, "duration": 8.0}, {"text": "say are we on the non first GPU because", "start": 11354.24, "duration": 7.119}, {"text": "in that case we'll need to continue the", "start": 11358.88, "duration": 5.28}, {"text": "back pass or else the gradient signal", "start": 11361.359, "duration": 6.241}, {"text": "will die. And in this case, we want to", "start": 11364.16, "duration": 8.8}, {"text": "send the gradients of the inputs to that", "start": 11367.6, "duration": 6.96}, {"text": "layer to the previous layer so that it", "start": 11372.96, "duration": 4.0}, {"text": "can compute its own gradients with", "start": 11374.56, "duration": 4.879}, {"text": "respect to the input. And as I mentioned", "start": 11376.96, "duration": 4.8}, {"text": "here, we want to check if we're the last", "start": 11379.439, "duration": 4.721}, {"text": "GPU, then we want to return loss so that", "start": 11381.76, "duration": 5.2}, {"text": "we can continue", "start": 11384.16, "duration": 5.68}, {"text": "the gradient accumulation or rather the", "start": 11386.96, "duration": 4.399}, {"text": "accumulation of the loss so that we can", "start": 11389.84, "duration": 5.439}, {"text": "just present that as a statistic.", "start": 11391.359, "duration": 7.601}, {"text": "Okay, so it looks like our entire setup", "start": 11395.279, "duration": 7.521}, {"text": "is good and let's just run it and see", "start": 11398.96, "duration": 6.0}, {"text": "what happens. So, we're going to come", "start": 11402.8, "duration": 6.08}, {"text": "into our terminal UV torch run. And in", "start": 11404.96, "duration": 6.319}, {"text": "this case, we still want to say that we", "start": 11408.88, "duration": 5.76}, {"text": "have four nodes. The one thing is I", "start": 11411.279, "duration": 6.801}, {"text": "don't think we've changed our chunk to", "start": 11414.64, "duration": 5.92}, {"text": "eight. So, let's Oh, no. We have it as", "start": 11418.08, "duration": 5.44}, {"text": "eight. So, there you go. And that number", "start": 11420.56, "duration": 5.839}, {"text": "of process per node here is four. We're", "start": 11423.52, "duration": 5.12}, {"text": "going to come into the my work directory", "start": 11426.399, "duration": 4.161}, {"text": "and we're going to do step six, not step", "start": 11428.64, "duration": 3.28}, {"text": "six, sorry, step five because we're", "start": 11430.56, "duration": 3.28}, {"text": "running the main function. and let's see", "start": 11431.92, "duration": 4.399}, {"text": "what happens.", "start": 11433.84, "duration": 4.16}, {"text": "Okay, so it looks like we have a bug", "start": 11436.319, "duration": 5.921}, {"text": "here. And what is that exactly? Ah, so", "start": 11438.0, "duration": 6.88}, {"text": "somewhere I said ranks by accident. So", "start": 11442.24, "duration": 5.36}, {"text": "there you go. Let's change that and", "start": 11444.88, "duration": 5.439}, {"text": "let's see now what happens what we get.", "start": 11447.6, "duration": 4.719}, {"text": "Okay, one more bug. Hopefully we can fix", "start": 11450.319, "duration": 7.201}, {"text": "this really fast. There is no input.", "start": 11452.319, "duration": 7.361}, {"text": "Method. Ah, input. Why am I calling", "start": 11457.52, "duration": 5.2}, {"text": "input? It's input data.", "start": 11459.68, "duration": 5.759}, {"text": "Oh yeah, that's a built-in method. Oops.", "start": 11462.72, "duration": 5.2}, {"text": "That's the Python built-in input method,", "start": 11465.439, "duration": 6.401}, {"text": "not the input data that is representing", "start": 11467.92, "duration": 6.8}, {"text": "the input to a specific forward layer.", "start": 11471.84, "duration": 5.36}, {"text": "Okay. And now we got the error that I", "start": 11474.72, "duration": 4.48}, {"text": "was wishing we h we would get. And", "start": 11477.2, "duration": 3.68}, {"text": "what's happening? We're getting a", "start": 11479.2, "duration": 4.48}, {"text": "deadlock error. So deadlock means", "start": 11480.88, "duration": 4.8}, {"text": "there's two processes that are expecting", "start": 11483.68, "duration": 3.44}, {"text": "to receive data from each other and", "start": 11485.68, "duration": 3.679}, {"text": "they're both sending. But since they're", "start": 11487.12, "duration": 3.68}, {"text": "both sending and they're both expecting", "start": 11489.359, "duration": 3.361}, {"text": "to receive, it's essentially that", "start": 11490.8, "duration": 3.599}, {"text": "they're like walking through the same", "start": 11492.72, "duration": 4.48}, {"text": "hallway and there's only space for one", "start": 11494.399, "duration": 3.761}, {"text": "person in that hallway and they're", "start": 11497.2, "duration": 3.36}, {"text": "trying to get past each other, but there", "start": 11498.16, "duration": 4.0}, {"text": "is not enough space to get past each", "start": 11500.56, "duration": 3.28}, {"text": "other. So, it's as if they're trying to", "start": 11502.16, "duration": 4.64}, {"text": "send each other two things. And for this", "start": 11503.84, "duration": 5.04}, {"text": "reason, it's getting stuck. And I have", "start": 11506.8, "duration": 3.92}, {"text": "an explanation for why it's happening.", "start": 11508.88, "duration": 3.36}, {"text": "But once again, this is mainly due to", "start": 11510.72, "duration": 3.52}, {"text": "this interleing 1 F1B structure, which", "start": 11512.24, "duration": 5.44}, {"text": "is not really sequential. And let's go", "start": 11514.24, "duration": 5.6}, {"text": "and see what's happening. So this is the", "start": 11517.68, "duration": 5.679}, {"text": "explanation why 1F1B needs an async", "start": 11519.84, "duration": 5.519}, {"text": "forward and request tracking. So we're", "start": 11523.359, "duration": 2.721}, {"text": "first going to talk about this", "start": 11525.359, "duration": 2.96}, {"text": "asynchronous forward which will use the", "start": 11526.08, "duration": 5.04}, {"text": "I send forward instead of just the send", "start": 11528.319, "duration": 4.321}, {"text": "forward method. And this just means it's", "start": 11531.12, "duration": 3.199}, {"text": "an asynchronous method such that it", "start": 11532.64, "duration": 4.4}, {"text": "doesn't wait until it actually is", "start": 11534.319, "duration": 6.641}, {"text": "received to move to the next part of the", "start": 11537.04, "duration": 5.359}, {"text": "function. Instead, when you have an", "start": 11540.96, "duration": 3.2}, {"text": "asynchronous forward, it will call that", "start": 11542.399, "duration": 3.04}, {"text": "forward and then move on to the next", "start": 11544.16, "duration": 3.279}, {"text": "piece of code and just add that to the", "start": 11545.439, "duration": 4.0}, {"text": "asynchronous queue. So, what's actually", "start": 11547.439, "duration": 3.92}, {"text": "happening here is that so rank two is", "start": 11549.439, "duration": 3.601}, {"text": "trying to forward microbatch one to rank", "start": 11551.359, "duration": 3.281}, {"text": "three and rank three is trying to", "start": 11553.04, "duration": 4.8}, {"text": "forward micro or rather backward", "start": 11554.64, "duration": 5.679}, {"text": "microbatch zero to rank two. So, where", "start": 11557.84, "duration": 4.08}, {"text": "does this happen in the code? It happens", "start": 11560.319, "duration": 3.681}, {"text": "right here. So, this is the GP of rank", "start": 11561.92, "duration": 5.359}, {"text": "two and it is trying to forward the", "start": 11564.0, "duration": 5.68}, {"text": "microbatch one. So once again, we're", "start": 11567.279, "duration": 3.761}, {"text": "doing zero indexing here, but it's kind", "start": 11569.68, "duration": 3.599}, {"text": "of misleading. It's trying to forward", "start": 11571.04, "duration": 8.64}, {"text": "microbatch 1 to GPU 3, and GPU 3 is", "start": 11573.279, "duration": 9.681}, {"text": "trying to send microbatch zero in the", "start": 11579.68, "duration": 6.88}, {"text": "backward pass to microbatch to GPU 2.", "start": 11582.96, "duration": 5.04}, {"text": "And because they're both trying to send", "start": 11586.56, "duration": 4.0}, {"text": "and receive, you can see actually we", "start": 11588.0, "duration": 4.96}, {"text": "have this like cross-hatch diagonal", "start": 11590.56, "duration": 5.12}, {"text": "thing going on here. Because of this,", "start": 11592.96, "duration": 4.56}, {"text": "none of them can receive since they're", "start": 11595.68, "duration": 3.28}, {"text": "both sending and they're both waiting", "start": 11597.52, "duration": 2.879}, {"text": "for the other to receive before they", "start": 11598.96, "duration": 2.8}, {"text": "move on to the next part of their code.", "start": 11600.399, "duration": 2.801}, {"text": "And for this reason, we need to do an", "start": 11601.76, "duration": 3.679}, {"text": "asynchronous send. And this is going to", "start": 11603.2, "duration": 5.84}, {"text": "be pretty simple to add to our code. So,", "start": 11605.439, "duration": 5.121}, {"text": "what we're going to do is first come", "start": 11609.04, "duration": 4.88}, {"text": "here and we only need to modify our send", "start": 11610.56, "duration": 6.24}, {"text": "forward method. So, that is right here.", "start": 11613.92, "duration": 4.88}, {"text": "We're just going to call this I send", "start": 11616.8, "duration": 4.72}, {"text": "forward. And then if we come into our", "start": 11618.8, "duration": 5.679}, {"text": "coms, I think I've already added it,", "start": 11621.52, "duration": 6.16}, {"text": "right? So it's identical almost to send", "start": 11624.479, "duration": 6.241}, {"text": "forward except instead of doing dist", "start": 11627.68, "duration": 6.0}, {"text": "send, it's just called distend. And just", "start": 11630.72, "duration": 4.48}, {"text": "as it says here, it sends a tensor", "start": 11633.68, "duration": 4.16}, {"text": "asynchronously as opposed to sending a", "start": 11635.2, "duration": 6.32}, {"text": "tensor in the above case synchronously.", "start": 11637.84, "duration": 4.88}, {"text": "So we're really just changing one", "start": 11641.52, "duration": 5.28}, {"text": "letter. But if you guys come back into", "start": 11642.72, "duration": 7.36}, {"text": "our code and if we terminate this and", "start": 11646.8, "duration": 5.599}, {"text": "try it again, we'll see that it actually", "start": 11650.08, "duration": 4.08}, {"text": "gets a different error now. Hopefully.", "start": 11652.399, "duration": 4.561}, {"text": "There you go. So, what's the new error?", "start": 11654.16, "duration": 6.319}, {"text": "Let's read this together.", "start": 11656.96, "duration": 6.399}, {"text": "It's pretty important. Yes. Cannot lock", "start": 11660.479, "duration": 5.361}, {"text": "pointer to unbound buffer. Okay. So,", "start": 11663.359, "duration": 4.561}, {"text": "what does that mean? It essentially", "start": 11665.84, "duration": 4.0}, {"text": "means that there's a pointer that's", "start": 11667.92, "duration": 3.359}, {"text": "pointing to some memory, but it's", "start": 11669.84, "duration": 3.28}, {"text": "unbound. meaning this memory is not", "start": 11671.279, "duration": 3.761}, {"text": "saved. So the pointer will not be able", "start": 11673.12, "duration": 5.359}, {"text": "to with certainty maintain this part of", "start": 11675.04, "duration": 5.6}, {"text": "memory and what's happening here. So", "start": 11678.479, "duration": 4.0}, {"text": "this is the second error that I have in", "start": 11680.64, "duration": 4.08}, {"text": "our little explanation if we come down.", "start": 11682.479, "duration": 3.84}, {"text": "So the first one was once again the send", "start": 11684.72, "duration": 3.36}, {"text": "forward deadlock and the second one is", "start": 11686.319, "duration": 3.841}, {"text": "we need to save these request handles to", "start": 11688.08, "duration": 4.56}, {"text": "prevent buffer deallocation. So what's", "start": 11690.16, "duration": 6.88}, {"text": "happening in I send forward without", "start": 11692.64, "duration": 9.12}, {"text": "request saving when we do our forward", "start": 11697.04, "duration": 7.52}, {"text": "pass it starts the async send but the", "start": 11701.76, "duration": 5.04}, {"text": "request is not saved anywhere and this", "start": 11704.56, "duration": 4.879}, {"text": "means that the function returns but the", "start": 11706.8, "duration": 4.479}, {"text": "request goes out of scope so it's just", "start": 11709.439, "duration": 3.92}, {"text": "kind of discarded as a local variable", "start": 11711.279, "duration": 4.801}, {"text": "and then the Python garbage collector", "start": 11713.359, "duration": 5.12}, {"text": "will potentially and this is what", "start": 11716.08, "duration": 4.48}, {"text": "happens because we got this sig term", "start": 11718.479, "duration": 3.361}, {"text": "error if you", "start": 11720.56, "duration": 4.24}, {"text": "So because of this error, this means", "start": 11721.84, "duration": 5.36}, {"text": "that the Python garbage collector freed", "start": 11724.8, "duration": 4.559}, {"text": "that memory and then now the pointer was", "start": 11727.2, "duration": 4.239}, {"text": "pointing to an unbound buffer and that's", "start": 11729.359, "duration": 3.361}, {"text": "what gave us the error. So what's", "start": 11731.439, "duration": 2.561}, {"text": "happening in the communication back end", "start": 11732.72, "duration": 2.719}, {"text": "when we get this error is that the", "start": 11734.0, "duration": 2.96}, {"text": "asynchronous send creates an internal", "start": 11735.439, "duration": 4.561}, {"text": "buffer that references this output and", "start": 11736.96, "duration": 5.439}, {"text": "then it does the send in the background", "start": 11740.0, "duration": 4.64}, {"text": "because it's asynchronous and this", "start": 11742.399, "duration": 4.481}, {"text": "request object is garbage collected. So", "start": 11744.64, "duration": 3.679}, {"text": "glue loses the reference to the", "start": 11746.88, "duration": 3.92}, {"text": "output.attach attach object and then it", "start": 11748.319, "duration": 4.561}, {"text": "tries to access the buffer again but it", "start": 11750.8, "duration": 4.08}, {"text": "cannot lock the pointer to an unbound", "start": 11752.88, "duration": 5.519}, {"text": "buffer and then it crashes. So because", "start": 11754.88, "duration": 5.28}, {"text": "of this what we're going to instead do", "start": 11758.399, "duration": 5.201}, {"text": "is create a list called async requests", "start": 11760.16, "duration": 6.319}, {"text": "which will just save the buffers as a", "start": 11763.6, "duration": 6.48}, {"text": "persistent state. And because this list", "start": 11766.479, "duration": 5.601}, {"text": "that we will define called async", "start": 11770.08, "duration": 3.84}, {"text": "requests will be defined outside of the", "start": 11772.08, "duration": 4.64}, {"text": "helper function, it will persist in", "start": 11773.92, "duration": 6.64}, {"text": "scope until the 1F1B function returns.", "start": 11776.72, "duration": 5.84}, {"text": "And this is good because by that point", "start": 11780.56, "duration": 3.36}, {"text": "all of the sends will have been", "start": 11782.56, "duration": 4.48}, {"text": "completed. So let me now show you what", "start": 11783.92, "duration": 5.6}, {"text": "this whole explanation looks like in", "start": 11787.04, "duration": 5.52}, {"text": "practice. So because we return the", "start": 11789.52, "duration": 5.52}, {"text": "asynchronous send object in this", "start": 11792.56, "duration": 4.56}, {"text": "function which is a distributed request", "start": 11795.04, "duration": 6.48}, {"text": "object then we can save it as just req", "start": 11797.12, "duration": 7.04}, {"text": "as we had in that diagram there like", "start": 11801.52, "duration": 4.16}, {"text": "this", "start": 11804.16, "duration": 5.119}, {"text": "and then we will do the following first", "start": 11805.68, "duration": 6.16}, {"text": "we'll say async", "start": 11809.279, "duration": 4.08}, {"text": "request and once again this list is", "start": 11811.84, "duration": 3.92}, {"text": "simply to save that memory so that it's", "start": 11813.359, "duration": 4.88}, {"text": "not save that buffer so that it's not", "start": 11815.76, "duration": 4.479}, {"text": "deallocated by the garbage collector. So", "start": 11818.239, "duration": 4.641}, {"text": "it's just empty list at the start", "start": 11820.239, "duration": 5.841}, {"text": "and then we will add to that empty list", "start": 11822.88, "duration": 6.64}, {"text": "once again right after the asynchronous", "start": 11826.08, "duration": 9.359}, {"text": "send. So we'll append the request", "start": 11829.52, "duration": 8.48}, {"text": "and yeah that's pretty much it at this", "start": 11835.439, "duration": 4.481}, {"text": "point. Now these requests will be saved", "start": 11838.0, "duration": 4.239}, {"text": "and they will not be garbage collected", "start": 11839.92, "duration": 5.28}, {"text": "and it should work. So let's try this", "start": 11842.239, "duration": 5.841}, {"text": "one final time and see what happens.", "start": 11845.2, "duration": 6.48}, {"text": "Okay, never mind. Torch has no object.", "start": 11848.08, "duration": 5.04}, {"text": "Ah, okay. So, I think we need to do", "start": 11851.68, "duration": 4.96}, {"text": "torch.autograd here. I forgot that it is", "start": 11853.12, "duration": 5.359}, {"text": "not just torch. But rather", "start": 11856.64, "duration": 3.52}, {"text": "torch.autograd.backward.", "start": 11858.479, "duration": 3.601}, {"text": "If you want to use this type of", "start": 11860.16, "duration": 4.64}, {"text": "notation. So, coming into our backward.", "start": 11862.08, "duration": 4.239}, {"text": "Where are we doing this? Right. It's", "start": 11864.8, "duration": 5.36}, {"text": "right here. So, autograd. Boom. There", "start": 11866.319, "duration": 7.04}, {"text": "you go. Now, it's filling up.", "start": 11870.16, "duration": 6.159}, {"text": "All right. Boom. So, we got our result.", "start": 11873.359, "duration": 4.721}, {"text": "This is really awesome. So now let's see", "start": 11876.319, "duration": 3.12}, {"text": "what we get with G-pipe just as a", "start": 11878.08, "duration": 4.399}, {"text": "comparison since I forget. And let's", "start": 11879.439, "duration": 4.88}, {"text": "check it out. Boom. So we do get the", "start": 11882.479, "duration": 4.721}, {"text": "same loss both with Gpipe and 1 F1B,", "start": 11884.319, "duration": 4.801}, {"text": "which is what we should get in", "start": 11887.2, "duration": 4.0}, {"text": "expectation since they're both doing the", "start": 11889.12, "duration": 3.84}, {"text": "exact same work. The only difference", "start": 11891.2, "duration": 4.96}, {"text": "being that the 1 F1B is interle forward", "start": 11892.96, "duration": 5.2}, {"text": "and backward passes in a smart way to", "start": 11896.16, "duration": 5.279}, {"text": "reduce the P peak activation memory. So", "start": 11898.16, "duration": 5.52}, {"text": "yeah, this is awesome. Our 1F1B is", "start": 11901.439, "duration": 4.96}, {"text": "finally working and running really", "start": 11903.68, "duration": 5.28}, {"text": "smoothly. So there you go. You can see", "start": 11906.399, "duration": 4.321}, {"text": "that when we define these forward and", "start": 11908.96, "duration": 4.08}, {"text": "backward passes as helper functions, we", "start": 11910.72, "duration": 5.2}, {"text": "can show our training loop in these", "start": 11913.04, "duration": 4.56}, {"text": "three distinct phases in a really clean", "start": 11915.92, "duration": 3.92}, {"text": "and concise way. And in my opinion, it", "start": 11917.6, "duration": 4.48}, {"text": "makes it really nice to understand. So", "start": 11919.84, "duration": 3.599}, {"text": "since we've now done all of our", "start": 11922.08, "duration": 3.199}, {"text": "implementations of the three algorithms", "start": 11923.439, "duration": 4.161}, {"text": "of the course, the only step that I", "start": 11925.279, "duration": 4.08}, {"text": "mentioned we would still do is to", "start": 11927.6, "duration": 4.56}, {"text": "profile these and see how they perform", "start": 11929.359, "duration": 6.801}, {"text": "in terms of GPU and device utilization.", "start": 11932.16, "duration": 7.119}, {"text": "So I should have this command saved. All", "start": 11936.16, "duration": 5.04}, {"text": "right. So I'm going to run profile main", "start": 11939.279, "duration": 3.921}, {"text": "right now. And if we just come to", "start": 11941.2, "duration": 4.0}, {"text": "profiled main, we'll see that it's", "start": 11943.2, "duration": 5.36}, {"text": "currently set up to run 1 F1B in the", "start": 11945.2, "duration": 6.0}, {"text": "profile schedule. So it's the exact same", "start": 11948.56, "duration": 6.4}, {"text": "1F1B as we implemented except it has", "start": 11951.2, "duration": 7.6}, {"text": "once again all of these context managers", "start": 11954.96, "duration": 6.72}, {"text": "to record the backward compute and the", "start": 11958.8, "duration": 4.0}, {"text": "different operations that occurred", "start": 11961.68, "duration": 3.92}, {"text": "during the pipeline process. So running", "start": 11962.8, "duration": 4.8}, {"text": "this now let's see what we get. It", "start": 11965.6, "duration": 4.799}, {"text": "should be better than the GPU", "start": 11967.6, "duration": 5.839}, {"text": "utilization that we got with our naive", "start": 11970.399, "duration": 5.04}, {"text": "pipeline. And in this case we can see it", "start": 11973.439, "duration": 5.681}, {"text": "is. So as we can see the compute share", "start": 11975.439, "duration": 8.321}, {"text": "of the time spent has increased and as", "start": 11979.12, "duration": 6.96}, {"text": "it was the same before since the last", "start": 11983.76, "duration": 5.519}, {"text": "GPU needs to do the model head in terms", "start": 11986.08, "duration": 5.52}, {"text": "of doing that classification binary", "start": 11989.279, "duration": 4.721}, {"text": "classification it has larger compute", "start": 11991.6, "duration": 4.879}, {"text": "compared to other GPUs and just out of", "start": 11994.0, "duration": 6.88}, {"text": "curiosity we can also run this sameuler", "start": 11996.479, "duration": 7.041}, {"text": "with Gpipe because we didn't do that", "start": 12000.88, "duration": 4.399}, {"text": "once we finished our implementation of", "start": 12003.52, "duration": 5.68}, {"text": "that. So let's just do this now for our", "start": 12005.279, "duration": 7.361}, {"text": "own curiosity and see what we get. It", "start": 12009.2, "duration": 6.4}, {"text": "should be slightly worse than our 1F1B", "start": 12012.64, "duration": 4.639}, {"text": "algorithm. And you can see that that's", "start": 12015.6, "duration": 6.56}, {"text": "the case. So 24.9 27 24.5 and 36 whereas", "start": 12017.279, "duration": 9.12}, {"text": "before we're getting 29 27 28 and 41. So", "start": 12022.16, "duration": 6.4}, {"text": "1 F1B as expected does perform better.", "start": 12026.399, "duration": 4.161}, {"text": "The one thing I will notice that I will", "start": 12028.56, "duration": 4.24}, {"text": "mention rather is that because we did", "start": 12030.56, "duration": 4.24}, {"text": "use synchronous operations for", "start": 12032.8, "duration": 4.4}, {"text": "everything except the forward send. So", "start": 12034.8, "duration": 4.96}, {"text": "you can see here that everything else", "start": 12037.2, "duration": 5.36}, {"text": "receive is synchronous. Um yeah, this is", "start": 12039.76, "duration": 4.639}, {"text": "the only asynchronous operation that's", "start": 12042.56, "duration": 4.16}, {"text": "in our entire codebase. Because of this,", "start": 12044.399, "duration": 3.761}, {"text": "we're still going to be pretty slow and", "start": 12046.72, "duration": 2.96}, {"text": "have to wait because these are blocking", "start": 12048.16, "duration": 3.04}, {"text": "communications. The asynchronous", "start": 12049.68, "duration": 4.24}, {"text": "functions once again wait until the send", "start": 12051.2, "duration": 5.039}, {"text": "or receive occurs before they move on.", "start": 12053.92, "duration": 5.04}, {"text": "If we did asynchronous interle then the", "start": 12056.239, "duration": 4.0}, {"text": "amount of time spent waiting would be", "start": 12058.96, "duration": 4.16}, {"text": "even less. But once again we're here to", "start": 12060.239, "duration": 5.12}, {"text": "learn the principles of pipeline payism", "start": 12063.12, "duration": 3.92}, {"text": "and not how to optimize it. So if we", "start": 12065.359, "duration": 3.92}, {"text": "come to the outline of the course we've", "start": 12067.04, "duration": 5.12}, {"text": "now done up to 1 F1B which is the last", "start": 12069.279, "duration": 5.2}, {"text": "part of our syllabus. Once again we have", "start": 12072.16, "duration": 4.72}, {"text": "this dualpipe slide which is just left", "start": 12074.479, "duration": 4.88}, {"text": "an exercise to the user. If you'd like,", "start": 12076.88, "duration": 5.519}, {"text": "you should now be able to go look up the", "start": 12079.359, "duration": 5.601}, {"text": "dualpipe method, see how it works, and", "start": 12082.399, "duration": 4.721}, {"text": "find a way to implement this using the", "start": 12084.96, "duration": 3.6}, {"text": "basic printers that we defined in this", "start": 12087.12, "duration": 5.04}, {"text": "course. And there you go. So, if you'd", "start": 12088.56, "duration": 7.04}, {"text": "like a future course where we design and", "start": 12092.16, "duration": 5.279}, {"text": "implement dual pipe from scratch, then", "start": 12095.6, "duration": 3.759}, {"text": "definitely let me know. Otherwise, I", "start": 12097.439, "duration": 3.441}, {"text": "hope you enjoyed this course and I hope", "start": 12099.359, "duration": 3.601}, {"text": "that now you have a greater intuition as", "start": 12100.88, "duration": 3.92}, {"text": "to the inner workings of pipeline", "start": 12102.96, "duration": 4.479}, {"text": "parallelism through this from scratch", "start": 12104.8, "duration": 4.559}, {"text": "implementation. So, thanks for watching", "start": 12107.439, "duration": 5.121}, {"text": "and have a nice day.", "start": 12109.359, "duration": 3.201}]]
