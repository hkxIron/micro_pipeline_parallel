## PipeDream: 1F1B Scheduling

- PipeDream accelerates pipeline parallelism by starting the backward pass for a microbatch as soon as its forward pass completes the final stage, enabling earlier disposal of cached activations.
    - ![](https://developer-blogs.nvidia.com/wp-content/uploads/2021/03/Pipeline_schedule_1F1B-1-625x185.png)
    - ![](https://sighingnow.github.io/resource/pipeline_parallelism/f-then-b.png)

- In the steady-state "1F1B" (1 Forward, 1 Backward) schedule, each device alternates between forward and backward passes. With 4 GPUs and 8 microbatches, at most 4 microbatches are in flight at any time, halving the same peak activation memory as GPipe.
    - ![](./flight.png)
    - A microbatch is in-flight if we performed >1 forward pass for it, but haven’t completed all the backward passes yet.
    - ![](https://siboehm.com/assets/img/distributed-DNNs/Pipedream_steady_state.png)

- Despite this memory advantage, the pipeline "bubble fraction"—idle time due to dependencies—is the same for GPipe and PipeDream because of the sequential dependency structure.
    - Visually, looking at the above PipeDream plot if you shift the blue forward passes left and the green backward passes right, you get GPipe. This explains why the bubble fraction is the same.

### Communication Volume

- Both GPipe and PipeDream require each GPU to send/receive `batch_size × N` data per forward and backward, totaling approximately `2 × (num_GPUs – 1) × batch_size × N` floats per batch for a model of hidden size `N`.
    - The -1 terms comes from the initial GPU not having to receive and the last GPU not having to send anything.

### Hypothetical Scenario

- If we had 100 micro-batches, GPipe would need 100x memory, but 1F1B would still only need about 4x. This is why we go through the trouble of the warm-up and cooldown!